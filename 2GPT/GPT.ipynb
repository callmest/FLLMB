{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "This notebook mainly implements GPT from nanoGPT.\n",
    "\n",
    "\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. pacakge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GPT model implementation\n",
    "\n",
    "It need to be mentioned that the modules implemented in previous charpter will not be detailed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataclass staticmethod classmethod: https://blog.csdn.net/sjxgghg/article/details/139861829\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # the length of a sentence\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embed: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CausalSelfAttention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # if we meet n_embed can not divided by n_head, raise an error\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "        # projection for qkv, it will be splitted below.\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias = config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n",
    "        # settings\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention will be deployed if avaliable, but support is only in pytorch > 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print('WARNING: using slow attention. Flash Attention requires Pytorch >= 2.0')\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence.\n",
    "            # about register buffer: https://blog.csdn.net/dagouxiaohui/article/details/125649813\n",
    "            # set parameters will be stored to state_dict, but not updated in training.\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))).view(1,1,config.block_size, config.block_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # batch_size, seq_len, embed_dim\n",
    "        B, T, C = x.size() \n",
    "\n",
    "        # split q, k, v\n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim = 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # apply torch attn\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention: q@k_t / sqrt(length)\n",
    "            att = (q @ k.transpose(-1, -2)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim = -1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "## MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias= config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias = config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GPT BLOCK\n",
    "Then we compile all the modules to a single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config=config)\n",
    "        self.ln_2 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.mlp = MLP(config=config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we apply pre-norm before inputting into the attention\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # check vocab_size and block_size\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        # actually decoder\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config=config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size,  bias = False)\n",
    "        # weight sharing between embedding layer and lm_head layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        # init all weights (embedding and lm_head)\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std= 0.02 / math.sqrt(2 * config.n_layer))\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        device = x.device\n",
    "        b, t = x.size()\n",
    "        # 检查句长是否超过限制\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        # generate embed and dropout\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # we only need to calculate the next token\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # in-place\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "    \n",
    "    def get_num_params(self, non_embedding = True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, block_size, block_size]\n",
    "\n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "         # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused = True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f'using fused adamW: {use_fused}')\n",
    "        return optimizer\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops (Floating Point Operations) utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        # get the attn shape\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embed // cfg.n_head, cfg.block_size\n",
    "        # per token calculated\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        # fwd and pwd need to iter every token\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.GPT Debug\n",
    "这一章节主要对于GPT的数据流程进行debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device and config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 50304\n",
    "block_size = 1024\n",
    "n_embed = 128\n",
    "n_head = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x shape: \n",
      " torch.Size([16, 256])\n",
      "input x: \n",
      " tensor([ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1], device='cuda:0')\n",
      "input y shape: \n",
      " torch.Size([16, 256])\n",
      "input y: \n",
      " tensor([13, 52, 42,  1, 26, 43, 56, 53,  1, 61], device='cuda:0')\n",
      "We found that actuall y is left-shfited for next token prediction\n"
     ]
    }
   ],
   "source": [
    "# 1. load data\n",
    "x = torch.load('X.tensor').to(device)\n",
    "y_true = torch.load('Y.tensor').to(device)\n",
    "print('input x shape: \\n', x.shape)\n",
    "print('input x: \\n', x[1, :10])\n",
    "print('input y shape: \\n', y_true.shape)\n",
    "print('input y: \\n', y_true[1, :10])\n",
    "print('We found that actuall y is left-shfited for next token prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 16, seq length is 256\n",
      "tok_embed x shape: \n",
      " torch.Size([16, 256, 128])\n",
      "tok_embed x: \n",
      " tensor([-0.1478,  0.7354, -0.4837,  0.1474,  0.5742,  0.3033, -0.5980,  0.1521,\n",
      "        -1.0428,  0.6998], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "pos: \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
      "pos_embed x shape: \n",
      " torch.Size([256, 128])\n",
      "pos_embed x: \n",
      " tensor([-0.4331, -0.2900, -0.4536,  0.8967, -0.3454, -1.3831, -1.5691, -0.1644,\n",
      "        -2.3333,  0.0184], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "embed out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "embed out: \n",
      " tensor([ 0.3274, -0.5053,  1.2319,  1.6149, -1.0405, -0.5273, -1.0943, -0.0129,\n",
      "         0.3036,  0.6458], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "YOU may notice that the out passes through the dropout layer and its value is changed!\n",
      "MORE INFOR: https://blog.csdn.net/weixin_43953686/article/details/105978308\n",
      "dropout out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "dropout out: \n",
      " tensor([ 0.4092, -0.6317,  1.5399,  2.0186, -1.3006, -0.6592, -1.3679, -0.0161,\n",
      "         0.3795,  0.0000], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. massenge passing through gpt block\n",
    "# b: batch_size, t: seq_len\n",
    "b, t = x.size()\n",
    "print(f'batch size is {b}, seq length is {t}')\n",
    "\n",
    "# 2.1 token and position embedding\n",
    "tok_embed = nn.Embedding(vocab_size, n_embed).to(device)\n",
    "tok_x = tok_embed(x)\n",
    "print('tok_embed x shape: \\n', tok_x.shape)\n",
    "print('tok_embed x: \\n', tok_x[0, 1, :10])\n",
    "pos = torch.arange(0, t, device=device, dtype=torch.long).to(device)\n",
    "print('pos: \\n', pos[:10])\n",
    "pos_embed = nn.Embedding(block_size, n_embed).to(device)\n",
    "pos_x = pos_embed(pos)\n",
    "print('pos_embed x shape: \\n', pos_x.shape)\n",
    "print('pos_embed x: \\n', pos_x[0, :10])\n",
    "# add and drop\n",
    "out = tok_x + pos_x\n",
    "print('embed out shape: \\n', out.shape)\n",
    "print('embed out: \\n', out[0, 1, :10])\n",
    "out = nn.Dropout(dropout)(out)\n",
    "print('YOU may notice that the out passes through the dropout layer and its value is changed!')\n",
    "print('MORE INFOR: https://blog.csdn.net/weixin_43953686/article/details/105978308')\n",
    "print('dropout out shape: \\n', out.shape)\n",
    "print('dropout out: \\n', out[0, 1, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "LayerNorm out: \n",
      " tensor([ 0.1493, -0.4678,  0.8197,  1.1036, -0.8644, -0.4841, -0.9043, -0.1028,\n",
      "         0.1317, -0.0933], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "qkv shape: \n",
      " torch.Size([16, 256, 384])\n",
      "qkv: \n",
      " tensor([-0.3934, -1.4019,  0.4857,  0.6232,  0.7293,  0.8850, -1.5802, -0.0346,\n",
      "        -0.2543,  0.2560], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Split qkv shape: \n",
      " torch.Size([16, 256, 128])\n",
      "Split qkv: \n",
      " tensor([-0.3934, -1.4019,  0.4857,  0.6232,  0.7293,  0.8850, -1.5802, -0.0346,\n",
      "        -0.2543,  0.2560], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Multi-head q shape: \n",
      " torch.Size([16, 4, 256, 32])\n",
      "Multi-head q : \n",
      " tensor([-0.3934, -1.4019,  0.4857,  0.6232,  0.7293,  0.8850, -1.5802, -0.0346,\n",
      "        -0.2543,  0.2560], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "attn score shape: \n",
      " torch.Size([16, 4, 256, 256])\n",
      "attn score : \n",
      " tensor([ 0.1649, -0.2066, -0.1766, -0.0480,  0.1454, -0.1804, -0.0288,  0.0559,\n",
      "        -0.3596,  0.1247], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "mask shape: \n",
      " torch.Size([1, 1, 1024, 1024])\n",
      "mask : \n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "masked attn shape: \n",
      " torch.Size([16, 4, 256, 256])\n",
      "masked attn : \n",
      " tensor([[-0.0983,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.1649, -0.2066,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.1595, -0.1468,  0.2420,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.0220,  0.0916,  0.0957,  0.0539,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.2210, -0.1821,  0.0119,  0.1536, -0.0486,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.1909, -0.0249, -0.0352,  0.2303,  0.0074, -0.1285,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.1330,  0.0665,  0.1148, -0.0764, -0.2693,  0.6072,  0.0763,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0461,  0.0720,  0.1877,  0.1446,  0.0262, -0.0145,  0.0099,  0.1355,\n",
      "            -inf,    -inf],\n",
      "        [ 0.1322,  0.1017, -0.0303, -0.0510,  0.0703, -0.5316, -0.1199, -0.1679,\n",
      "          0.2042,    -inf],\n",
      "        [ 0.1316, -0.2312,  0.0836, -0.1292, -0.1082, -0.2958,  0.1733,  0.3448,\n",
      "         -0.0807, -0.1064]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "attn output shape: \n",
      " torch.Size([16, 4, 256, 32])\n",
      "attn output : \n",
      " tensor([[ 1.1521,  0.1173, -0.2288,  0.0520,  0.2455, -0.7359,  0.7350, -1.0087,\n",
      "          0.6164,  1.7478],\n",
      "        [ 0.6818,  0.0694, -0.1354,  0.0308,  0.1453, -0.4355,  0.4350, -0.5970,\n",
      "          0.3648,  1.0343],\n",
      "        [ 0.4516, -0.1610, -0.0568, -0.5725, -0.7301, -0.3447,  0.6312, -0.0843,\n",
      "          0.4216,  0.4132],\n",
      "        [ 0.0832,  0.2318, -0.2408, -0.2788, -0.2008, -0.1750,  0.0257,  0.1116,\n",
      "          0.1034,  0.4465],\n",
      "        [ 0.2339,  0.0484, -0.2774, -0.4657, -0.7715,  0.1574,  0.2306,  0.2767,\n",
      "          0.0931, -0.0643],\n",
      "        [ 0.2207,  0.5015,  0.0442, -0.1949, -0.4931, -0.1723,  0.0901,  0.0168,\n",
      "          0.2188, -0.1804],\n",
      "        [ 0.1680,  0.5274,  0.0221, -0.0772, -0.5683, -0.1037,  0.0127,  0.0534,\n",
      "         -0.1198, -0.0385],\n",
      "        [ 0.2403,  0.1101, -0.1666,  0.0941, -0.6262,  0.1286,  0.1294, -0.0598,\n",
      "         -0.2158, -0.0087],\n",
      "        [-0.0812,  0.1839, -0.0490, -0.1305, -0.5740, -0.1476,  0.1283,  0.1019,\n",
      "          0.0502, -0.0880],\n",
      "        [ 0.2997,  0.2112, -0.2320,  0.0521, -0.7202, -0.0806,  0.2577, -0.1446,\n",
      "         -0.0623,  0.0143]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "y shape: \n",
      " torch.Size([16, 256, 128])\n",
      "y : \n",
      " tensor([ 1.1521,  0.1173, -0.2288,  0.0520,  0.2455, -0.7359,  0.7350, -1.0087,\n",
      "         0.6164,  1.7478], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "y proj out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "y proj out : \n",
      " tensor([-0.0000,  0.7530, -0.3289, -0.0776, -0.6845, -0.3170, -0.2950,  0.6256,\n",
      "        -0.2754,  0.1839], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "logits shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "logits: \n",
      " tensor([[ 0.7135,  0.2817, -0.3304, -1.1819,  0.0091,  0.8219,  0.3289, -0.3925,\n",
      "          0.2039,  0.3811]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "predicted logit for first word: \n",
      " tensor([0.8219], device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.2 attention\n",
    "\n",
    "## pre-layernorm\n",
    "out = nn.LayerNorm(n_embed).to(device)(out)\n",
    "print('LayerNorm out shape: \\n', out.shape)\n",
    "print('LayerNorm out: \\n', out[0, 1, :10])\n",
    "\n",
    "## qkv\n",
    "# generate qkv once\n",
    "qkv = nn.Linear(n_embed, 3 * n_embed).to(device)(out)\n",
    "print('qkv shape: \\n', qkv.shape)\n",
    "print('qkv: \\n', qkv[0, 1, :10])\n",
    "q, k, v = qkv.split(n_embed, dim = -1)\n",
    "print('Split qkv shape: \\n', q.shape)\n",
    "print('Split qkv: \\n', q[0, 1, :10])\n",
    "\n",
    "## multi-head\n",
    "q = q.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "k = k.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "v = v.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "print('Multi-head q shape: \\n', q.shape)\n",
    "print('Multi-head q : \\n', q[0, 0, 1, :10])\n",
    "\n",
    "# self attention\n",
    "attn = q@k.transpose(-1,-2) / math.sqrt(n_embed)\n",
    "print('attn score shape: \\n', attn.shape)\n",
    "print('attn score : \\n', attn[0, 0, 1, :10])\n",
    "\n",
    "# mask fill\n",
    "## generate mask -> mask is a tril matrix for next token prediction \n",
    "## mask is applied to q@k and shape is (b, t, seq_len, seq_len)\n",
    "bias = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size).to(device=device)\n",
    "print('mask shape: \\n', bias.shape)\n",
    "print('mask : \\n', bias[0, 0, :10, :10])\n",
    "## some times, t may less than max_len\n",
    "attn = attn.masked_fill(bias[:, :, :t, :t] == 0, float('-inf'))\n",
    "print('masked attn shape: \\n', attn.shape)\n",
    "print('masked attn : \\n', attn[0, 0, :10, :10])\n",
    "\n",
    "# softmax\n",
    "attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "# dropout\n",
    "attn = nn.Dropout(dropout)(attn)\n",
    "\n",
    "# output\n",
    "## (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "y = attn @ v\n",
    "print('attn output shape: \\n', y.shape)\n",
    "print('attn output : \\n', y[0, 0, :10, :10])\n",
    "## transpose and output\n",
    "y = y.transpose(1, 2).contiguous().view(b, t, n_embed)\n",
    "print('y shape: \\n', y.shape)\n",
    "print('y : \\n', y[0, 0, :10])\n",
    "# proj\n",
    "y = nn.Dropout(dropout)(nn.Linear(n_embed, n_embed).to(device)(y))\n",
    "print('y proj out shape: \\n', y.shape)\n",
    "print('y proj out : \\n', y[0, 0, :10])\n",
    "\n",
    "# layerNorm\n",
    "y = nn.LayerNorm(n_embed).to(device)(y)\n",
    "\n",
    "# output\n",
    "## we just get last token logits\n",
    "logits = nn.Linear(n_embed, vocab_size).to(device)(y)\n",
    "print('logits shape: \\n', logits.shape)\n",
    "print('logits: \\n', logits[0, [-1], :10])\n",
    "print('predicted logit for first word: \\n', logits[0, [-1], torch.argmax(logits[0, [-1], :10])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 training\n",
    "Our training and inference way are the same with nanoGPT, which DDP (distributed data parallel) is supported.\n",
    "see also at [nanoGTP](https://github.com/karpathy/nanoGPT) on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "probs: \n",
      " tensor([[ 0.7135,  0.2817, -0.3304, -1.1819,  0.0091,  0.8219,  0.3289, -0.3925,\n",
      "          0.2039,  0.3811]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "y lable shape: \n",
      " torch.Size([16, 256])\n",
      "y lable: \n",
      " tensor([40, 43,  0, 42, 39, 51, 52, 43, 42,  1], device='cuda:0')\n",
      "loss shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "loss: \n",
      " tensor(10.8260, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### 1. compute loss\n",
    "\n",
    "probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "print('probs shape: \\n', logits.shape)\n",
    "print('probs: \\n', logits[0, [-1], :10])\n",
    "\n",
    "print('y lable shape: \\n',y_true.shape)\n",
    "print('y lable: \\n', y_true[0,:10])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# transpose for loss calculation, torch crossentropy need (B, C, T)\n",
    "loss = loss_fn(probs.transpose(1, 2), y_true)\n",
    "\n",
    "print('loss shape: \\n', logits.shape)\n",
    "print('loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: \n",
      " (1003854,)\n",
      "train data: \n",
      " [18 47 56 57 58  1 15 47 58 47]\n",
      "val data shape: \n",
      " (111540,)\n",
      "val data: \n",
      " [12  0  0 19 30 17 25 21 27 10]\n"
     ]
    }
   ],
   "source": [
    "#### 2. load data\n",
    "path = os.getcwd()\n",
    "# set data path\n",
    "data_path = os.path.join(path, 'data')\n",
    "# set dataset\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join(data_path, dataset)\n",
    "# load train and val data, you can also see the raw data in the data folder\n",
    "# np.memmap: https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\n",
    "# np.memmap: map a file to memory and access it as np.array, it is useful for large data\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "print('train data shape: \\n', train_data.shape)\n",
    "print('train data: \\n', train_data[:10])\n",
    "print('val data shape: \\n', val_data.shape)\n",
    "print('val data: \\n', val_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_keys: \n",
      " ['device', 'vocab_size', 'block_size', 'n_embed', 'n_head', 'dropout', 'b', 't', 'bias', 'path', 'data_path', 'dataset', 'data_dir', 'backend', 'gradient_accumulation_steps', 'batch_size', 'out_dir', 'dtype', 'iter_num', 'best_val_loss', 'always_save_checkpoint', 'log_interval', 'max_iters', 'n_layer', 'compile', 'weight_decay', 'learning_rate', 'decay_lr', 'beta1', 'beta2', 'warmup_iters', 'lr_decay_iters', 'min_lr', 'grad_clip', 'eval_interval', 'eval_iters', 'eval_only']\n",
      "config: \n",
      " {'device': 'cuda', 'vocab_size': 50304, 'block_size': 1024, 'n_embed': 768, 'n_head': 12, 'dropout': 0.0, 'b': 16, 't': 256, 'bias': False, 'path': '/public/share/sd23/d2l/AI_study/LLM/GPT', 'data_path': '/public/share/sd23/d2l/AI_study/LLM/GPT/data', 'dataset': 'shakespeare_char', 'data_dir': '/public/share/sd23/d2l/AI_study/LLM/GPT/data/shakespeare_char', 'backend': 'nccl', 'gradient_accumulation_steps': 40, 'batch_size': 12, 'out_dir': 'results', 'dtype': 'bfloat16', 'iter_num': 0, 'best_val_loss': 1e-09, 'always_save_checkpoint': False, 'log_interval': 1, 'max_iters': 10000, 'n_layer': 12, 'compile': True, 'weight_decay': 0.1, 'learning_rate': 0.0006, 'decay_lr': True, 'beta1': 0.9, 'beta2': 0.95, 'warmup_iters': 2000, 'lr_decay_iters': 10000, 'min_lr': 6e-05, 'grad_clip': 1.0, 'eval_interval': 2000, 'eval_iters': 200, 'eval_only': False}\n"
     ]
    }
   ],
   "source": [
    "#### 3. config setting\n",
    "# -----------------------------------------------------------------------\n",
    "backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "# eight gpus\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 12\n",
    "# seq_len\n",
    "block_size = 1024\n",
    "# out data path\n",
    "out_dir = 'results'\n",
    "# examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "device = 'cuda'\n",
    "# float16 and bfloat16: https://deepinout.com/pytorch/pytorch-questions/8_pytorch_how_to_select_half_precision_bfloat16_vs_float16_for_your_trained_model.html\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "\n",
    "# training config\n",
    "iter_num = 0\n",
    "best_val_loss = 1e-9\n",
    "always_save_checkpoint = False\n",
    "log_interval = 1\n",
    "# this is different from raw project for presentation\n",
    "max_iters = 10000\n",
    "\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embed = 768\n",
    "block_size = 1024\n",
    "# do we use bias inside LayerNorm and Linear layers?\n",
    "bias = False\n",
    "# for pretraining 0 is good, for finetuning try 0.1+\n",
    "dropout = 0.0\n",
    "compile = True\n",
    "\n",
    "# optimizer\n",
    "weight_decay = 1e-1\n",
    "learning_rate = 6e-4\n",
    "decay_lr = True\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "warmup_iters = 2000\n",
    "# this is different from raw project for presentation\n",
    "lr_decay_iters = 10000\n",
    "min_lr = 6e-5\n",
    "# clip gradients at this value, or disable if == 0.0\n",
    "grad_clip = 1.0\n",
    "\n",
    "# evaluate\n",
    "eval_interval = 2000\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "print('config_keys: \\n', config_keys)\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "print('config: \\n', config)\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_accumulation_steps: 40\n",
      "ddp_world_size: 1\n",
      "batch_size: 12\n",
      "block_size: 1024\n",
      "tokens per iter: 491520\n",
      "meta data: /public/share/sd23/d2l/AI_study/LLM/GPT/data/meta.pkl\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 123.59M\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
      "using fused adamW: True\n",
      "Compiling the model>>>\n"
     ]
    }
   ],
   "source": [
    "#### 4. trian process setting\n",
    "\n",
    "# whether there is a ddp run, we can check it using os.environ['RANK'], it will return GPU index\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "\n",
    "if ddp:\n",
    "    # init process group, backend: nccl or gloo or mpi\n",
    "    init_process_group(backend=backend)\n",
    "    # rank is the GPU index, 1 GPU will be 0, 2 GPUs will be 0, 1 on global\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    # local rank is the GPU index on one node, 1 GPU will be 0, 2 GPUs will be 0, 1 on one node\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    # world size is the total number of GPUs * nodes\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device=device)\n",
    "    # # this process will do logging, checkpointing etc.\n",
    "    master_process = ddp_rank == 0\n",
    "    # each process gets a different seed\n",
    "    seed_offset = ddp_rank\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "    \n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f'gradient_accumulation_steps: {gradient_accumulation_steps}')\n",
    "print(f'ddp_world_size: {ddp_world_size}')\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'block_size: {block_size}')\n",
    "print(f'tokens per iter: {tokens_per_iter}')\n",
    "\n",
    "# ddp: 0; non-ddp: True\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(42 + seed_offset)\n",
    "# allow tf32 on matmul\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# allow tf32 on cudnn\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "# for later use in torch.autocast\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "# automatic mixed precison,amp, ofter blend with gradscaler\n",
    "# when we train the model with mixed precison, we may need a gradscaler to shrink gradients to avoid Gradient Underflow\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "print(f'meta data: {meta_path}')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f'found vocab_size = {meta_vocab_size} (inside {meta_path})')\n",
    "\n",
    "# model init\n",
    "model_args = dict(\n",
    "    n_layer = n_layer,\n",
    "    n_head = n_head,\n",
    "    n_embed = n_embed,\n",
    "    block_size = block_size,\n",
    "    bias = bias,\n",
    "    vocab_size = None,\n",
    "    dropout = dropout\n",
    ")\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size=block_size)\n",
    "model.to(device)\n",
    "# unwrap DDP container if needed\n",
    "raw_model = model.module if ddp else model\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizer(\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    betas=(beta1, beta2),\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = None\n",
    "if compile:\n",
    "    print('Compiling the model>>>')\n",
    "    unoptimizer_model = model\n",
    "    # require torch >= 2.0\n",
    "    model = torch.compile(model)\n",
    "    model.to(device)\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([12, 1024]), X: tensor([41, 39, 50, 43,  8,  0,  0, 28, 27, 25], device='cuda:0')\n",
      "Y shape: torch.Size([12, 1024]), Y: tensor([39, 50, 43,  8,  0,  0, 28, 27, 25, 28], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### 4.1 prepare batch\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # sample from range(len(data) - block_size) and the shape is (batch_size,)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    # shift right as label\n",
    "    y = torch.stack([torch.from_numpy((data[i+1 : i + block_size + 1]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device=device, non_blocking = True), y.pin_memory().to(device=device, non_blocking = True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "print(f'X shape: {X.shape}, X: {X[0,:10]}')\n",
    "print(f'Y shape: {Y.shape}, Y: {Y[0,:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 training record\n",
    "t0 = time.time()\n",
    "# number of iterations in the lifetime of this process\n",
    "local_iter_num = 0\n",
    "\n",
    "running_mfu = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 4.3 learning rate settings\n",
    "def get_lr(iter):\n",
    "    # 1. linear warmup for warmup_iters steps\n",
    "    if iter < warmup_iters:\n",
    "        return learning_rate * (iter / warmup_iters)\n",
    "    # 2. set the minimum lr when the iter more than decay iters\n",
    "    if iter > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3. in between, use cosine decay\n",
    "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    # coeff ranges from 0-1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "# show the lr plateau \n",
    "x_list = []\n",
    "y_list = []\n",
    "for i in range(800000):\n",
    "    lr = get_lr(i)\n",
    "    x_list.append(i)\n",
    "    y_list.append(lr)\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9656, val loss 10.9660\n",
      "iter 0: loss 10.9638, time 55536.26ms, mfu -100.00%\n",
      "iter 1: loss 10.9647, time 7416.97ms, mfu -100.00%\n",
      "iter 2: loss 10.9601, time 7821.97ms, mfu -100.00%\n",
      "iter 3: loss 10.9583, time 7846.93ms, mfu -100.00%\n",
      "iter 4: loss 10.9416, time 7848.42ms, mfu -100.00%\n",
      "iter 5: loss 10.9345, time 7869.55ms, mfu 17.11%\n",
      "iter 6: loss 10.9007, time 7923.24ms, mfu 17.10%\n",
      "iter 7: loss 10.8904, time 7965.74ms, mfu 17.08%\n",
      "iter 8: loss 10.8722, time 7954.93ms, mfu 17.07%\n",
      "iter 9: loss 10.8328, time 7916.41ms, mfu 17.06%\n",
      "iter 10: loss 10.7939, time 7898.52ms, mfu 17.06%\n",
      "iter 11: loss 10.7610, time 7889.97ms, mfu 17.06%\n",
      "iter 12: loss 10.7198, time 7857.73ms, mfu 17.07%\n",
      "iter 13: loss 10.6761, time 7867.32ms, mfu 17.07%\n",
      "iter 14: loss 10.6430, time 7896.29ms, mfu 17.07%\n",
      "iter 15: loss 10.5781, time 7947.35ms, mfu 17.06%\n",
      "iter 16: loss 10.5249, time 7992.31ms, mfu 17.04%\n",
      "iter 17: loss 10.4574, time 7995.89ms, mfu 17.02%\n",
      "iter 18: loss 10.3811, time 7972.91ms, mfu 17.00%\n",
      "iter 19: loss 10.3110, time 7948.29ms, mfu 17.00%\n",
      "iter 20: loss 10.2556, time 7915.66ms, mfu 17.00%\n",
      "iter 21: loss 10.1689, time 7919.61ms, mfu 17.00%\n",
      "iter 22: loss 10.0814, time 7943.81ms, mfu 17.00%\n",
      "iter 23: loss 9.9915, time 7982.53ms, mfu 16.98%\n",
      "iter 24: loss 9.8953, time 8066.74ms, mfu 16.95%\n",
      "iter 25: loss 9.8075, time 8190.21ms, mfu 16.90%\n",
      "iter 26: loss 9.6884, time 8234.55ms, mfu 16.85%\n",
      "iter 27: loss 9.6007, time 8242.70ms, mfu 16.80%\n",
      "iter 28: loss 9.4586, time 8978.31ms, mfu 16.62%\n",
      "iter 29: loss 9.3713, time 9083.84ms, mfu 16.44%\n",
      "iter 30: loss 9.2524, time 9146.07ms, mfu 16.27%\n",
      "iter 31: loss 9.1341, time 9062.04ms, mfu 16.13%\n",
      "iter 32: loss 9.0477, time 9012.96ms, mfu 16.01%\n",
      "iter 33: loss 8.9240, time 9237.05ms, mfu 15.86%\n",
      "iter 34: loss 8.7809, time 9158.15ms, mfu 15.75%\n",
      "iter 35: loss 8.7225, time 8647.82ms, mfu 15.73%\n",
      "iter 36: loss 8.5780, time 8508.92ms, mfu 15.74%\n",
      "iter 37: loss 8.4691, time 8511.79ms, mfu 15.75%\n",
      "iter 38: loss 8.4226, time 8357.78ms, mfu 15.78%\n",
      "iter 39: loss 8.3144, time 8249.57ms, mfu 15.84%\n",
      "iter 40: loss 8.2805, time 8183.68ms, mfu 15.90%\n",
      "iter 41: loss 8.1521, time 8247.51ms, mfu 15.94%\n",
      "iter 42: loss 8.1042, time 8213.21ms, mfu 15.99%\n",
      "iter 43: loss 8.0089, time 8193.29ms, mfu 16.03%\n",
      "iter 44: loss 8.0226, time 8165.80ms, mfu 16.08%\n",
      "iter 45: loss 7.8134, time 8197.79ms, mfu 16.11%\n",
      "iter 46: loss 7.8170, time 8186.15ms, mfu 16.15%\n",
      "iter 47: loss 7.7170, time 8171.73ms, mfu 16.18%\n",
      "iter 48: loss 7.6959, time 8175.06ms, mfu 16.21%\n",
      "iter 49: loss 7.5850, time 8147.99ms, mfu 16.24%\n",
      "iter 50: loss 7.5650, time 8161.66ms, mfu 16.27%\n",
      "iter 51: loss 7.4942, time 8098.27ms, mfu 16.30%\n",
      "iter 52: loss 7.5223, time 8097.50ms, mfu 16.34%\n",
      "iter 53: loss 7.3784, time 8068.52ms, mfu 16.37%\n",
      "iter 54: loss 7.4198, time 8043.64ms, mfu 16.41%\n",
      "iter 55: loss 7.3451, time 8041.11ms, mfu 16.44%\n",
      "iter 56: loss 7.2793, time 8042.50ms, mfu 16.47%\n",
      "iter 57: loss 7.2675, time 8041.97ms, mfu 16.50%\n",
      "iter 58: loss 7.2459, time 8036.78ms, mfu 16.52%\n",
      "iter 59: loss 7.2917, time 8023.42ms, mfu 16.55%\n",
      "iter 60: loss 7.1610, time 8011.55ms, mfu 16.58%\n",
      "iter 61: loss 7.0723, time 8015.50ms, mfu 16.60%\n",
      "iter 62: loss 7.0798, time 8015.33ms, mfu 16.62%\n",
      "iter 63: loss 7.0642, time 8015.67ms, mfu 16.64%\n",
      "iter 64: loss 7.0336, time 7999.16ms, mfu 16.66%\n",
      "iter 65: loss 6.9671, time 8000.34ms, mfu 16.67%\n",
      "iter 66: loss 6.9245, time 7999.25ms, mfu 16.69%\n",
      "iter 67: loss 6.9348, time 8006.40ms, mfu 16.70%\n",
      "iter 68: loss 6.9033, time 7998.33ms, mfu 16.72%\n",
      "iter 69: loss 6.9384, time 7996.92ms, mfu 16.73%\n",
      "iter 70: loss 6.8707, time 7997.00ms, mfu 16.74%\n",
      "iter 71: loss 6.7584, time 7994.13ms, mfu 16.75%\n",
      "iter 72: loss 6.7772, time 7995.99ms, mfu 16.76%\n",
      "iter 73: loss 6.7869, time 7987.20ms, mfu 16.77%\n",
      "iter 74: loss 6.6946, time 7988.47ms, mfu 16.78%\n",
      "iter 75: loss 6.6984, time 7992.99ms, mfu 16.78%\n",
      "iter 76: loss 6.6400, time 7989.40ms, mfu 16.79%\n",
      "iter 77: loss 6.5835, time 7987.85ms, mfu 16.80%\n",
      "iter 78: loss 6.6013, time 7987.77ms, mfu 16.80%\n",
      "iter 79: loss 6.5241, time 7987.35ms, mfu 16.81%\n",
      "iter 80: loss 6.5571, time 7989.97ms, mfu 16.81%\n",
      "iter 81: loss 6.5281, time 7991.57ms, mfu 16.82%\n",
      "iter 82: loss 6.4801, time 7966.37ms, mfu 16.83%\n",
      "iter 83: loss 6.4218, time 7916.11ms, mfu 16.84%\n",
      "iter 84: loss 6.4364, time 7909.92ms, mfu 16.86%\n",
      "iter 85: loss 6.4195, time 7915.44ms, mfu 16.88%\n",
      "iter 86: loss 6.3838, time 7910.09ms, mfu 16.89%\n",
      "iter 87: loss 6.2885, time 7896.20ms, mfu 16.91%\n",
      "iter 88: loss 6.2455, time 7899.98ms, mfu 16.92%\n",
      "iter 89: loss 6.2268, time 7903.47ms, mfu 16.93%\n",
      "iter 90: loss 6.1826, time 7891.55ms, mfu 16.95%\n",
      "iter 91: loss 6.2157, time 7898.66ms, mfu 16.96%\n",
      "iter 92: loss 6.1195, time 7901.58ms, mfu 16.97%\n",
      "iter 93: loss 6.0972, time 7891.45ms, mfu 16.98%\n",
      "iter 94: loss 6.1208, time 7903.92ms, mfu 16.98%\n",
      "iter 95: loss 5.9848, time 7925.36ms, mfu 16.98%\n",
      "iter 96: loss 5.9520, time 7964.10ms, mfu 16.97%\n",
      "iter 97: loss 5.9382, time 8086.26ms, mfu 16.94%\n",
      "iter 98: loss 5.8882, time 8211.67ms, mfu 16.89%\n",
      "iter 99: loss 5.9081, time 8404.45ms, mfu 16.80%\n",
      "iter 100: loss 5.8504, time 8183.18ms, mfu 16.77%\n",
      "iter 101: loss 5.7568, time 8195.24ms, mfu 16.73%\n",
      "iter 102: loss 5.7975, time 8482.71ms, mfu 16.65%\n",
      "iter 103: loss 5.7065, time 8510.99ms, mfu 16.57%\n",
      "iter 104: loss 5.6683, time 8179.35ms, mfu 16.55%\n",
      "iter 105: loss 5.6263, time 8173.65ms, mfu 16.55%\n",
      "iter 106: loss 5.6764, time 8172.69ms, mfu 16.54%\n",
      "iter 107: loss 5.5516, time 8243.56ms, mfu 16.52%\n",
      "iter 108: loss 5.5668, time 8194.46ms, mfu 16.51%\n",
      "iter 109: loss 5.5072, time 8187.92ms, mfu 16.50%\n",
      "iter 110: loss 5.3839, time 8612.03ms, mfu 16.42%\n",
      "iter 111: loss 5.3889, time 8434.14ms, mfu 16.37%\n",
      "iter 112: loss 5.3477, time 8194.00ms, mfu 16.38%\n",
      "iter 113: loss 5.3296, time 8053.81ms, mfu 16.41%\n",
      "iter 114: loss 5.3503, time 8009.05ms, mfu 16.45%\n",
      "iter 115: loss 5.3048, time 7995.77ms, mfu 16.49%\n",
      "iter 116: loss 5.2812, time 7982.80ms, mfu 16.53%\n",
      "iter 117: loss 5.1984, time 7969.83ms, mfu 16.57%\n",
      "iter 118: loss 5.1517, time 7964.27ms, mfu 16.60%\n",
      "iter 119: loss 5.1713, time 7959.03ms, mfu 16.63%\n",
      "iter 120: loss 5.1292, time 7951.86ms, mfu 16.66%\n",
      "iter 121: loss 5.0691, time 7954.27ms, mfu 16.69%\n",
      "iter 122: loss 4.9745, time 7944.06ms, mfu 16.72%\n",
      "iter 123: loss 4.9155, time 7903.01ms, mfu 16.75%\n",
      "iter 124: loss 4.8998, time 7877.75ms, mfu 16.78%\n",
      "iter 125: loss 4.9117, time 7866.03ms, mfu 16.82%\n",
      "iter 126: loss 4.8143, time 7831.25ms, mfu 16.85%\n",
      "iter 127: loss 4.8526, time 7827.63ms, mfu 16.89%\n",
      "iter 128: loss 4.7549, time 7830.69ms, mfu 16.92%\n",
      "iter 129: loss 4.6775, time 7836.47ms, mfu 16.95%\n",
      "iter 130: loss 4.7230, time 7845.45ms, mfu 16.97%\n",
      "iter 131: loss 4.5953, time 7880.92ms, mfu 16.98%\n",
      "iter 132: loss 4.6279, time 7930.36ms, mfu 16.98%\n",
      "iter 133: loss 4.5854, time 8002.19ms, mfu 16.96%\n",
      "iter 134: loss 4.5020, time 8161.86ms, mfu 16.92%\n",
      "iter 135: loss 4.4691, time 8258.33ms, mfu 16.86%\n",
      "iter 136: loss 4.4439, time 8190.16ms, mfu 16.82%\n",
      "iter 137: loss 4.3654, time 8750.70ms, mfu 16.67%\n",
      "iter 138: loss 4.3877, time 8876.09ms, mfu 16.52%\n",
      "iter 139: loss 4.2965, time 9218.44ms, mfu 16.33%\n",
      "iter 140: loss 4.2582, time 9035.04ms, mfu 16.19%\n",
      "iter 141: loss 4.2729, time 9206.77ms, mfu 16.03%\n",
      "iter 142: loss 4.2359, time 9209.57ms, mfu 15.89%\n",
      "iter 143: loss 4.1682, time 9077.15ms, mfu 15.79%\n",
      "iter 144: loss 4.1853, time 9065.97ms, mfu 15.69%\n",
      "iter 145: loss 4.1502, time 8801.01ms, mfu 15.65%\n",
      "iter 146: loss 4.0813, time 8748.00ms, mfu 15.63%\n",
      "iter 147: loss 4.1053, time 8705.90ms, mfu 15.61%\n",
      "iter 148: loss 4.0909, time 8416.70ms, mfu 15.65%\n",
      "iter 149: loss 4.0256, time 8188.50ms, mfu 15.73%\n",
      "iter 150: loss 3.9594, time 8195.95ms, mfu 15.80%\n",
      "iter 151: loss 3.9400, time 8211.18ms, mfu 15.86%\n",
      "iter 152: loss 3.9124, time 8131.45ms, mfu 15.93%\n",
      "iter 153: loss 3.9063, time 8041.90ms, mfu 16.01%\n",
      "iter 154: loss 3.8790, time 8008.03ms, mfu 16.09%\n",
      "iter 155: loss 3.8267, time 7987.16ms, mfu 16.17%\n",
      "iter 156: loss 3.8304, time 7976.96ms, mfu 16.24%\n",
      "iter 157: loss 3.7864, time 7967.93ms, mfu 16.31%\n",
      "iter 158: loss 3.7723, time 7957.74ms, mfu 16.37%\n",
      "iter 159: loss 3.7378, time 7948.37ms, mfu 16.42%\n",
      "iter 160: loss 3.8052, time 7923.37ms, mfu 16.48%\n",
      "iter 161: loss 3.7439, time 7920.10ms, mfu 16.53%\n",
      "iter 162: loss 3.7074, time 7917.18ms, mfu 16.58%\n",
      "iter 163: loss 3.6610, time 7919.17ms, mfu 16.62%\n",
      "iter 164: loss 3.6578, time 7921.11ms, mfu 16.66%\n",
      "iter 165: loss 3.6249, time 7926.77ms, mfu 16.69%\n",
      "iter 166: loss 3.5854, time 7927.36ms, mfu 16.72%\n",
      "iter 167: loss 3.5268, time 7931.92ms, mfu 16.75%\n",
      "iter 168: loss 3.5456, time 7940.47ms, mfu 16.77%\n",
      "iter 169: loss 3.5522, time 7944.22ms, mfu 16.79%\n",
      "iter 170: loss 3.5510, time 7949.22ms, mfu 16.80%\n",
      "iter 171: loss 3.5205, time 7947.90ms, mfu 16.82%\n",
      "iter 172: loss 3.4872, time 7921.56ms, mfu 16.84%\n",
      "iter 173: loss 3.4750, time 7912.76ms, mfu 16.85%\n",
      "iter 174: loss 3.4720, time 7907.05ms, mfu 16.87%\n",
      "iter 175: loss 3.4705, time 7917.77ms, mfu 16.88%\n",
      "iter 176: loss 3.4604, time 7930.98ms, mfu 16.89%\n",
      "iter 177: loss 3.3972, time 7958.61ms, mfu 16.90%\n",
      "iter 178: loss 3.4586, time 7996.22ms, mfu 16.89%\n",
      "iter 179: loss 3.3930, time 8106.23ms, mfu 16.86%\n",
      "iter 180: loss 3.4011, time 8226.16ms, mfu 16.81%\n",
      "iter 181: loss 3.4322, time 8266.15ms, mfu 16.76%\n",
      "iter 182: loss 3.4388, time 8237.55ms, mfu 16.72%\n",
      "iter 183: loss 3.4220, time 8207.71ms, mfu 16.69%\n",
      "iter 184: loss 3.3772, time 8182.27ms, mfu 16.67%\n",
      "iter 185: loss 3.4818, time 8198.64ms, mfu 16.64%\n",
      "iter 186: loss 3.4311, time 8194.45ms, mfu 16.62%\n",
      "iter 187: loss 3.3594, time 8393.09ms, mfu 16.56%\n",
      "iter 188: loss 3.3712, time 8152.84ms, mfu 16.56%\n",
      "iter 189: loss 3.3750, time 8036.35ms, mfu 16.58%\n",
      "iter 190: loss 3.3723, time 7998.08ms, mfu 16.60%\n",
      "iter 191: loss 3.3673, time 7957.68ms, mfu 16.64%\n",
      "iter 192: loss 3.3926, time 7911.17ms, mfu 16.67%\n",
      "iter 193: loss 3.3972, time 7898.15ms, mfu 16.71%\n",
      "iter 194: loss 3.3511, time 7893.92ms, mfu 16.75%\n",
      "iter 195: loss 3.3295, time 7902.13ms, mfu 16.78%\n",
      "iter 196: loss 3.3399, time 7914.19ms, mfu 16.80%\n",
      "iter 197: loss 3.3490, time 7930.74ms, mfu 16.82%\n",
      "iter 198: loss 3.3313, time 7946.86ms, mfu 16.83%\n",
      "iter 199: loss 3.3027, time 7982.66ms, mfu 16.83%\n",
      "iter 200: loss 3.3323, time 8082.69ms, mfu 16.82%\n",
      "iter 201: loss 3.3633, time 8203.08ms, mfu 16.78%\n",
      "iter 202: loss 3.3868, time 8194.68ms, mfu 16.74%\n",
      "iter 203: loss 3.3297, time 8192.53ms, mfu 16.71%\n",
      "iter 204: loss 3.3949, time 8201.16ms, mfu 16.68%\n",
      "iter 205: loss 3.3551, time 8634.00ms, mfu 16.57%\n",
      "iter 206: loss 3.3400, time 8290.74ms, mfu 16.54%\n",
      "iter 207: loss 3.2984, time 8096.76ms, mfu 16.55%\n",
      "iter 208: loss 3.3691, time 8017.99ms, mfu 16.57%\n",
      "iter 209: loss 3.3212, time 7989.76ms, mfu 16.60%\n",
      "iter 210: loss 3.3490, time 7976.24ms, mfu 16.63%\n",
      "iter 211: loss 3.3237, time 7963.45ms, mfu 16.66%\n",
      "iter 212: loss 3.2987, time 7958.59ms, mfu 16.68%\n",
      "iter 213: loss 3.3112, time 7950.29ms, mfu 16.71%\n",
      "iter 214: loss 3.3484, time 7922.47ms, mfu 16.74%\n",
      "iter 215: loss 3.2861, time 7917.70ms, mfu 16.77%\n",
      "iter 216: loss 3.3278, time 7917.52ms, mfu 16.79%\n",
      "iter 217: loss 3.3452, time 7907.34ms, mfu 16.81%\n",
      "iter 218: loss 3.3173, time 7887.59ms, mfu 16.84%\n",
      "iter 219: loss 3.3365, time 7876.57ms, mfu 16.87%\n",
      "iter 220: loss 3.2702, time 7874.92ms, mfu 16.89%\n",
      "iter 221: loss 3.3411, time 7887.46ms, mfu 16.91%\n",
      "iter 222: loss 3.3325, time 7906.50ms, mfu 16.92%\n",
      "iter 223: loss 3.3432, time 7931.53ms, mfu 16.93%\n",
      "iter 224: loss 3.3162, time 7981.87ms, mfu 16.92%\n",
      "iter 225: loss 3.3530, time 8089.34ms, mfu 16.89%\n",
      "iter 226: loss 3.2957, time 8091.30ms, mfu 16.87%\n",
      "iter 227: loss 3.3786, time 7968.72ms, mfu 16.87%\n",
      "iter 228: loss 3.3254, time 7948.19ms, mfu 16.88%\n",
      "iter 229: loss 3.3162, time 7932.78ms, mfu 16.89%\n",
      "iter 230: loss 3.3217, time 7924.32ms, mfu 16.90%\n",
      "iter 231: loss 3.3278, time 7933.91ms, mfu 16.91%\n",
      "iter 232: loss 3.3331, time 7939.71ms, mfu 16.91%\n",
      "iter 233: loss 3.3365, time 7958.55ms, mfu 16.91%\n",
      "iter 234: loss 3.3148, time 8003.53ms, mfu 16.90%\n",
      "iter 235: loss 3.3198, time 8150.41ms, mfu 16.87%\n",
      "iter 236: loss 3.2895, time 8254.03ms, mfu 16.81%\n",
      "iter 237: loss 3.3308, time 8298.54ms, mfu 16.75%\n",
      "iter 238: loss 3.2842, time 8196.08ms, mfu 16.72%\n",
      "iter 239: loss 3.3184, time 8843.13ms, mfu 16.57%\n",
      "iter 240: loss 3.3781, time 8541.95ms, mfu 16.49%\n",
      "iter 241: loss 3.3556, time 8054.43ms, mfu 16.51%\n",
      "iter 242: loss 3.3326, time 7967.15ms, mfu 16.55%\n",
      "iter 243: loss 3.3046, time 7968.41ms, mfu 16.59%\n",
      "iter 244: loss 3.3290, time 7991.15ms, mfu 16.61%\n",
      "iter 245: loss 3.3104, time 8179.63ms, mfu 16.60%\n",
      "iter 246: loss 3.3133, time 8202.30ms, mfu 16.58%\n",
      "iter 247: loss 3.3289, time 8185.88ms, mfu 16.57%\n",
      "iter 248: loss 3.3354, time 8911.28ms, mfu 16.42%\n",
      "iter 249: loss 3.3009, time 9133.31ms, mfu 16.25%\n",
      "iter 250: loss 3.2456, time 9299.77ms, mfu 16.08%\n",
      "iter 251: loss 3.2857, time 9416.61ms, mfu 15.90%\n",
      "iter 252: loss 3.3336, time 9471.78ms, mfu 15.73%\n",
      "iter 253: loss 3.3265, time 9403.96ms, mfu 15.59%\n",
      "iter 254: loss 3.3387, time 9337.76ms, mfu 15.47%\n",
      "iter 255: loss 3.3449, time 8975.74ms, mfu 15.43%\n",
      "iter 256: loss 3.2921, time 8570.16ms, mfu 15.45%\n",
      "iter 257: loss 3.3473, time 8190.74ms, mfu 15.55%\n",
      "iter 258: loss 3.3315, time 8063.27ms, mfu 15.67%\n",
      "iter 259: loss 3.2975, time 8019.54ms, mfu 15.78%\n",
      "iter 260: loss 3.2922, time 7994.60ms, mfu 15.89%\n",
      "iter 261: loss 3.3783, time 7975.82ms, mfu 15.99%\n",
      "iter 262: loss 3.3214, time 7921.24ms, mfu 16.09%\n",
      "iter 263: loss 3.2685, time 7901.06ms, mfu 16.18%\n",
      "iter 264: loss 3.3183, time 7888.74ms, mfu 16.27%\n",
      "iter 265: loss 3.3309, time 7869.53ms, mfu 16.36%\n",
      "iter 266: loss 3.2683, time 7864.37ms, mfu 16.43%\n",
      "iter 267: loss 3.3093, time 7874.56ms, mfu 16.50%\n",
      "iter 268: loss 3.3072, time 7898.01ms, mfu 16.55%\n",
      "iter 269: loss 3.2864, time 7932.44ms, mfu 16.60%\n",
      "iter 270: loss 3.2727, time 7995.40ms, mfu 16.62%\n",
      "iter 271: loss 3.2601, time 8218.36ms, mfu 16.60%\n",
      "iter 272: loss 3.2739, time 8415.43ms, mfu 16.54%\n",
      "iter 273: loss 3.2955, time 8213.08ms, mfu 16.52%\n",
      "iter 274: loss 3.3418, time 9078.39ms, mfu 16.35%\n",
      "iter 275: loss 3.3348, time 8309.82ms, mfu 16.34%\n",
      "iter 276: loss 3.2792, time 8189.53ms, mfu 16.35%\n",
      "iter 277: loss 3.2717, time 8390.59ms, mfu 16.32%\n",
      "iter 278: loss 3.2830, time 8393.97ms, mfu 16.29%\n",
      "iter 279: loss 3.2626, time 8364.29ms, mfu 16.27%\n",
      "iter 280: loss 3.2865, time 8206.05ms, mfu 16.29%\n",
      "iter 281: loss 3.2677, time 8149.26ms, mfu 16.31%\n",
      "iter 282: loss 3.2893, time 8055.66ms, mfu 16.35%\n",
      "iter 283: loss 3.2568, time 8021.01ms, mfu 16.39%\n",
      "iter 284: loss 3.2930, time 8005.34ms, mfu 16.44%\n",
      "iter 285: loss 3.2643, time 7992.42ms, mfu 16.48%\n",
      "iter 286: loss 3.2870, time 7983.40ms, mfu 16.52%\n",
      "iter 287: loss 3.2409, time 7972.62ms, mfu 16.55%\n",
      "iter 288: loss 3.2650, time 7968.40ms, mfu 16.59%\n",
      "iter 289: loss 3.2840, time 7971.53ms, mfu 16.62%\n",
      "iter 290: loss 3.2859, time 7965.53ms, mfu 16.65%\n",
      "iter 291: loss 3.2569, time 7964.69ms, mfu 16.67%\n",
      "iter 292: loss 3.2224, time 7967.89ms, mfu 16.70%\n",
      "iter 293: loss 3.2218, time 7965.69ms, mfu 16.72%\n",
      "iter 294: loss 3.2427, time 7926.32ms, mfu 16.74%\n",
      "iter 295: loss 3.2222, time 7910.51ms, mfu 16.77%\n",
      "iter 296: loss 3.2223, time 7904.38ms, mfu 16.80%\n",
      "iter 297: loss 3.1843, time 7905.25ms, mfu 16.82%\n",
      "iter 298: loss 3.2587, time 7917.15ms, mfu 16.84%\n",
      "iter 299: loss 3.1847, time 7935.53ms, mfu 16.85%\n",
      "iter 300: loss 3.1695, time 7950.75ms, mfu 16.86%\n",
      "iter 301: loss 3.1871, time 7984.93ms, mfu 16.86%\n",
      "iter 302: loss 3.2174, time 8066.37ms, mfu 16.85%\n",
      "iter 303: loss 3.2165, time 8165.25ms, mfu 16.81%\n",
      "iter 304: loss 3.2280, time 8174.67ms, mfu 16.78%\n",
      "iter 305: loss 3.2213, time 8238.74ms, mfu 16.73%\n",
      "iter 306: loss 3.2694, time 8254.89ms, mfu 16.69%\n",
      "iter 307: loss 3.3004, time 8181.54ms, mfu 16.67%\n",
      "iter 308: loss 3.1751, time 8557.77ms, mfu 16.57%\n",
      "iter 309: loss 3.2480, time 8444.03ms, mfu 16.51%\n",
      "iter 310: loss 3.2963, time 8356.66ms, mfu 16.47%\n",
      "iter 311: loss 3.2046, time 8220.89ms, mfu 16.46%\n",
      "iter 312: loss 3.1395, time 8153.10ms, mfu 16.47%\n",
      "iter 313: loss 3.2318, time 8014.99ms, mfu 16.50%\n",
      "iter 314: loss 3.2950, time 7970.41ms, mfu 16.54%\n",
      "iter 315: loss 3.2456, time 7955.28ms, mfu 16.58%\n",
      "iter 316: loss 3.1387, time 7922.33ms, mfu 16.62%\n",
      "iter 317: loss 3.2107, time 7892.83ms, mfu 16.67%\n",
      "iter 318: loss 3.2450, time 7890.41ms, mfu 16.71%\n",
      "iter 319: loss 3.1928, time 7889.16ms, mfu 16.74%\n",
      "iter 320: loss 3.1766, time 7881.26ms, mfu 16.78%\n",
      "iter 321: loss 3.1620, time 7893.20ms, mfu 16.80%\n",
      "iter 322: loss 3.2163, time 7901.25ms, mfu 16.83%\n",
      "iter 323: loss 3.1761, time 7894.41ms, mfu 16.85%\n",
      "iter 324: loss 3.2343, time 7898.41ms, mfu 16.87%\n",
      "iter 325: loss 3.1578, time 7921.42ms, mfu 16.88%\n",
      "iter 326: loss 3.1654, time 7971.98ms, mfu 16.88%\n",
      "iter 327: loss 3.1984, time 8140.40ms, mfu 16.85%\n",
      "iter 328: loss 3.1807, time 8196.43ms, mfu 16.81%\n",
      "iter 329: loss 3.1483, time 8190.21ms, mfu 16.77%\n",
      "iter 330: loss 3.1227, time 8574.44ms, mfu 16.66%\n",
      "iter 331: loss 3.1368, time 8422.04ms, mfu 16.60%\n",
      "iter 332: loss 3.1337, time 8725.95ms, mfu 16.48%\n",
      "iter 333: loss 3.1222, time 8967.39ms, mfu 16.33%\n",
      "iter 334: loss 3.1570, time 8714.73ms, mfu 16.25%\n",
      "iter 335: loss 3.1238, time 8700.83ms, mfu 16.17%\n",
      "iter 336: loss 3.1033, time 9063.98ms, mfu 16.04%\n",
      "iter 337: loss 3.1113, time 8897.72ms, mfu 15.95%\n",
      "iter 338: loss 3.0726, time 9010.04ms, mfu 15.85%\n",
      "iter 339: loss 3.0941, time 9147.25ms, mfu 15.73%\n",
      "iter 340: loss 3.1053, time 9038.05ms, mfu 15.65%\n",
      "iter 341: loss 3.1219, time 8815.11ms, mfu 15.61%\n",
      "iter 342: loss 3.0946, time 8825.94ms, mfu 15.58%\n",
      "iter 343: loss 3.1250, time 8698.50ms, mfu 15.57%\n",
      "iter 344: loss 3.1165, time 8493.43ms, mfu 15.60%\n",
      "iter 345: loss 3.0282, time 8188.68ms, mfu 15.68%\n",
      "iter 346: loss 3.1025, time 8267.86ms, mfu 15.74%\n",
      "iter 347: loss 3.0605, time 8134.17ms, mfu 15.82%\n",
      "iter 348: loss 3.0242, time 8059.07ms, mfu 15.91%\n",
      "iter 349: loss 3.0358, time 8031.23ms, mfu 16.00%\n",
      "iter 350: loss 3.0657, time 8017.82ms, mfu 16.08%\n",
      "iter 351: loss 3.0498, time 8017.67ms, mfu 16.15%\n",
      "iter 352: loss 3.0659, time 8010.81ms, mfu 16.22%\n",
      "iter 353: loss 3.0336, time 8003.81ms, mfu 16.28%\n",
      "iter 354: loss 2.9981, time 8004.29ms, mfu 16.33%\n",
      "iter 355: loss 2.9973, time 8006.42ms, mfu 16.38%\n",
      "iter 356: loss 3.0107, time 7997.29ms, mfu 16.43%\n",
      "iter 357: loss 3.0303, time 7994.92ms, mfu 16.47%\n",
      "iter 358: loss 2.9827, time 7994.41ms, mfu 16.50%\n",
      "iter 359: loss 3.0174, time 7989.75ms, mfu 16.54%\n",
      "iter 360: loss 2.9899, time 7993.12ms, mfu 16.57%\n",
      "iter 361: loss 2.9980, time 7993.38ms, mfu 16.60%\n",
      "iter 362: loss 2.9714, time 7996.03ms, mfu 16.62%\n",
      "iter 363: loss 2.9527, time 7994.62ms, mfu 16.64%\n",
      "iter 364: loss 3.0012, time 7988.40ms, mfu 16.67%\n",
      "iter 365: loss 2.9767, time 7994.96ms, mfu 16.68%\n",
      "iter 366: loss 2.9520, time 7998.18ms, mfu 16.70%\n",
      "iter 367: loss 2.9346, time 7997.79ms, mfu 16.71%\n",
      "iter 368: loss 2.9304, time 7997.77ms, mfu 16.73%\n",
      "iter 369: loss 2.9509, time 7996.37ms, mfu 16.74%\n",
      "iter 370: loss 2.9695, time 7998.30ms, mfu 16.75%\n",
      "iter 371: loss 3.0135, time 7997.17ms, mfu 16.76%\n",
      "iter 372: loss 2.9487, time 7996.77ms, mfu 16.76%\n",
      "iter 373: loss 3.0022, time 7997.66ms, mfu 16.77%\n",
      "iter 374: loss 2.9980, time 7998.01ms, mfu 16.78%\n",
      "iter 375: loss 2.9463, time 7996.99ms, mfu 16.78%\n",
      "iter 376: loss 2.9592, time 7997.01ms, mfu 16.79%\n",
      "iter 377: loss 2.9548, time 7998.26ms, mfu 16.79%\n",
      "iter 378: loss 2.9684, time 7994.10ms, mfu 16.80%\n",
      "iter 379: loss 2.9515, time 7957.56ms, mfu 16.81%\n",
      "iter 380: loss 2.9355, time 7919.39ms, mfu 16.83%\n",
      "iter 381: loss 2.9139, time 7904.11ms, mfu 16.85%\n",
      "iter 382: loss 2.9597, time 7887.50ms, mfu 16.87%\n",
      "iter 383: loss 2.9114, time 7883.55ms, mfu 16.89%\n",
      "iter 384: loss 2.9072, time 7885.35ms, mfu 16.91%\n",
      "iter 385: loss 2.8965, time 7879.94ms, mfu 16.93%\n",
      "iter 386: loss 2.9051, time 7882.07ms, mfu 16.95%\n",
      "iter 387: loss 2.9197, time 7890.29ms, mfu 16.96%\n",
      "iter 388: loss 2.9371, time 7884.28ms, mfu 16.97%\n",
      "iter 389: loss 2.9336, time 7894.10ms, mfu 16.98%\n",
      "iter 390: loss 2.9331, time 7908.75ms, mfu 16.98%\n",
      "iter 391: loss 2.9391, time 7893.55ms, mfu 16.99%\n",
      "iter 392: loss 2.9559, time 7890.99ms, mfu 17.00%\n",
      "iter 393: loss 2.9249, time 7895.12ms, mfu 17.00%\n",
      "iter 394: loss 2.9285, time 7885.39ms, mfu 17.01%\n",
      "iter 395: loss 2.8977, time 7884.86ms, mfu 17.02%\n",
      "iter 396: loss 2.8784, time 7889.40ms, mfu 17.02%\n",
      "iter 397: loss 2.8938, time 7884.16ms, mfu 17.03%\n",
      "iter 398: loss 2.9003, time 7891.61ms, mfu 17.03%\n",
      "iter 399: loss 2.9588, time 7907.64ms, mfu 17.03%\n",
      "iter 400: loss 2.8741, time 7908.89ms, mfu 17.03%\n",
      "iter 401: loss 2.8979, time 7904.56ms, mfu 17.03%\n",
      "iter 402: loss 2.8992, time 7918.09ms, mfu 17.03%\n",
      "iter 403: loss 2.9344, time 7943.55ms, mfu 17.02%\n",
      "iter 404: loss 2.8988, time 8017.54ms, mfu 17.00%\n",
      "iter 405: loss 2.8723, time 8160.19ms, mfu 16.95%\n",
      "iter 406: loss 2.8723, time 8238.70ms, mfu 16.89%\n",
      "iter 407: loss 2.8611, time 8252.18ms, mfu 16.83%\n",
      "iter 408: loss 2.8208, time 8188.93ms, mfu 16.79%\n",
      "iter 409: loss 2.8671, time 9230.47ms, mfu 16.57%\n",
      "iter 410: loss 2.8827, time 8776.24ms, mfu 16.45%\n",
      "iter 411: loss 2.8390, time 8745.28ms, mfu 16.34%\n",
      "iter 412: loss 2.8348, time 8749.14ms, mfu 16.25%\n",
      "iter 413: loss 2.8460, time 8325.63ms, mfu 16.24%\n",
      "iter 414: loss 2.8312, time 8715.46ms, mfu 16.16%\n",
      "iter 415: loss 2.8488, time 8603.09ms, mfu 16.11%\n",
      "iter 416: loss 2.8257, time 8484.10ms, mfu 16.09%\n",
      "iter 417: loss 2.8471, time 8380.70ms, mfu 16.09%\n",
      "iter 418: loss 2.8424, time 8208.73ms, mfu 16.12%\n",
      "iter 419: loss 2.8148, time 8174.86ms, mfu 16.15%\n",
      "iter 420: loss 2.8434, time 8138.90ms, mfu 16.19%\n",
      "iter 421: loss 2.8122, time 8085.49ms, mfu 16.24%\n",
      "iter 422: loss 2.8500, time 8058.20ms, mfu 16.29%\n",
      "iter 423: loss 2.8278, time 8046.42ms, mfu 16.33%\n",
      "iter 424: loss 2.8225, time 8021.58ms, mfu 16.38%\n",
      "iter 425: loss 2.8111, time 8012.11ms, mfu 16.42%\n",
      "iter 426: loss 2.8229, time 8013.23ms, mfu 16.46%\n",
      "iter 427: loss 2.8442, time 8014.93ms, mfu 16.49%\n",
      "iter 428: loss 2.8197, time 8007.20ms, mfu 16.52%\n",
      "iter 429: loss 2.7843, time 8008.15ms, mfu 16.55%\n",
      "iter 430: loss 2.7969, time 8009.96ms, mfu 16.58%\n",
      "iter 431: loss 2.8121, time 8004.94ms, mfu 16.60%\n",
      "iter 432: loss 2.7589, time 7999.63ms, mfu 16.63%\n",
      "iter 433: loss 2.7943, time 8012.02ms, mfu 16.64%\n",
      "iter 434: loss 2.7836, time 8000.28ms, mfu 16.66%\n",
      "iter 435: loss 2.7736, time 7997.12ms, mfu 16.68%\n",
      "iter 436: loss 2.7990, time 7996.78ms, mfu 16.70%\n",
      "iter 437: loss 2.7323, time 7998.18ms, mfu 16.71%\n",
      "iter 438: loss 2.7699, time 7996.85ms, mfu 16.72%\n",
      "iter 439: loss 2.7371, time 7997.68ms, mfu 16.73%\n",
      "iter 440: loss 2.7373, time 7996.86ms, mfu 16.75%\n",
      "iter 441: loss 2.7891, time 7998.07ms, mfu 16.75%\n",
      "iter 442: loss 2.7453, time 7997.02ms, mfu 16.76%\n",
      "iter 443: loss 2.7629, time 7997.43ms, mfu 16.77%\n",
      "iter 444: loss 2.7692, time 7998.33ms, mfu 16.78%\n",
      "iter 445: loss 2.6922, time 7997.16ms, mfu 16.78%\n",
      "iter 446: loss 2.7349, time 7997.16ms, mfu 16.79%\n",
      "iter 447: loss 2.7354, time 7997.43ms, mfu 16.79%\n",
      "iter 448: loss 2.7380, time 7997.44ms, mfu 16.80%\n",
      "iter 449: loss 2.7319, time 7997.61ms, mfu 16.80%\n",
      "iter 450: loss 2.7637, time 7997.10ms, mfu 16.81%\n",
      "iter 451: loss 2.7803, time 7999.24ms, mfu 16.81%\n",
      "iter 452: loss 2.7415, time 8000.19ms, mfu 16.81%\n",
      "iter 453: loss 2.6959, time 8003.49ms, mfu 16.81%\n",
      "iter 454: loss 2.7337, time 8006.77ms, mfu 16.81%\n",
      "iter 455: loss 2.7383, time 7997.06ms, mfu 16.82%\n",
      "iter 456: loss 2.7173, time 7998.07ms, mfu 16.82%\n",
      "iter 457: loss 2.7061, time 7998.80ms, mfu 16.82%\n",
      "iter 458: loss 2.7331, time 8006.66ms, mfu 16.82%\n",
      "iter 459: loss 2.7120, time 8000.95ms, mfu 16.82%\n",
      "iter 460: loss 2.7405, time 7951.98ms, mfu 16.83%\n",
      "iter 461: loss 2.7329, time 7923.47ms, mfu 16.85%\n",
      "iter 462: loss 2.6913, time 7921.59ms, mfu 16.86%\n",
      "iter 463: loss 2.6830, time 7934.68ms, mfu 16.87%\n",
      "iter 464: loss 2.6999, time 7947.77ms, mfu 16.88%\n",
      "iter 465: loss 2.6816, time 7974.15ms, mfu 16.88%\n",
      "iter 466: loss 2.7192, time 7996.36ms, mfu 16.88%\n",
      "iter 467: loss 2.7372, time 8089.52ms, mfu 16.85%\n",
      "iter 468: loss 2.6861, time 8201.07ms, mfu 16.81%\n",
      "iter 469: loss 2.7179, time 8263.77ms, mfu 16.76%\n",
      "iter 470: loss 2.6775, time 8188.99ms, mfu 16.73%\n",
      "iter 471: loss 2.7167, time 8539.35ms, mfu 16.63%\n",
      "iter 472: loss 2.6887, time 8254.74ms, mfu 16.60%\n",
      "iter 473: loss 2.6763, time 8406.91ms, mfu 16.54%\n",
      "iter 474: loss 2.6742, time 8183.14ms, mfu 16.53%\n",
      "iter 475: loss 2.6893, time 8109.86ms, mfu 16.54%\n",
      "iter 476: loss 2.6777, time 8038.75ms, mfu 16.56%\n",
      "iter 477: loss 2.6957, time 8015.28ms, mfu 16.59%\n",
      "iter 478: loss 2.6762, time 7999.05ms, mfu 16.61%\n",
      "iter 479: loss 2.7037, time 7969.45ms, mfu 16.64%\n",
      "iter 480: loss 2.7019, time 7949.31ms, mfu 16.67%\n",
      "iter 481: loss 2.6560, time 7940.67ms, mfu 16.70%\n",
      "iter 482: loss 2.6656, time 7932.94ms, mfu 16.73%\n",
      "iter 483: loss 2.6963, time 7926.60ms, mfu 16.75%\n",
      "iter 484: loss 2.6578, time 7924.61ms, mfu 16.78%\n",
      "iter 485: loss 2.6605, time 7924.58ms, mfu 16.80%\n",
      "iter 486: loss 2.6741, time 7925.49ms, mfu 16.82%\n",
      "iter 487: loss 2.6739, time 7926.33ms, mfu 16.83%\n",
      "iter 488: loss 2.6624, time 7932.19ms, mfu 16.85%\n",
      "iter 489: loss 2.6637, time 7939.79ms, mfu 16.86%\n",
      "iter 490: loss 2.6513, time 7952.71ms, mfu 16.87%\n",
      "iter 491: loss 2.6803, time 7960.16ms, mfu 16.87%\n",
      "iter 492: loss 2.6534, time 7965.25ms, mfu 16.88%\n",
      "iter 493: loss 2.6408, time 7954.02ms, mfu 16.88%\n",
      "iter 494: loss 2.6557, time 7925.80ms, mfu 16.89%\n",
      "iter 495: loss 2.6480, time 7924.71ms, mfu 16.90%\n",
      "iter 496: loss 2.6671, time 7932.37ms, mfu 16.91%\n",
      "iter 497: loss 2.6589, time 7950.78ms, mfu 16.91%\n",
      "iter 498: loss 2.6434, time 7980.42ms, mfu 16.91%\n",
      "iter 499: loss 2.6611, time 8006.11ms, mfu 16.90%\n",
      "iter 500: loss 2.6901, time 8123.38ms, mfu 16.87%\n",
      "iter 501: loss 2.6466, time 8317.48ms, mfu 16.80%\n",
      "iter 502: loss 2.6589, time 8191.60ms, mfu 16.76%\n",
      "iter 503: loss 2.6738, time 9006.11ms, mfu 16.58%\n",
      "iter 504: loss 2.6619, time 8902.28ms, mfu 16.44%\n",
      "iter 505: loss 2.6518, time 8741.53ms, mfu 16.33%\n",
      "iter 506: loss 2.6293, time 8717.29ms, mfu 16.24%\n",
      "iter 507: loss 2.6269, time 8532.52ms, mfu 16.20%\n",
      "iter 508: loss 2.6683, time 8212.51ms, mfu 16.22%\n",
      "iter 509: loss 2.6567, time 8136.47ms, mfu 16.25%\n",
      "iter 510: loss 2.6542, time 8047.01ms, mfu 16.30%\n",
      "iter 511: loss 2.6431, time 8017.68ms, mfu 16.35%\n",
      "iter 512: loss 2.6315, time 8007.69ms, mfu 16.40%\n",
      "iter 513: loss 2.6626, time 7996.08ms, mfu 16.44%\n",
      "iter 514: loss 2.6435, time 7984.56ms, mfu 16.48%\n",
      "iter 515: loss 2.6733, time 7969.61ms, mfu 16.52%\n",
      "iter 516: loss 2.6197, time 7949.04ms, mfu 16.57%\n",
      "iter 517: loss 2.6752, time 7944.80ms, mfu 16.60%\n",
      "iter 518: loss 2.7004, time 7940.88ms, mfu 16.64%\n",
      "iter 519: loss 2.6594, time 7936.98ms, mfu 16.67%\n",
      "iter 520: loss 2.6437, time 7940.62ms, mfu 16.70%\n",
      "iter 521: loss 2.6236, time 7943.15ms, mfu 16.73%\n",
      "iter 522: loss 2.6256, time 7950.42ms, mfu 16.75%\n",
      "iter 523: loss 2.6285, time 7952.39ms, mfu 16.77%\n",
      "iter 524: loss 2.6294, time 7957.69ms, mfu 16.78%\n",
      "iter 525: loss 2.6430, time 7936.23ms, mfu 16.80%\n",
      "iter 526: loss 2.6069, time 7914.41ms, mfu 16.82%\n",
      "iter 527: loss 2.6649, time 7907.90ms, mfu 16.84%\n",
      "iter 528: loss 2.6402, time 7895.59ms, mfu 16.86%\n",
      "iter 529: loss 2.6262, time 7890.85ms, mfu 16.88%\n",
      "iter 530: loss 2.6066, time 7887.14ms, mfu 16.90%\n",
      "iter 531: loss 2.6409, time 7886.17ms, mfu 16.92%\n",
      "iter 532: loss 2.6028, time 7894.01ms, mfu 16.93%\n",
      "iter 533: loss 2.6065, time 7921.56ms, mfu 16.94%\n",
      "iter 534: loss 2.6076, time 7938.06ms, mfu 16.94%\n",
      "iter 535: loss 2.6219, time 7986.31ms, mfu 16.93%\n",
      "iter 536: loss 2.6176, time 8027.99ms, mfu 16.92%\n",
      "iter 537: loss 2.6041, time 8104.18ms, mfu 16.89%\n",
      "iter 538: loss 2.6118, time 8240.94ms, mfu 16.83%\n",
      "iter 539: loss 2.6416, time 8593.49ms, mfu 16.72%\n",
      "iter 540: loss 2.5964, time 8741.58ms, mfu 16.59%\n",
      "iter 541: loss 2.5897, time 8585.44ms, mfu 16.50%\n",
      "iter 542: loss 2.6117, time 8027.60ms, mfu 16.52%\n",
      "iter 543: loss 2.6161, time 7986.35ms, mfu 16.56%\n",
      "iter 544: loss 2.5788, time 7982.48ms, mfu 16.59%\n",
      "iter 545: loss 2.5805, time 7990.10ms, mfu 16.61%\n",
      "iter 546: loss 2.6152, time 8015.11ms, mfu 16.63%\n",
      "iter 547: loss 2.5973, time 8015.69ms, mfu 16.65%\n",
      "iter 548: loss 2.5745, time 7979.71ms, mfu 16.67%\n",
      "iter 549: loss 2.5844, time 7963.09ms, mfu 16.70%\n",
      "iter 550: loss 2.6234, time 7946.77ms, mfu 16.72%\n",
      "iter 551: loss 2.5954, time 7912.84ms, mfu 16.75%\n",
      "iter 552: loss 2.5679, time 7906.28ms, mfu 16.78%\n",
      "iter 553: loss 2.6337, time 7908.44ms, mfu 16.80%\n",
      "iter 554: loss 2.5849, time 7924.10ms, mfu 16.82%\n",
      "iter 555: loss 2.6372, time 7968.15ms, mfu 16.83%\n",
      "iter 556: loss 2.6126, time 7989.66ms, mfu 16.83%\n",
      "iter 557: loss 2.5853, time 8130.80ms, mfu 16.81%\n",
      "iter 558: loss 2.5965, time 8231.23ms, mfu 16.76%\n",
      "iter 559: loss 2.5717, time 8178.63ms, mfu 16.73%\n",
      "iter 560: loss 2.6123, time 8026.97ms, mfu 16.74%\n",
      "iter 561: loss 2.6017, time 7983.55ms, mfu 16.75%\n",
      "iter 562: loss 2.6042, time 7970.93ms, mfu 16.76%\n",
      "iter 563: loss 2.6168, time 7927.62ms, mfu 16.79%\n",
      "iter 564: loss 2.6242, time 7899.16ms, mfu 16.81%\n",
      "iter 565: loss 2.5819, time 7904.70ms, mfu 16.83%\n",
      "iter 566: loss 2.5766, time 7929.99ms, mfu 16.85%\n",
      "iter 567: loss 2.6058, time 7945.43ms, mfu 16.86%\n",
      "iter 568: loss 2.5993, time 7982.82ms, mfu 16.86%\n",
      "iter 569: loss 2.5860, time 8086.77ms, mfu 16.84%\n",
      "iter 570: loss 2.5805, time 8166.09ms, mfu 16.80%\n",
      "iter 571: loss 2.6004, time 8071.89ms, mfu 16.79%\n",
      "iter 572: loss 2.6084, time 7993.60ms, mfu 16.80%\n",
      "iter 573: loss 2.5855, time 7987.35ms, mfu 16.80%\n",
      "iter 574: loss 2.5755, time 7992.03ms, mfu 16.81%\n",
      "iter 575: loss 2.5781, time 8000.26ms, mfu 16.81%\n",
      "iter 576: loss 2.5617, time 8073.02ms, mfu 16.80%\n",
      "iter 577: loss 2.5509, time 8164.01ms, mfu 16.77%\n",
      "iter 578: loss 2.5343, time 8275.41ms, mfu 16.72%\n",
      "iter 579: loss 2.5365, time 8524.02ms, mfu 16.63%\n",
      "iter 580: loss 2.5600, time 8274.18ms, mfu 16.59%\n",
      "iter 581: loss 2.5746, time 9079.04ms, mfu 16.41%\n",
      "iter 582: loss 2.5645, time 9107.52ms, mfu 16.25%\n",
      "iter 583: loss 2.5062, time 9043.35ms, mfu 16.12%\n",
      "iter 584: loss 2.5487, time 9182.57ms, mfu 15.97%\n",
      "iter 585: loss 2.5943, time 9023.96ms, mfu 15.87%\n",
      "iter 586: loss 2.5725, time 8774.34ms, mfu 15.81%\n",
      "iter 587: loss 2.5371, time 8903.85ms, mfu 15.74%\n",
      "iter 588: loss 2.5932, time 8656.52ms, mfu 15.73%\n",
      "iter 589: loss 2.5649, time 8295.52ms, mfu 15.78%\n",
      "iter 590: loss 2.5651, time 8056.29ms, mfu 15.87%\n",
      "iter 591: loss 2.5349, time 8000.24ms, mfu 15.97%\n",
      "iter 592: loss 2.5684, time 7991.55ms, mfu 16.05%\n",
      "iter 593: loss 2.5682, time 7985.66ms, mfu 16.14%\n",
      "iter 594: loss 2.5671, time 7990.15ms, mfu 16.21%\n",
      "iter 595: loss 2.5678, time 8005.88ms, mfu 16.27%\n",
      "iter 596: loss 2.5651, time 8076.23ms, mfu 16.31%\n",
      "iter 597: loss 2.5852, time 8210.14ms, mfu 16.32%\n",
      "iter 598: loss 2.5832, time 8192.66ms, mfu 16.33%\n",
      "iter 599: loss 2.5318, time 8972.38ms, mfu 16.20%\n",
      "iter 600: loss 2.5642, time 9131.47ms, mfu 16.05%\n",
      "iter 601: loss 2.5811, time 9099.23ms, mfu 15.93%\n",
      "iter 602: loss 2.5907, time 8718.12ms, mfu 15.88%\n",
      "iter 603: loss 2.5503, time 8756.90ms, mfu 15.83%\n",
      "iter 604: loss 2.5290, time 8878.61ms, mfu 15.76%\n",
      "iter 605: loss 2.5396, time 8694.86ms, mfu 15.74%\n",
      "iter 606: loss 2.5396, time 8497.05ms, mfu 15.75%\n",
      "iter 607: loss 2.5677, time 8307.11ms, mfu 15.79%\n",
      "iter 608: loss 2.5563, time 8165.03ms, mfu 15.86%\n",
      "iter 609: loss 2.5239, time 8102.65ms, mfu 15.94%\n",
      "iter 610: loss 2.5688, time 8056.43ms, mfu 16.02%\n",
      "iter 611: loss 2.5501, time 8038.17ms, mfu 16.09%\n",
      "iter 612: loss 2.5469, time 8015.88ms, mfu 16.16%\n",
      "iter 613: loss 2.5512, time 8008.29ms, mfu 16.23%\n",
      "iter 614: loss 2.5501, time 8004.58ms, mfu 16.29%\n",
      "iter 615: loss 2.5654, time 8015.06ms, mfu 16.34%\n",
      "iter 616: loss 2.5420, time 8002.04ms, mfu 16.39%\n",
      "iter 617: loss 2.5506, time 8013.21ms, mfu 16.43%\n",
      "iter 618: loss 2.5451, time 7999.69ms, mfu 16.47%\n",
      "iter 619: loss 2.5317, time 7996.57ms, mfu 16.51%\n",
      "iter 620: loss 2.5321, time 7998.57ms, mfu 16.54%\n",
      "iter 621: loss 2.5463, time 8002.59ms, mfu 16.57%\n",
      "iter 622: loss 2.5374, time 8007.45ms, mfu 16.59%\n",
      "iter 623: loss 2.5127, time 7999.30ms, mfu 16.62%\n",
      "iter 624: loss 2.5502, time 7999.15ms, mfu 16.64%\n",
      "iter 625: loss 2.5243, time 7996.76ms, mfu 16.66%\n",
      "iter 626: loss 2.5365, time 7997.34ms, mfu 16.68%\n",
      "iter 627: loss 2.5193, time 7998.03ms, mfu 16.69%\n",
      "iter 628: loss 2.5486, time 7997.93ms, mfu 16.71%\n",
      "iter 629: loss 2.5620, time 7996.73ms, mfu 16.72%\n",
      "iter 630: loss 2.5469, time 7997.50ms, mfu 16.73%\n",
      "iter 631: loss 2.5442, time 7997.31ms, mfu 16.74%\n",
      "iter 632: loss 2.5296, time 7997.30ms, mfu 16.75%\n",
      "iter 633: loss 2.5244, time 7996.90ms, mfu 16.76%\n",
      "iter 634: loss 2.5174, time 7997.49ms, mfu 16.77%\n",
      "iter 635: loss 2.5463, time 7997.19ms, mfu 16.78%\n",
      "iter 636: loss 2.5435, time 7995.01ms, mfu 16.78%\n",
      "iter 637: loss 2.5448, time 7999.37ms, mfu 16.79%\n",
      "iter 638: loss 2.5206, time 7998.08ms, mfu 16.79%\n",
      "iter 639: loss 2.5275, time 7997.91ms, mfu 16.80%\n",
      "iter 640: loss 2.5298, time 7990.91ms, mfu 16.80%\n",
      "iter 641: loss 2.5454, time 7946.08ms, mfu 16.82%\n",
      "iter 642: loss 2.5391, time 7924.60ms, mfu 16.83%\n",
      "iter 643: loss 2.5431, time 7910.08ms, mfu 16.85%\n",
      "iter 644: loss 2.5328, time 7888.07ms, mfu 16.87%\n",
      "iter 645: loss 2.5691, time 7893.51ms, mfu 16.89%\n",
      "iter 646: loss 2.5342, time 7903.32ms, mfu 16.91%\n",
      "iter 647: loss 2.5374, time 7895.68ms, mfu 16.92%\n",
      "iter 648: loss 2.5698, time 7907.44ms, mfu 16.93%\n",
      "iter 649: loss 2.5496, time 7923.20ms, mfu 16.94%\n",
      "iter 650: loss 2.5621, time 7911.22ms, mfu 16.95%\n",
      "iter 651: loss 2.5109, time 7919.62ms, mfu 16.95%\n",
      "iter 652: loss 2.5342, time 7932.20ms, mfu 16.96%\n",
      "iter 653: loss 2.5381, time 7951.69ms, mfu 16.95%\n",
      "iter 654: loss 2.5181, time 7985.62ms, mfu 16.94%\n",
      "iter 655: loss 2.5069, time 8089.63ms, mfu 16.91%\n",
      "iter 656: loss 2.5328, time 8200.18ms, mfu 16.87%\n",
      "iter 657: loss 2.5314, time 8203.09ms, mfu 16.82%\n",
      "iter 658: loss 2.5383, time 8616.24ms, mfu 16.70%\n",
      "iter 659: loss 2.5091, time 8660.00ms, mfu 16.59%\n",
      "iter 660: loss 2.5155, time 8801.33ms, mfu 16.46%\n",
      "iter 661: loss 2.5296, time 8953.49ms, mfu 16.32%\n",
      "iter 662: loss 2.5228, time 9116.69ms, mfu 16.16%\n",
      "iter 663: loss 2.4986, time 9132.77ms, mfu 16.02%\n",
      "iter 664: loss 2.5426, time 9093.51ms, mfu 15.90%\n",
      "iter 665: loss 2.5051, time 9167.44ms, mfu 15.78%\n",
      "iter 666: loss 2.5464, time 8873.04ms, mfu 15.72%\n",
      "iter 667: loss 2.5212, time 8776.89ms, mfu 15.68%\n",
      "iter 668: loss 2.5180, time 8850.44ms, mfu 15.63%\n",
      "iter 669: loss 2.5184, time 8627.56ms, mfu 15.63%\n",
      "iter 670: loss 2.5108, time 8424.29ms, mfu 15.67%\n",
      "iter 671: loss 2.5057, time 8191.31ms, mfu 15.74%\n",
      "iter 672: loss 2.4963, time 8158.09ms, mfu 15.82%\n",
      "iter 673: loss 2.5047, time 8038.35ms, mfu 15.91%\n",
      "iter 674: loss 2.5215, time 8017.83ms, mfu 16.00%\n",
      "iter 675: loss 2.5031, time 8010.72ms, mfu 16.08%\n",
      "iter 676: loss 2.5159, time 8015.92ms, mfu 16.15%\n",
      "iter 677: loss 2.4990, time 8010.41ms, mfu 16.22%\n",
      "iter 678: loss 2.5141, time 8001.91ms, mfu 16.28%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync \u001b[38;5;241m=\u001b[39m (micro_step \u001b[38;5;241m==\u001b[39m gradient_accumulation_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 61\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# report number of parameters\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of parameters: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_params()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m,))\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     30\u001b[0m     b, t \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:917\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    915\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m    916\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:88\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     86\u001b[0m             args_[idx] \u001b[38;5;241m=\u001b[39m args_[idx]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 88\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with torch.no_grad.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled():\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:113\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:505\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m    498\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, seed, offset)\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m fakified_out\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:113\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_inductor/codecache.py:906\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:784\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs):\n\u001b[1;32m    783\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_inductor/codecache.py:934\u001b[0m, in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m compiled_graph\u001b[38;5;241m.\u001b[39martifact_path\n\u001b[1;32m    927\u001b[0m     compiled_graph\u001b[38;5;241m.\u001b[39mcompiled_artifact \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m    928\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_key,\n\u001b[1;32m    929\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39martifact_path,\n\u001b[1;32m    930\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_linemap,\n\u001b[1;32m    931\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mconstants,\n\u001b[1;32m    932\u001b[0m     )\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_sd23/lk/clks3v2rupr4knmqlwxq4pavh5rlw5abfcjnre4icsxax7jjghs6.py:1215\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1213\u001b[0m buf118 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((\u001b[38;5;241m12288\u001b[39m, \u001b[38;5;241m768\u001b[39m), (\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# Source Nodes: [l__self___transformer_h_4_attn_c_proj], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m-> 1215\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf112\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf117\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf118\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m buf119 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1217\u001b[0m buf120 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12288\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### 4.4 training loop\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # mixed precison need to be closed\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            # save the loss during each iter\n",
    "            losses[k] = loss.item()\n",
    "        # get the mean loss across one eval iteration\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    # you may notice that this is the pre-checking step rather than training itself\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # if loss lower than best loss we set, we will update the loss to current loss\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,                    \n",
    "                }\n",
    "\n",
    "                print(f'saving checkpoint to {out_dir}')\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable.\n",
    "            \n",
    "            # only sync gradients at last\n",
    "            # it means that if in ddp, we only need to forward pass when last step\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1 )\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        # if we choose to clip gradients, we need to unscale the gradients first to clip the right gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        # grad clip is the max gradients during training to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "\n",
    "    # if not in ddp, we use this logging\n",
    "    if iter_num  % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            # 0.9 and 0.1 can dynamticly adjust to monitor training flop\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1    \n",
    "    # this notebool is just for presentation, so the max_iters will set to 10000, in raw project, it will be set to 600000.\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "    \n",
    "# ddp needs init and destory\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 inference\n",
    "During inference, we can utilize the sample method to generate responses for a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model raw state dict: \n",
      "('_orig_mod.transformer.wte.weight', tensor([[-0.0144, -0.0432, -0.0355,  ...,  0.0037, -0.0293, -0.0303],\n",
      "        [-0.0176,  0.0060, -0.0204,  ..., -0.0106, -0.0338, -0.0159],\n",
      "        [ 0.0446, -0.0146, -0.0076,  ...,  0.0023,  0.0303, -0.0201],\n",
      "        ...,\n",
      "        [-0.0003, -0.0093,  0.0030,  ..., -0.0067,  0.0018,  0.0263],\n",
      "        [ 0.0176, -0.0502,  0.0234,  ..., -0.0665,  0.0055,  0.0134],\n",
      "        [ 0.0312, -0.0296,  0.0299,  ..., -0.0214,  0.0347,  0.0068]],\n",
      "       device='cuda:0'))\n",
      "get dataset meta alphabet information: \n",
      "{'vocab_size': 65, 'itos': {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}, 'stoi': {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}}\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e', 'l']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e', 'l', 'd']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e', 'l', 'd', ' ']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e', 'l', 'd', ' ', 't']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', ' ', 't', 'h', 'e', 'l', 'd', ' ', 't', ' ']\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# we will not start from stratch like nanoGPT instead of inference with our trained model above.\n",
    "# But one thing you need to konw is when we save the model.state_dict\n",
    "# the module name would have a prefix '_orig_mod_'\n",
    "show_state_dict = model.state_dict().items()\n",
    "# just like: \n",
    "print(f'model raw state dict: \\n{list(show_state_dict)[0]}')\n",
    "# so when you load a model you trained before, you may remove the prefix and the detail is in nanoGPT repertory -> sample.py\n",
    "\n",
    "#### 1. load encode and decode\n",
    "# This step means we need to convert word into ids, such as: 'I love you' -> '<SOS> 5 2 0 <EOS>'\n",
    "# there are two ways to complement that:\n",
    "# 1. load meta\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb+') as f:\n",
    "        meta = pickle.load(f)\n",
    "        print(f'get dataset meta alphabet information: \\n{meta}')\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: [itos[i] for i in l]\n",
    "else:\n",
    "    # 2. load from tiktoken\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "#### 2. inference\n",
    "# assuming that we start at the very beginning\n",
    "start = 'I love'\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None,...])\n",
    "\n",
    "# inference config\n",
    "# assuming that we need to generate 2 responses for a sample\n",
    "sample_num = 10\n",
    "# max tokens generate for a sample\n",
    "max_new_tokens = 1\n",
    "# temperature 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "temperature = 0.8\n",
    "# top_k retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "top_k = 10\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(sample_num):\n",
    "            # NOTE you may notice that the nanoGPT `generate` method is in the GPT model\n",
    "            # but I will put it as a independent method for better understanding\n",
    "            \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "            \"\"\"\n",
    "            for _ in range(max_new_tokens):\n",
    "                # if the sequence context is growing too long we must crop it at block_size\n",
    "                idx_cond = x if x.size(1) <= gptconf.block_size else x[:, -gptconf.block_size:]\n",
    "                # forward the model to get the logits for the index in the sequence\n",
    "                logits, _ = model(idx_cond)\n",
    "                # pluck the logits at the final step and scale by desired temperature\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # optionally crop the logits to only the top k options\n",
    "                if top_k is not None:\n",
    "                    # torch.topk returns two variables: v means value and _ means indices\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    # v[:, [-1]] means select values in logits that all lower than v last dimension\n",
    "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                # apply softmax to convert logits to (normalized) probabilities\n",
    "                probs = F.softmax(logits, dim = -1)\n",
    "                # sample from the distribution, num_samples means sample a idx once and NOT the max prob idx must be sampled\n",
    "                # to make sure diversity!\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                # append sampled index to the running sequence and continue\n",
    "                x = torch.cat((x, idx_next), dim = -1)\n",
    "\n",
    "            #### 3.output\n",
    "            print(decode(x[0].tolist()))\n",
    "            print('-------------------------')         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
