{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA\n",
    "**Welcome** to the llama notebook. And I must say that this charpter will have several challenging points you need to pay attention to.\n",
    "1. RMS-Norm\n",
    "2. SwiGLU\n",
    "3. Rope\n",
    "4. KVcache\n",
    "5. Grouped-Query-Attention\n",
    "\n",
    "But feel free to get over with them. Cause Studying itself is just fun. So, enjoy it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import pacakge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import math\n",
    "import inspect\n",
    "import tiktoken\n",
    "\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLaMA model parameters\n",
    "Not all of parameters are setting in the first place, some of them are added during the model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLaMAconfig:\n",
    "    # one-head dimension\n",
    "    n_embedding: int = 128\n",
    "    block_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LLaMA model\n",
    "we will implement the LLaMA model step by step. But you should know that in this part, I will NOT blend one module with another, which means I will not infer to KV cache when debug attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 RMS-Norm (Root Mean Square Layer Normalization)\n",
    "Question: What is the difference between **pre-norm** and **post-norm**? (It will be answered in the end of this part.)\n",
    "\n",
    "Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. \n",
    "\n",
    "BSD 3-Clause License: https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "\n",
    "see also: https://blog.csdn.net/yjw123456/article/details/138139970\n",
    "\n",
    "<img src=\"./image/RMSNorm.png\" alt=\"RMSNorm and LayerNorm\" style=\"width: 350px; height: 200px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        # this is the W\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        \n",
    "        # calculate the root of 1/H * sigma{(x_i)**2}\n",
    "        norm_x = torch.mean(x * x, dim = self.dim, keepdim = True)\n",
    "        # calculate rms_x, eps is applied to avoid devided 0.\n",
    "        rms_x = torch.rsqrt(norm_x + self.eps)\n",
    "        # calculate the normed_x\n",
    "        normed_x = x * rms_x\n",
    "        # attach the learning params\n",
    "        scaled_x = normed_x * self.scale\n",
    "        return scaled_x\n",
    "\n",
    "# note: rmsnorm is designed to reduce computation caused by mean in layernorm, that can improve computational effiency and precision simultaneously.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 SwiGLU\n",
    "\n",
    "SwiGLU means a learning gated object function, element wise.\n",
    "\n",
    "<img src=\"./image/SwiGLU.png\" alt=\"SwiGLU\" style=\"width: 450px; height: 260px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.silu(x): x * sigmoid(x)\n",
    "\n",
    "def mlp_silu(x):\n",
    "    # here, the embedding dim will not change for showing and we will discuss this latter.\n",
    "    fc1 = nn.Linear(x.size(-1), x.size(-1))\n",
    "    fc2 = nn.Linear(x.size(-1), x.size(-1))\n",
    "    # proj fc2 with SiLU\n",
    "    x = fc1(x)\n",
    "    gated_x = F.silu(fc2(x))\n",
    "    # element wise multiply matrix\n",
    "    output = x * gated_x\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 ROPE\n",
    "One about rope you need to pay attention is that rope encodes the **absolute position** with a rotation matrix and meanwhile incorporates the explicit **relative positive** dependency in **self-attention** formulation. --rope paper\n",
    "\n",
    "We can try to understand sentence above with a logical line: absolute position -> self-attention -> relative position information\n",
    "\n",
    "⭐there are **two** aspects:\n",
    "- ✅ Definition of RoPE.\n",
    "- ✅ Extention of RoPE to a long context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Definition of RoPE\n",
    "reference 1: https://blog.csdn.net/weixin_43646592/article/details/130924280\n",
    "\n",
    "reference 2: https://oi-wiki.org/math/complex/\n",
    "\n",
    "⭐⭐⭐ (recommended) reference 3: https://blog.csdn.net/v_JULY_v/article/details/134085503 \n",
    "\n",
    "NOTE: I had beed finished the math explanation of the RoPE, you can find it in the `Addition` part of my github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta shape: \n",
      " torch.Size([64])\n",
      "theta: \n",
      " tensor([1.0000, 0.8660, 0.7499, 0.6494, 0.5623])\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-compute rope -> to preduce the cos and sin matrix\n",
    "batch_size = 32\n",
    "seq_len = 1024\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "# base is used to calculate \\theta\n",
    "base = 10000\n",
    "# \\theta is the rotary angle\n",
    "theta = 1.0 / (base ** (torch.arange(0, n_embed, 2) / n_embed))\n",
    "print('theta shape: \\n', theta.shape)\n",
    "print('theta: \\n', theta[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_idx shape: \n",
      " torch.Size([1024])\n",
      "id_theta shape: \n",
      " torch.Size([1024, 64])\n",
      "id_theta: \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.8660, 0.7499, 0.6494, 0.5623]])\n"
     ]
    }
   ],
   "source": [
    "seq_idx = torch.arange(seq_len)\n",
    "print('seq_idx shape: \\n', seq_idx.shape)\n",
    "# outer is the dot computation, idx_theta is the m\\theta is the math equation.\n",
    "idx_theta = torch.outer(seq_idx, theta).float()\n",
    "print('id_theta shape: \\n', idx_theta.shape)\n",
    "print('id_theta: \\n', idx_theta[:2, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache shape: \n",
      " torch.Size([1024, 64, 2])\n",
      "cos: \n",
      " tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.6479,  0.7318,  0.7965,  0.8460],\n",
      "        [-0.4161, -0.1604,  0.0709,  0.2687,  0.4315],\n",
      "        [-0.9900, -0.8558, -0.6279, -0.3685, -0.1160],\n",
      "        [-0.6536, -0.9485, -0.9899, -0.8556, -0.6277]])\n",
      "sin: \n",
      " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.7617,  0.6816,  0.6047,  0.5332],\n",
      "        [ 0.9093,  0.9870,  0.9975,  0.9632,  0.9021],\n",
      "        [ 0.1411,  0.5173,  0.7783,  0.9296,  0.9933],\n",
      "        [-0.7568, -0.3167,  0.1415,  0.5176,  0.7785]])\n",
      "cache: \n",
      " tensor([[[ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5403,  0.8415],\n",
      "         [ 0.6479,  0.7617],\n",
      "         [ 0.7318,  0.6816],\n",
      "         [ 0.7965,  0.6047],\n",
      "         [ 0.8460,  0.5332]],\n",
      "\n",
      "        [[-0.4161,  0.9093],\n",
      "         [-0.1604,  0.9870],\n",
      "         [ 0.0709,  0.9975],\n",
      "         [ 0.2687,  0.9632],\n",
      "         [ 0.4315,  0.9021]],\n",
      "\n",
      "        [[-0.9900,  0.1411],\n",
      "         [-0.8558,  0.5173],\n",
      "         [-0.6279,  0.7783],\n",
      "         [-0.3685,  0.9296],\n",
      "         [-0.1160,  0.9933]],\n",
      "\n",
      "        [[-0.6536, -0.7568],\n",
      "         [-0.9485, -0.3167],\n",
      "         [-0.9899,  0.1415],\n",
      "         [-0.8556,  0.5176],\n",
      "         [-0.6277,  0.7785]]])\n"
     ]
    }
   ],
   "source": [
    "cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim= -1)\n",
    "print('cache shape: \\n', cache.shape)\n",
    "# we need to explain this transformation\n",
    "cos = torch.cos(idx_theta)\n",
    "print('cos: \\n', cos[:5, :5])\n",
    "sin = torch.sin(idx_theta)\n",
    "print('sin: \\n', sin[:5, :5])\n",
    "print('cache: \\n', cache[:5, :5, :])\n",
    "# In the cache last dimension, the first column is cos(m\\theta), the second column is sin(m\\theta). m is the position and theta is the angle with dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([32, 1024, 8, 128])\n",
      "x_ shape: \n",
      " torch.Size([32, 1024, 8, 64, 2])\n",
      "rope_cache shape: \n",
      " torch.Size([1, 1024, 1, 64, 2])\n",
      "x_0 shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "rope_cache_0 shape: \n",
      " torch.Size([1, 1024, 1, 64])\n",
      "x_even shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "x_odd shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "out_put shape: \n",
      " torch.Size([32, 1024, 8, 64, 2])\n",
      "out_put shape: \n",
      " torch.Size([32, 1024, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# 2. apply rope -> there if an input q, and we transit it to right form and then multiply it with cache above.\n",
    "## we can construct an input when debug\n",
    "## an input may look like: ( batch_size, seq_len, num_head, dim)\n",
    "x = torch.randn((batch_size, seq_len, n_head, n_embed))\n",
    "print('x shape: \\n', x.shape)\n",
    "## 1. trucate to avoid length out of range\n",
    "seq_len = x.size(1)\n",
    "cache = cache[:seq_len]\n",
    "## 2. reshape the x -> let the last two dim -> (-1, 2) -> (128) -> (64, 2)\n",
    "x_ = x.reshape(*x.shape[:-1], -1, 2)\n",
    "print('x_ shape: \\n', x_.shape)\n",
    "rope_cache = cache.view(1, x_.size(1), 1, x_.size(3), 2)\n",
    "print('rope_cache shape: \\n', rope_cache.shape)\n",
    "\n",
    "## then we compute the rope output according to the formulation, ... means all dimension except the pointed dimension.\n",
    "x_0 = x_[..., 0]\n",
    "x_1 = x_[..., 1]\n",
    "print('x_0 shape: \\n', x_0.shape)\n",
    "rope_cache_0 = rope_cache[..., 0]\n",
    "rope_cache_1 = rope_cache[..., 1]\n",
    "print('rope_cache_0 shape: \\n', rope_cache_0.shape)\n",
    "\n",
    "## * is element-wise matrix-multiplying\n",
    "# In even dimension: 0,2,...\n",
    "x_even = x_0 * rope_cache_0 - x_1 * rope_cache_1\n",
    "print('x_even shape: \\n', x_even.shape)\n",
    "# In odds dimension: 1, 3,...\n",
    "x_odd = x_1 * rope_cache_0 + x_0 * rope_cache_1\n",
    "print('x_odd shape: \\n', x_odd.shape)\n",
    "out_put = torch.stack([x_even, x_odd], dim = -1)\n",
    "print('out_put shape: \\n', out_put.shape)\n",
    "# reshape from the third dimension\n",
    "out_put = out_put.flatten(3)\n",
    "print('out_put shape: \\n', out_put.shape)\n",
    "\n",
    "# output shape is indentity with inputshape\n",
    "# you may also see RoPE notebook to see more information.\n",
    "\n",
    "# we can conclude code above\n",
    "def bulid_rope_cache(\n",
    "        seq_len:int,\n",
    "        n_embed: int,\n",
    "        dtype: torch.dtype,\n",
    "        device: torch.device,\n",
    "        base: int = 10000\n",
    "):\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_embed, 2, dtype=dtype, device=device) / n_embed))\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half() \n",
    "    return cache\n",
    "         \n",
    "def apply_rope(x: torch.tensor, rope_cache):\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "\n",
    "    x_out = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out = x_out.flatten(3)\n",
    "    \n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 KVcache ⭐⭐⭐\n",
    "KV cache is used to reduce computational resources during **inference**. In general, we need to calculate self-attention every time when we pass the next token to raw sentences, then calculate next next token.\n",
    "\n",
    "The idea is that we can store all the previous k v in self-attention calculating so that we can reduce repeat computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([1, 10])\n",
      "embed_x shape: \n",
      " torch.Size([1, 10, 128])\n",
      "Q shape: \n",
      " torch.Size([1, 10, 128])\n",
      "K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "score shape: \n",
      " torch.Size([1, 10, 10])\n",
      "attn shape: \n",
      " torch.Size([1, 10, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 10, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 10, 100])\n",
      "output: \n",
      " tensor(44)\n",
      "new_x shape: \n",
      " torch.Size([1, 11])\n",
      "Q shape: \n",
      " torch.Size([1, 11, 128])\n",
      "K shape: \n",
      " torch.Size([1, 11, 128])\n",
      "V shape: \n",
      " torch.Size([1, 11, 128])\n",
      "score shape: \n",
      " torch.Size([1, 11, 11])\n",
      "attn shape: \n",
      " torch.Size([1, 11, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 11, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 11, 100])\n",
      "output: \n",
      " tensor(39)\n"
     ]
    }
   ],
   "source": [
    "# First, we construct an input, shape is: (1, input_len), assumpt that embedding dim is 128, vocab_size is 100\n",
    "input_len = 10\n",
    "vocab_size = 100\n",
    "embedding_size = 128\n",
    "x = torch.randint(0, 100, (1, input_len))\n",
    "print('x shape: \\n', x.shape)\n",
    "# **WITHOUT KVCAHCE**\n",
    "# 1. first, we need to see the raw attention **WITHOUT KVCAHCE**\n",
    "# When inference, we passed the inputs into the model.\n",
    "## 1.1 pos embedding, here we just nn.embedding\n",
    "### 1. first iteration\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "embed_x = embedding(x)\n",
    "print('embed_x shape: \\n', embed_x.shape)\n",
    "wq = nn.Linear(embedding_size, embedding_size)\n",
    "wk = nn.Linear(embedding_size, embedding_size)\n",
    "wv = nn.Linear(embedding_size, embedding_size)\n",
    "Q = wq(embed_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "# we discard all the other procudures\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "### 2. second iteration/generation\n",
    "new_x = torch.cat((x, output.unsqueeze(0).view(1, -1)), dim=-1)\n",
    "print('new_x shape: \\n', new_x.shape)\n",
    "# then we repeat operations above\n",
    "embed_new_x = embedding(new_x)\n",
    "Q = wq(embed_new_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_new_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_new_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in attention calculation, iteration 1 and 2 both calculate the whole attention, and we can see in iteration 1 and 2, the wq and wk are all the same！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([1, 10])\n",
      "embed_x shape: \n",
      " torch.Size([1, 10, 128])\n",
      "Q shape: \n",
      " torch.Size([1, 10, 128])\n",
      "K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "cache_K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "cache_V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "score shape: \n",
      " torch.Size([1, 10, 10])\n",
      "attn shape: \n",
      " torch.Size([1, 10, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 10, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 10, 100])\n",
      "output: \n",
      " tensor(56)\n",
      "new_x shape: \n",
      " torch.Size([1, 1])\n",
      "Q shape: \n",
      " torch.Size([1, 1, 128])\n",
      "K shape: \n",
      " torch.Size([1, 1, 128])\n",
      "V shape: \n",
      " torch.Size([1, 1, 128])\n",
      "cached_K shape: \n",
      " torch.Size([1, 11, 128])\n",
      "cached_V shape: \n",
      " torch.Size([1, 11, 128])\n",
      "score shape: \n",
      " torch.Size([1, 1, 11])\n",
      "attn shape: \n",
      " torch.Size([1, 1, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 1, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 1, 100])\n",
      "output: \n",
      " tensor(68)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **WITH KVCAHCE**\n",
    "## In KVcache, we can just store the first calculated k and v, and then concat new k an v to generate a complete whole length k and v\n",
    "input_len = 10\n",
    "vocab_size = 100\n",
    "embedding_size = 128\n",
    "x = torch.randint(0, 100, (1, input_len))\n",
    "print('x shape: \\n', x.shape)\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "embed_x = embedding(x)\n",
    "print('embed_x shape: \\n', embed_x.shape)\n",
    "wq = nn.Linear(embedding_size, embedding_size)\n",
    "wk = nn.Linear(embedding_size, embedding_size)\n",
    "wv = nn.Linear(embedding_size, embedding_size)\n",
    "Q = wq(embed_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "####################KVCACHE##########################\n",
    "cache_K = K\n",
    "print('cache_K shape: \\n', cache_K.shape)\n",
    "cache_V = V\n",
    "print('cache_V shape: \\n', cache_V.shape)\n",
    "####################KVCACHE##########################\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "# we discard all the other procudures\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "### 2. second iteration/generation\n",
    "#############x is different######################\n",
    "new_x = output.unsqueeze(0).view(1, -1)\n",
    "print('new_x shape: \\n', new_x.shape)\n",
    "# then we repeat operations above\n",
    "## and notice the QKV shape\n",
    "embed_new_x = embedding(new_x)\n",
    "Q = wq(embed_new_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_new_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_new_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "###################concat cache####################\n",
    "K = torch.concat((cache_K, K),dim=1)\n",
    "print('cached_K shape: \\n', K.shape)\n",
    "V = torch.concat((cache_V, V),dim=1)\n",
    "print('cached_V shape: \\n', V.shape)\n",
    "cache_K = K\n",
    "cache_V = V\n",
    "###################concat cache####################\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "# In conclusion, the computation is reduced from Q[L+1,D] @ K[L+1,D] @ V[L+1,D] to  Q[1,D] @ K[L+1,D] @ V[L+1,D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Grouped-Query-Attention\n",
    "The KV cache can significantly reduce inference computational burdens. But the ocupation of GPU was massive. And there are mainly four ways to make kvcache effiency: \n",
    "- cut length; \n",
    "- reduce self-attention head-nums - MQA/GQA; \n",
    "- quantization of kvcache;\n",
    "- paged attention \n",
    "\n",
    "In lit-LLaMA, adopted cut length by rolling; And we will introduce another method GQA.\n",
    "\n",
    "In general, there are several heads during self-attention and we grouped them, such as: 8 heads -> 2 grouped heads, means that there are 4 heads in one group. In one group, the kv just save once and others were copied from this saved kv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/GQA.png\" alt=\"GQA\" style=\"width: 500px; height: 450px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.1 parameters\n",
    "we assumed the embedding dim is 18, and head num is 6, it means there are 3 channels in one head. Hence that:\n",
    "- Q dim 18, 6 heads -> (Q1, Q2, Q3, Q4, Q5, Q6)\n",
    "\n",
    "- K dim 18, 2 grouped, -> (grouped_K1, grouped_K2) \n",
    "-> (grouped_K1_copy1, grouped_K1_copy2, grouped_K1_copy3,\n",
    "grouped_K2_copy1, grouped_K2_copy2,grouped_K2_copy3)\n",
    "- grouped_K1 + grouped_K2, dim: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 18\n",
    "    # attention layers\n",
    "    n_layers: int = 1\n",
    "    # q heads\n",
    "    n_heads: int = 6\n",
    "    # kv grouped heads\n",
    "    n_kv_heads: int =  2\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 10  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    # rms norm eps\n",
    "    norm_eps: float = 1e-5\n",
    "    # theta bese\n",
    "    rope_theta: float = 500000\n",
    "    max_batch_size: int = 2\n",
    "    max_seq_len: int = 17\n",
    "    model_parallel_size = 1\n",
    "\n",
    "config = ModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 GQA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated k shape: \n",
      "torch.Size([1, 7, 6, 3])\n",
      "k: \n",
      "tensor([[ 1.1777, -0.6196, -0.3396],\n",
      "        [ 0.1806,  0.8465, -0.8715]])\n",
      "repeated k: \n",
      "tensor([[ 1.1777, -0.6196, -0.3396],\n",
      "        [ 1.1777, -0.6196, -0.3396],\n",
      "        [ 1.1777, -0.6196, -0.3396],\n",
      "        [ 0.1806,  0.8465, -0.8715],\n",
      "        [ 0.1806,  0.8465, -0.8715],\n",
      "        [ 0.1806,  0.8465, -0.8715]])\n"
     ]
    }
   ],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    # x[:, :, :, None, :] means that add one new dim in None dim\n",
    "    # x: [b, s, h, d] -> [b, s, h, 1, d]\n",
    "    # expand operation will expand vector dims to desired shape\n",
    "    # BUT you may notice that it may NOT copy data substanially\n",
    "    # As a fact, it just looks like the vector is copied by torch broadcast\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim) # \n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "# debug\n",
    "k = torch.randn(1, 7, 2, 3)\n",
    "repeat_k = repeat_kv(k, 3)\n",
    "print(f'repeated k shape: \\n{repeat_k.shape}')\n",
    "print(f'k: \\n{k[0, 0, :, :]}')\n",
    "print(f'repeated k: \\n{repeat_k[0, 0, :, :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wq_shape: \n",
      "\ttorch.Size([18, 18])\n",
      "wk_shape: \n",
      "\ttorch.Size([6, 18])\n",
      "wv_shape: \n",
      "\ttorch.Size([6, 18])\n",
      "wo_shape: \n",
      "\ttorch.Size([18, 18])\n",
      "attn shape with GQA: \n",
      "\tAttention(\n",
      "  (wq): Linear(in_features=18, out_features=18, bias=False)\n",
      "  (wk): Linear(in_features=18, out_features=6, bias=False)\n",
      "  (wv): Linear(in_features=18, out_features=6, bias=False)\n",
      "  (wo): Linear(in_features=18, out_features=18, bias=False)\n",
      ")\n",
      "x_src shape: \n",
      "\ttorch.Size([2, 17, 18])\n",
      "q shape: \n",
      "\ttorch.Size([2, 17, 6, 3])\n",
      "keys shape: \n",
      "\ttorch.Size([2, 17, 2, 3])\n",
      "values shape: \n",
      "\ttorch.Size([2, 17, 2, 3])\n",
      "q shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "repeated_keys shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "repeated_values shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "output shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "concated shape: \n",
      "\ttorch.Size([2, 17, 18])\n",
      "y shape: \n",
      "\ttorch.Size([2, 17, 18])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # this is the parallel parameter\n",
    "        model_parallel_size = args.model_parallel_size\n",
    "        # local heads means that total heads are distributed into several nodes\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(in_features=args.dim, out_features=args.n_heads * self.head_dim,bias=False)\n",
    "        self.wk = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wo = nn.Linear(in_features=args.n_heads * self.head_dim, out_features=args.dim,bias=False)\n",
    "\n",
    "        print(f'wq_shape: \\n\\t{self.wq.weight.shape}')\n",
    "        print(f'wk_shape: \\n\\t{self.wk.weight.shape}')\n",
    "        print(f'wv_shape: \\n\\t{self.wv.weight.shape}')\n",
    "        print(f'wo_shape: \\n\\t{self.wo.weight.shape}')\n",
    "\n",
    "        # kvcache, since that we grouped the kv, so we just need to store one grouped k and v\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        # notice that the shape of k and v\n",
    "        # we can find that 'Grouped' DOES NOT mean that calculate the whole k and v first then split them into groups\n",
    "        # while calculate one grouped k and v, then expand them into whole length\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        # you may notice the position of applying rope\n",
    "        # calculate qkv -> apply rope -> cache kv -> expand\n",
    "        # xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # ignore RoPE\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        # By default, this is the inference environment\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv \n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        print(f'q shape: \\n\\t{xq.shape}')\n",
    "        print(f'keys shape: \\n\\t{keys.shape}')\n",
    "        print(f'values shape: \\n\\t{values.shape}')\n",
    "\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        print(f'q shape: \\n\\t{xq.shape}')\n",
    "        print(f'repeated_keys shape: \\n\\t{keys.shape}')\n",
    "        print(f'repeated_values shape: \\n\\t{values.shape}')\n",
    "\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values) \n",
    "        print(f'output shape: \\n\\t{output.shape}')\n",
    "        # (b, h, l, h_d) -> (b, l, d)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        print(f'concated shape: \\n\\t{output.shape}')\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "# debug\n",
    "attn = Attention(config)\n",
    "print(f'attn shape with GQA: \\n\\t{attn}')\n",
    "batch_size = config.max_batch_size\n",
    "seq_len = config.max_seq_len\n",
    "embedding_dim = config.dim\n",
    "x_src = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "print(f'x_src shape: \\n\\t{x_src.shape}')\n",
    "y = attn(x_src, start_pos = 0, freqs_cis=None, mask=None)\n",
    "print(f'y shape: \\n\\t{y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 LLAMA Model Construction\n",
    "In this step, we will assemble a llama model using modules just like other chapters before.\n",
    "\n",
    "<img src=\"./image/LLaMA.png\" alt=\"LLaMA\" style=\"width: 600px; height: 400px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of LLaMA implements lit-llama\n",
    "##### 2.5.1 LLAMA Config\n",
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    # seq_len\n",
    "    block_size: int = 2048\n",
    "    # vocab_size\n",
    "    vocab_size: int = 100\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embed: int = 4096\n",
    "    \n",
    "##### 2.5.2 CausalSelfAttention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config:LLaMAConfig):\n",
    "        super().__init__()\n",
    "        # check head\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "\n",
    "        # qkv in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "\n",
    "        # output proj\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        # settings\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.tensor,\n",
    "                rope: torch.tensor,\n",
    "                mask: torch.tensor,\n",
    "                max_seq_length: int,\n",
    "                input_pos: Optional[torch.tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None,\n",
    "                ):\n",
    "            # (Batch_size, seq_len, embedding_dim)\n",
    "            B, T, C = x.size()\n",
    "            # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "            q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "\n",
    "            head_size = C // self.n_head\n",
    "            k = k.view(B, T, self.n_head, head_size)\n",
    "            q = q.view(B, T, self.n_head, head_size)\n",
    "            v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "            q = apply_rope(q, rope)\n",
    "            k = apply_rope(k, rope)\n",
    "\n",
    "            k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "            q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "            v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "            # kvcache:\n",
    "            if kv_cache is not None:\n",
    "                cache_k, cache_v = kv_cache\n",
    "                if input_pos[-1] >= max_seq_length:\n",
    "                    input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                    # torch.roll: https://pytorch.org/docs/stable/generated/torch.roll.html\n",
    "                    cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                    cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "                # index_copy: https://blog.csdn.net/hjxu2016/article/details/130161239\n",
    "                # means insert k into cache_k with input_pos in dim 2\n",
    "                k = cache_k.index_copy(2, input_pos, k)\n",
    "                v = cache_v.index_copy(2, input_pos, v)\n",
    "                kv_cache = k, v\n",
    "            \n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "            y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            y = self.c_proj(y)\n",
    "            return y, kv_cache\n",
    "    \n",
    "##### 2.5.3 silu mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config:LLaMAConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embed\n",
    "        # 2/3 hidden_dim\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        # let n_hidden is multiple 256\n",
    "        if n_hidden % 256 != 0:\n",
    "            n_hidden = n_hidden + 256 - (n_hidden % 256)\n",
    "        \n",
    "        self.c_fc1 = nn.Linear(config.n_embed, n_hidden, bias = False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embed, n_hidden, bias = False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embed, bias = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "##### 2.5.3 LLaMA block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig):\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embed)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ):\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x),\n",
    "                                    rope,\n",
    "                                    mask,\n",
    "                                    max_seq_length,\n",
    "                                    input_pos,\n",
    "                                    kv_cache)\n",
    "        # short cut 1\n",
    "        x = x + h\n",
    "        # short cut 2\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache\n",
    "    \n",
    "##### 2.5.3 LLaMA model\n",
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig):\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embed),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embed),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            idx: torch.Tensor,\n",
    "            targets: torch.tensor = None,\n",
    "            max_seq_length: Optional[int] = None,\n",
    "            input_pos: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        B, T = idx.size()\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            # index_select: https://pytorch.org/docs/stable/generated/torch.index_select.html\n",
    "            # rope: (T, n_embed/2, 2)\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            # mask: (1, 1, :T, :T)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "        \n",
    "        x = self.transformer.wte(idx)\n",
    "        # during training, we will not use qvcache\n",
    "        if input_pos is None:\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            # kvcaches: [layer1:(k,v), layer2:(k,v), layer3:(k,v)]\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embed // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                # initial kv_cache\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # we only need to calculate the next token\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "         # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused = True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f'using fused adamW: {use_fused}')\n",
    "        return optimizer\n",
    "    \n",
    "    def get_num_params(self, non_embedding = True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wte.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops (Floating Point Operations) utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        # get the attn shape\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embed // cfg.n_head, cfg.block_size\n",
    "        # per token calculated\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        # fwd and pwd need to iter every token\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "    \n",
    "    @classmethod\n",
    "    def from_name(cls, name: str):\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "    \n",
    "    def build_rope_cache(self, idx: torch.tensor):\n",
    "        return bulid_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_embed=self.config.n_embed // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx):\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        # make a tril angle matrix -> (1, 1, block_size, block_size)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: \n",
      "\t(1003854,)\n",
      "train_data shape: \n",
      "\t(111540,)\n",
      "model config: \n",
      "\tLLaMAConfig(block_size=1024, vocab_size=100, padded_vocab_size=128, n_layer=2, n_head=8, n_embed=128, bias=False, dropout=0.0)\n",
      "model arc: \n",
      "\tLLaMA(\n",
      "  (lm_head): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(128, 128)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_fc2): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      ")\n",
      "config_keys: \n",
      "\t ['batch_size', 'seq_len', 'n_embed', 'n_head', 'base', 'input_len', 'vocab_size', 'embedding_size', 'embedding_dim', 'work_dir', 'data_dir', 'out_dir', 'backend', 'gradient_accumulation_steps', 'block_size', 'max_iters', 'log_interval', 'device', 'dtype', 'iter_num', 'best_val_loss', 'always_save_checkpoint', 'weight_decay', 'learning_rate', 'decay_lr', 'beta1', 'beta2', 'warmup_iters', 'lr_decay_iters', 'min_lr', 'grad_clip', 'eval_interval', 'eval_iters', 'eval_only', 'ddp', 'master_process', 'seed_offset', 'ddp_world_size', 'tokens_per_iter', 'device_type', 'meta_path', 'meta_vocab_size', 't0', 'local_iter_num', 'running_mfu', 'lr', 'micro_step', 't1', 'dt', 'lossf']\n",
      "config: \n",
      "\t {'batch_size': 12, 'seq_len': 17, 'n_embed': 128, 'n_head': 8, 'base': 10000, 'input_len': 10, 'vocab_size': 100, 'embedding_size': 128, 'embedding_dim': 18, 'work_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA', 'data_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA/data/shakespeare_char', 'out_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA/results', 'backend': 'nccl', 'gradient_accumulation_steps': 40, 'block_size': 1024, 'max_iters': 1000, 'log_interval': 1, 'device': 'cuda', 'dtype': 'bfloat16', 'iter_num': 0, 'best_val_loss': 1e-09, 'always_save_checkpoint': False, 'weight_decay': 0.1, 'learning_rate': 0.0006, 'decay_lr': True, 'beta1': 0.9, 'beta2': 0.95, 'warmup_iters': 2000, 'lr_decay_iters': 1000, 'min_lr': 6e-05, 'grad_clip': 1.0, 'eval_interval': 2000, 'eval_iters': 200, 'eval_only': False, 'ddp': False, 'master_process': True, 'seed_offset': 0, 'ddp_world_size': 1, 'tokens_per_iter': 491520, 'device_type': 'cuda', 'meta_path': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA/data/shakespeare_char/meta.pkl', 'meta_vocab_size': 65, 't0': 1724085029.962613, 'local_iter_num': 5, 'running_mfu': -1.0, 'lr': 1.4999999999999998e-06, 'micro_step': 39, 't1': 1724085029.962613, 'dt': 0.26012682914733887, 'lossf': 5.001888275146484}\n",
      "gradient_accumulation_steps: 40\n",
      "ddp_world_size: 1\n",
      "batch_size: 12\n",
      "block_size: 1024\n",
      "tokens per iter: 491520\n",
      "meta data: /public/share/sd23/d2l/AI_study/LLaMA/LLaMA/data/shakespeare_char/meta.pkl\n",
      "found vocab_size = 65 (inside /public/share/sd23/d2l/AI_study/LLaMA/LLaMA/data/shakespeare_char/meta.pkl)\n",
      "num decayed parameter tensors: 12, with 557,056 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,664 parameters\n",
      "using fused adamW: True\n",
      "Compiling the model>>>\n",
      "X shape: torch.Size([12, 1024]), X: tensor([39, 56,  1, 53, 44,  1, 59, 57,  0, 13], device='cuda:0')\n",
      "Y shape: torch.Size([12, 1024]), Y: tensor([56,  1, 53, 44,  1, 59, 57,  0, 13, 52], device='cuda:0')\n",
      "step 0: train loss 5.0038, val loss 4.9985\n",
      "iter 0: loss 5.0165, time 12359.75ms, mfu -100.00%\n",
      "iter 1: loss 5.0175, time 257.89ms, mfu -100.00%\n",
      "iter 2: loss 4.9984, time 262.88ms, mfu -100.00%\n",
      "iter 3: loss 5.0041, time 267.00ms, mfu -100.00%\n",
      "iter 4: loss 5.0100, time 301.05ms, mfu -100.00%\n",
      "iter 5: loss 5.0019, time 218.96ms, mfu 4.60%\n",
      "iter 6: loss 5.0052, time 265.07ms, mfu 4.52%\n",
      "iter 7: loss 4.9938, time 262.00ms, mfu 4.46%\n",
      "iter 8: loss 5.0078, time 246.02ms, mfu 4.42%\n",
      "iter 9: loss 5.0049, time 271.39ms, mfu 4.35%\n",
      "iter 10: loss 4.9945, time 247.85ms, mfu 4.32%\n",
      "iter 11: loss 5.0132, time 255.34ms, mfu 4.28%\n",
      "iter 12: loss 5.0018, time 254.80ms, mfu 4.25%\n",
      "iter 13: loss 5.0045, time 263.07ms, mfu 4.21%\n",
      "iter 14: loss 4.9837, time 252.77ms, mfu 4.19%\n",
      "iter 15: loss 5.0011, time 264.79ms, mfu 4.15%\n",
      "iter 16: loss 4.9984, time 256.98ms, mfu 4.13%\n",
      "iter 17: loss 4.9858, time 257.92ms, mfu 4.11%\n",
      "iter 18: loss 5.0009, time 262.88ms, mfu 4.08%\n",
      "iter 19: loss 5.0046, time 258.49ms, mfu 4.06%\n",
      "iter 20: loss 4.9894, time 257.81ms, mfu 4.05%\n",
      "iter 21: loss 4.9851, time 253.77ms, mfu 4.04%\n",
      "iter 22: loss 4.9872, time 272.46ms, mfu 4.00%\n",
      "iter 23: loss 4.9895, time 245.73ms, mfu 4.01%\n",
      "iter 24: loss 4.9800, time 258.27ms, mfu 4.00%\n",
      "iter 25: loss 4.9824, time 261.42ms, mfu 3.99%\n",
      "iter 26: loss 4.9836, time 258.51ms, mfu 3.98%\n",
      "iter 27: loss 4.9877, time 255.17ms, mfu 3.98%\n",
      "iter 28: loss 4.9822, time 264.93ms, mfu 3.96%\n",
      "iter 29: loss 4.9844, time 268.88ms, mfu 3.94%\n",
      "iter 30: loss 4.9736, time 257.11ms, mfu 3.94%\n",
      "iter 31: loss 4.9735, time 265.49ms, mfu 3.92%\n",
      "iter 32: loss 4.9525, time 253.16ms, mfu 3.93%\n",
      "iter 33: loss 4.9658, time 284.90ms, mfu 3.89%\n",
      "iter 34: loss 4.9617, time 237.62ms, mfu 3.93%\n",
      "iter 35: loss 4.9620, time 259.20ms, mfu 3.92%\n",
      "iter 36: loss 4.9639, time 258.73ms, mfu 3.92%\n",
      "iter 37: loss 4.9686, time 259.80ms, mfu 3.92%\n",
      "iter 38: loss 4.9450, time 262.96ms, mfu 3.91%\n",
      "iter 39: loss 4.9443, time 254.34ms, mfu 3.91%\n",
      "iter 40: loss 4.9488, time 262.54ms, mfu 3.91%\n",
      "iter 41: loss 4.9436, time 259.78ms, mfu 3.90%\n",
      "iter 42: loss 4.9403, time 261.72ms, mfu 3.90%\n",
      "iter 43: loss 4.9271, time 277.80ms, mfu 3.87%\n",
      "iter 44: loss 4.9378, time 239.12ms, mfu 3.91%\n",
      "iter 45: loss 4.9391, time 263.56ms, mfu 3.90%\n",
      "iter 46: loss 4.9192, time 264.23ms, mfu 3.89%\n",
      "iter 47: loss 4.9310, time 253.63ms, mfu 3.90%\n",
      "iter 48: loss 4.9207, time 282.99ms, mfu 3.86%\n",
      "iter 49: loss 4.9327, time 253.19ms, mfu 3.88%\n",
      "iter 50: loss 4.9146, time 250.71ms, mfu 3.89%\n",
      "iter 51: loss 4.9104, time 261.66ms, mfu 3.89%\n",
      "iter 52: loss 4.9017, time 259.65ms, mfu 3.89%\n",
      "iter 53: loss 4.9136, time 270.76ms, mfu 3.87%\n",
      "iter 54: loss 4.8960, time 256.53ms, mfu 3.88%\n",
      "iter 55: loss 4.8995, time 264.07ms, mfu 3.87%\n",
      "iter 56: loss 4.8909, time 255.85ms, mfu 3.88%\n",
      "iter 57: loss 4.8813, time 265.01ms, mfu 3.87%\n",
      "iter 58: loss 4.8799, time 263.12ms, mfu 3.87%\n",
      "iter 59: loss 4.8808, time 261.18ms, mfu 3.87%\n",
      "iter 60: loss 4.8720, time 260.19ms, mfu 3.87%\n",
      "iter 61: loss 4.8857, time 260.14ms, mfu 3.87%\n",
      "iter 62: loss 4.8673, time 267.22ms, mfu 3.86%\n",
      "iter 63: loss 4.8636, time 266.07ms, mfu 3.85%\n",
      "iter 64: loss 4.8703, time 254.51ms, mfu 3.86%\n",
      "iter 65: loss 4.8480, time 261.57ms, mfu 3.86%\n",
      "iter 66: loss 4.8513, time 261.02ms, mfu 3.86%\n",
      "iter 67: loss 4.8454, time 259.88ms, mfu 3.86%\n",
      "iter 68: loss 4.8308, time 261.82ms, mfu 3.86%\n",
      "iter 69: loss 4.8243, time 260.76ms, mfu 3.86%\n",
      "iter 70: loss 4.8186, time 262.40ms, mfu 3.86%\n",
      "iter 71: loss 4.8279, time 262.17ms, mfu 3.86%\n",
      "iter 72: loss 4.8337, time 262.30ms, mfu 3.86%\n",
      "iter 73: loss 4.8207, time 265.82ms, mfu 3.85%\n",
      "iter 74: loss 4.7975, time 258.27ms, mfu 3.86%\n",
      "iter 75: loss 4.7937, time 262.64ms, mfu 3.85%\n",
      "iter 76: loss 4.7989, time 267.06ms, mfu 3.85%\n",
      "iter 77: loss 4.7861, time 256.56ms, mfu 3.85%\n",
      "iter 78: loss 4.7822, time 278.60ms, mfu 3.83%\n",
      "iter 79: loss 4.7681, time 261.77ms, mfu 3.83%\n",
      "iter 80: loss 4.7711, time 253.96ms, mfu 3.85%\n",
      "iter 81: loss 4.7663, time 267.54ms, mfu 3.84%\n",
      "iter 82: loss 4.7670, time 260.09ms, mfu 3.84%\n",
      "iter 83: loss 4.7512, time 269.24ms, mfu 3.83%\n",
      "iter 84: loss 4.7597, time 256.44ms, mfu 3.84%\n",
      "iter 85: loss 4.7408, time 262.39ms, mfu 3.84%\n",
      "iter 86: loss 4.7330, time 262.84ms, mfu 3.84%\n",
      "iter 87: loss 4.7320, time 263.36ms, mfu 3.84%\n",
      "iter 88: loss 4.7292, time 270.90ms, mfu 3.83%\n",
      "iter 89: loss 4.7171, time 257.76ms, mfu 3.84%\n",
      "iter 90: loss 4.7145, time 262.98ms, mfu 3.84%\n",
      "iter 91: loss 4.7018, time 263.22ms, mfu 3.84%\n",
      "iter 92: loss 4.6947, time 278.49ms, mfu 3.81%\n",
      "iter 93: loss 4.6956, time 246.99ms, mfu 3.84%\n",
      "iter 94: loss 4.6951, time 266.17ms, mfu 3.84%\n",
      "iter 95: loss 4.6774, time 272.21ms, mfu 3.82%\n",
      "iter 96: loss 4.6735, time 277.11ms, mfu 3.80%\n",
      "iter 97: loss 4.6754, time 242.49ms, mfu 3.84%\n",
      "iter 98: loss 4.6488, time 264.00ms, mfu 3.84%\n",
      "iter 99: loss 4.6529, time 267.02ms, mfu 3.83%\n",
      "iter 100: loss 4.6455, time 264.88ms, mfu 3.83%\n",
      "iter 101: loss 4.6377, time 260.54ms, mfu 3.83%\n",
      "iter 102: loss 4.6302, time 266.47ms, mfu 3.83%\n",
      "iter 103: loss 4.6033, time 261.64ms, mfu 3.83%\n",
      "iter 104: loss 4.6083, time 273.18ms, mfu 3.82%\n",
      "iter 105: loss 4.6046, time 274.19ms, mfu 3.80%\n",
      "iter 106: loss 4.5924, time 245.03ms, mfu 3.83%\n",
      "iter 107: loss 4.5832, time 266.81ms, mfu 3.83%\n",
      "iter 108: loss 4.5807, time 262.27ms, mfu 3.83%\n",
      "iter 109: loss 4.5634, time 266.31ms, mfu 3.83%\n",
      "iter 110: loss 4.5475, time 261.92ms, mfu 3.83%\n",
      "iter 111: loss 4.5565, time 265.27ms, mfu 3.83%\n",
      "iter 112: loss 4.5172, time 269.07ms, mfu 3.82%\n",
      "iter 113: loss 4.5350, time 273.09ms, mfu 3.80%\n",
      "iter 114: loss 4.5184, time 263.89ms, mfu 3.81%\n",
      "iter 115: loss 4.5166, time 261.45ms, mfu 3.81%\n",
      "iter 116: loss 4.4873, time 264.31ms, mfu 3.81%\n",
      "iter 117: loss 4.4892, time 264.45ms, mfu 3.81%\n",
      "iter 118: loss 4.4767, time 268.46ms, mfu 3.81%\n",
      "iter 119: loss 4.4708, time 264.44ms, mfu 3.81%\n",
      "iter 120: loss 4.4679, time 273.29ms, mfu 3.80%\n",
      "iter 121: loss 4.4762, time 255.40ms, mfu 3.81%\n",
      "iter 122: loss 4.4502, time 268.95ms, mfu 3.80%\n",
      "iter 123: loss 4.4270, time 260.65ms, mfu 3.81%\n",
      "iter 124: loss 4.4295, time 268.29ms, mfu 3.81%\n",
      "iter 125: loss 4.4245, time 275.50ms, mfu 3.79%\n",
      "iter 126: loss 4.4381, time 263.21ms, mfu 3.79%\n",
      "iter 127: loss 4.3788, time 262.16ms, mfu 3.80%\n",
      "iter 128: loss 4.3960, time 265.75ms, mfu 3.80%\n",
      "iter 129: loss 4.3876, time 267.56ms, mfu 3.80%\n",
      "iter 130: loss 4.3585, time 268.15ms, mfu 3.79%\n",
      "iter 131: loss 4.3836, time 287.33ms, mfu 3.76%\n",
      "iter 132: loss 4.3393, time 246.83ms, mfu 3.80%\n",
      "iter 133: loss 4.3445, time 261.14ms, mfu 3.80%\n",
      "iter 134: loss 4.3312, time 272.74ms, mfu 3.79%\n",
      "iter 135: loss 4.3099, time 272.60ms, mfu 3.78%\n",
      "iter 136: loss 4.3135, time 273.52ms, mfu 3.77%\n",
      "iter 137: loss 4.3024, time 269.59ms, mfu 3.77%\n",
      "iter 138: loss 4.2908, time 252.84ms, mfu 3.79%\n",
      "iter 139: loss 4.2534, time 274.72ms, mfu 3.78%\n",
      "iter 140: loss 4.2519, time 258.89ms, mfu 3.79%\n",
      "iter 141: loss 4.2405, time 268.54ms, mfu 3.79%\n",
      "iter 142: loss 4.2186, time 264.27ms, mfu 3.79%\n",
      "iter 143: loss 4.2129, time 274.98ms, mfu 3.78%\n",
      "iter 144: loss 4.2369, time 265.34ms, mfu 3.78%\n",
      "iter 145: loss 4.1908, time 265.68ms, mfu 3.78%\n",
      "iter 146: loss 4.1706, time 267.64ms, mfu 3.78%\n",
      "iter 147: loss 4.1663, time 268.35ms, mfu 3.78%\n",
      "iter 148: loss 4.1863, time 266.48ms, mfu 3.78%\n",
      "iter 149: loss 4.1653, time 268.03ms, mfu 3.78%\n",
      "iter 150: loss 4.1693, time 279.61ms, mfu 3.76%\n",
      "iter 151: loss 4.1093, time 268.08ms, mfu 3.76%\n",
      "iter 152: loss 4.1261, time 273.15ms, mfu 3.75%\n",
      "iter 153: loss 4.0909, time 267.33ms, mfu 3.75%\n",
      "iter 154: loss 4.0529, time 265.63ms, mfu 3.76%\n",
      "iter 155: loss 4.0780, time 273.31ms, mfu 3.75%\n",
      "iter 156: loss 4.0611, time 277.58ms, mfu 3.74%\n",
      "iter 157: loss 4.0583, time 257.62ms, mfu 3.76%\n",
      "iter 158: loss 4.0519, time 267.77ms, mfu 3.76%\n",
      "iter 159: loss 4.0184, time 266.56ms, mfu 3.76%\n",
      "iter 160: loss 4.0233, time 273.61ms, mfu 3.75%\n",
      "iter 161: loss 3.9948, time 262.40ms, mfu 3.76%\n",
      "iter 162: loss 3.9903, time 274.63ms, mfu 3.75%\n",
      "iter 163: loss 3.9503, time 275.26ms, mfu 3.74%\n",
      "iter 164: loss 3.9767, time 272.68ms, mfu 3.74%\n",
      "iter 165: loss 3.9314, time 256.51ms, mfu 3.76%\n",
      "iter 166: loss 3.9494, time 281.48ms, mfu 3.74%\n",
      "iter 167: loss 3.9300, time 260.61ms, mfu 3.75%\n",
      "iter 168: loss 3.9308, time 271.08ms, mfu 3.75%\n",
      "iter 169: loss 3.9163, time 275.66ms, mfu 3.74%\n",
      "iter 170: loss 3.8912, time 261.94ms, mfu 3.75%\n",
      "iter 171: loss 3.8534, time 275.73ms, mfu 3.74%\n",
      "iter 172: loss 3.8640, time 266.77ms, mfu 3.75%\n",
      "iter 173: loss 3.8703, time 268.12ms, mfu 3.75%\n",
      "iter 174: loss 3.8294, time 273.42ms, mfu 3.74%\n",
      "iter 175: loss 3.8167, time 266.03ms, mfu 3.75%\n",
      "iter 176: loss 3.8447, time 264.94ms, mfu 3.75%\n",
      "iter 177: loss 3.8092, time 268.63ms, mfu 3.75%\n",
      "iter 178: loss 3.7778, time 269.85ms, mfu 3.75%\n",
      "iter 179: loss 3.7571, time 270.60ms, mfu 3.75%\n",
      "iter 180: loss 3.7307, time 271.24ms, mfu 3.74%\n",
      "iter 181: loss 3.7590, time 270.11ms, mfu 3.74%\n",
      "iter 182: loss 3.7384, time 284.93ms, mfu 3.72%\n",
      "iter 183: loss 3.7246, time 274.84ms, mfu 3.72%\n",
      "iter 184: loss 3.7224, time 281.77ms, mfu 3.70%\n",
      "iter 185: loss 3.6966, time 263.56ms, mfu 3.72%\n",
      "iter 186: loss 3.6862, time 275.58ms, mfu 3.71%\n",
      "iter 187: loss 3.6746, time 274.17ms, mfu 3.71%\n",
      "iter 188: loss 3.6647, time 277.05ms, mfu 3.70%\n",
      "iter 189: loss 3.6216, time 278.57ms, mfu 3.69%\n",
      "iter 190: loss 3.6467, time 280.58ms, mfu 3.68%\n",
      "iter 191: loss 3.6721, time 266.49ms, mfu 3.69%\n",
      "iter 192: loss 3.6156, time 278.21ms, mfu 3.69%\n",
      "iter 193: loss 3.6600, time 277.03ms, mfu 3.68%\n",
      "iter 194: loss 3.6056, time 275.69ms, mfu 3.68%\n",
      "iter 195: loss 3.5681, time 291.13ms, mfu 3.66%\n",
      "iter 196: loss 3.5259, time 274.39ms, mfu 3.66%\n",
      "iter 197: loss 3.5163, time 283.17ms, mfu 3.65%\n",
      "iter 198: loss 3.5753, time 280.77ms, mfu 3.64%\n",
      "iter 199: loss 3.5710, time 275.20ms, mfu 3.65%\n",
      "iter 200: loss 3.5202, time 280.39ms, mfu 3.64%\n",
      "iter 201: loss 3.4920, time 278.08ms, mfu 3.64%\n",
      "iter 202: loss 3.4970, time 286.39ms, mfu 3.63%\n",
      "iter 203: loss 3.4682, time 273.25ms, mfu 3.63%\n",
      "iter 204: loss 3.4785, time 290.88ms, mfu 3.62%\n",
      "iter 205: loss 3.4813, time 272.21ms, mfu 3.63%\n",
      "iter 206: loss 3.4756, time 283.18ms, mfu 3.62%\n",
      "iter 207: loss 3.4476, time 287.63ms, mfu 3.61%\n",
      "iter 208: loss 3.4356, time 277.42ms, mfu 3.61%\n",
      "iter 209: loss 3.4739, time 281.59ms, mfu 3.61%\n",
      "iter 210: loss 3.4577, time 281.46ms, mfu 3.60%\n",
      "iter 211: loss 3.3982, time 289.03ms, mfu 3.59%\n",
      "iter 212: loss 3.3921, time 275.49ms, mfu 3.60%\n",
      "iter 213: loss 3.3862, time 285.20ms, mfu 3.59%\n",
      "iter 214: loss 3.3377, time 287.61ms, mfu 3.58%\n",
      "iter 215: loss 3.3848, time 278.60ms, mfu 3.59%\n",
      "iter 216: loss 3.3621, time 286.17ms, mfu 3.58%\n",
      "iter 217: loss 3.3651, time 285.54ms, mfu 3.58%\n",
      "iter 218: loss 3.3402, time 282.85ms, mfu 3.58%\n",
      "iter 219: loss 3.3267, time 279.06ms, mfu 3.58%\n",
      "iter 220: loss 3.3065, time 295.51ms, mfu 3.56%\n",
      "iter 221: loss 3.2962, time 284.83ms, mfu 3.56%\n",
      "iter 222: loss 3.3303, time 296.09ms, mfu 3.54%\n",
      "iter 223: loss 3.3260, time 281.09ms, mfu 3.55%\n",
      "iter 224: loss 3.2877, time 281.41ms, mfu 3.55%\n",
      "iter 225: loss 3.2520, time 283.51ms, mfu 3.55%\n",
      "iter 226: loss 3.2488, time 286.58ms, mfu 3.55%\n",
      "iter 227: loss 3.2765, time 283.32ms, mfu 3.55%\n",
      "iter 228: loss 3.2360, time 285.99ms, mfu 3.55%\n",
      "iter 229: loss 3.2400, time 283.05ms, mfu 3.55%\n",
      "iter 230: loss 3.2295, time 287.75ms, mfu 3.54%\n",
      "iter 231: loss 3.2055, time 286.82ms, mfu 3.54%\n",
      "iter 232: loss 3.2599, time 278.57ms, mfu 3.55%\n",
      "iter 233: loss 3.2386, time 283.63ms, mfu 3.55%\n",
      "iter 234: loss 3.2142, time 291.37ms, mfu 3.54%\n",
      "iter 235: loss 3.1833, time 277.74ms, mfu 3.55%\n",
      "iter 236: loss 3.2262, time 285.88ms, mfu 3.55%\n",
      "iter 237: loss 3.1783, time 283.73ms, mfu 3.55%\n",
      "iter 238: loss 3.1935, time 287.86ms, mfu 3.54%\n",
      "iter 239: loss 3.1608, time 298.99ms, mfu 3.53%\n",
      "iter 240: loss 3.1573, time 292.42ms, mfu 3.52%\n",
      "iter 241: loss 3.1497, time 279.32ms, mfu 3.53%\n",
      "iter 242: loss 3.1417, time 285.27ms, mfu 3.53%\n",
      "iter 243: loss 3.0961, time 289.75ms, mfu 3.52%\n",
      "iter 244: loss 3.1240, time 291.43ms, mfu 3.52%\n",
      "iter 245: loss 3.0961, time 282.08ms, mfu 3.52%\n",
      "iter 246: loss 3.1202, time 291.57ms, mfu 3.52%\n",
      "iter 247: loss 3.0883, time 280.93ms, mfu 3.52%\n",
      "iter 248: loss 3.1272, time 290.70ms, mfu 3.52%\n",
      "iter 249: loss 3.1053, time 288.50ms, mfu 3.52%\n",
      "iter 250: loss 3.0713, time 280.81ms, mfu 3.52%\n",
      "iter 251: loss 3.0883, time 289.06ms, mfu 3.52%\n",
      "iter 252: loss 3.1003, time 295.23ms, mfu 3.51%\n",
      "iter 253: loss 3.0595, time 291.28ms, mfu 3.50%\n",
      "iter 254: loss 3.0873, time 287.43ms, mfu 3.50%\n",
      "iter 255: loss 3.0527, time 286.81ms, mfu 3.51%\n",
      "iter 256: loss 3.1104, time 286.93ms, mfu 3.51%\n",
      "iter 257: loss 3.0575, time 292.83ms, mfu 3.50%\n",
      "iter 258: loss 3.0236, time 278.64ms, mfu 3.51%\n",
      "iter 259: loss 3.0714, time 300.20ms, mfu 3.50%\n",
      "iter 260: loss 3.0376, time 282.64ms, mfu 3.50%\n",
      "iter 261: loss 3.0587, time 287.45ms, mfu 3.50%\n",
      "iter 262: loss 3.0034, time 287.20ms, mfu 3.50%\n",
      "iter 263: loss 2.9857, time 287.48ms, mfu 3.50%\n",
      "iter 264: loss 2.9977, time 286.10ms, mfu 3.51%\n",
      "iter 265: loss 2.9765, time 285.77ms, mfu 3.51%\n",
      "iter 266: loss 2.9836, time 288.67ms, mfu 3.51%\n",
      "iter 267: loss 2.9634, time 285.37ms, mfu 3.51%\n",
      "iter 268: loss 3.0017, time 286.51ms, mfu 3.51%\n",
      "iter 269: loss 2.9514, time 288.69ms, mfu 3.51%\n",
      "iter 270: loss 2.9252, time 287.55ms, mfu 3.51%\n",
      "iter 271: loss 2.9756, time 287.45ms, mfu 3.51%\n",
      "iter 272: loss 2.9424, time 286.57ms, mfu 3.51%\n",
      "iter 273: loss 2.9457, time 296.92ms, mfu 3.50%\n",
      "iter 274: loss 2.9682, time 288.96ms, mfu 3.50%\n",
      "iter 275: loss 2.9510, time 285.07ms, mfu 3.50%\n",
      "iter 276: loss 2.9485, time 295.91ms, mfu 3.49%\n",
      "iter 277: loss 2.9698, time 286.33ms, mfu 3.49%\n",
      "iter 278: loss 2.8665, time 286.17ms, mfu 3.50%\n",
      "iter 279: loss 2.9450, time 290.35ms, mfu 3.50%\n",
      "iter 280: loss 2.9266, time 289.98ms, mfu 3.49%\n",
      "iter 281: loss 2.9047, time 298.38ms, mfu 3.48%\n",
      "iter 282: loss 2.9346, time 280.90ms, mfu 3.49%\n",
      "iter 283: loss 2.9474, time 284.95ms, mfu 3.50%\n",
      "iter 284: loss 2.8617, time 286.42ms, mfu 3.50%\n",
      "iter 285: loss 2.9172, time 298.34ms, mfu 3.49%\n",
      "iter 286: loss 2.8209, time 292.14ms, mfu 3.48%\n",
      "iter 287: loss 2.8892, time 292.31ms, mfu 3.48%\n",
      "iter 288: loss 2.8815, time 299.18ms, mfu 3.47%\n",
      "iter 289: loss 2.8667, time 288.00ms, mfu 3.47%\n",
      "iter 290: loss 2.8860, time 298.44ms, mfu 3.46%\n",
      "iter 291: loss 2.8527, time 286.64ms, mfu 3.47%\n",
      "iter 292: loss 2.8473, time 290.45ms, mfu 3.47%\n",
      "iter 293: loss 2.8090, time 293.44ms, mfu 3.47%\n",
      "iter 294: loss 2.8549, time 292.06ms, mfu 3.46%\n",
      "iter 295: loss 2.8571, time 295.50ms, mfu 3.46%\n",
      "iter 296: loss 2.8420, time 292.65ms, mfu 3.46%\n",
      "iter 297: loss 2.8257, time 290.08ms, mfu 3.46%\n",
      "iter 298: loss 2.8386, time 289.44ms, mfu 3.46%\n",
      "iter 299: loss 2.8131, time 290.32ms, mfu 3.46%\n",
      "iter 300: loss 2.8070, time 297.77ms, mfu 3.46%\n",
      "iter 301: loss 2.8229, time 290.31ms, mfu 3.46%\n",
      "iter 302: loss 2.8420, time 292.52ms, mfu 3.46%\n",
      "iter 303: loss 2.7812, time 289.04ms, mfu 3.46%\n",
      "iter 304: loss 2.8192, time 289.44ms, mfu 3.46%\n",
      "iter 305: loss 2.8338, time 291.29ms, mfu 3.46%\n",
      "iter 306: loss 2.8193, time 290.10ms, mfu 3.46%\n",
      "iter 307: loss 2.8141, time 289.43ms, mfu 3.46%\n",
      "iter 308: loss 2.7957, time 291.44ms, mfu 3.46%\n",
      "iter 309: loss 2.7630, time 289.52ms, mfu 3.47%\n",
      "iter 310: loss 2.7557, time 289.27ms, mfu 3.47%\n",
      "iter 311: loss 2.7557, time 292.01ms, mfu 3.47%\n",
      "iter 312: loss 2.7858, time 288.49ms, mfu 3.47%\n",
      "iter 313: loss 2.7871, time 302.36ms, mfu 3.46%\n",
      "iter 314: loss 2.7949, time 282.02ms, mfu 3.47%\n",
      "iter 315: loss 2.7966, time 289.51ms, mfu 3.47%\n",
      "iter 316: loss 2.7493, time 287.22ms, mfu 3.47%\n",
      "iter 317: loss 2.7678, time 287.25ms, mfu 3.48%\n",
      "iter 318: loss 2.7459, time 291.97ms, mfu 3.47%\n",
      "iter 319: loss 2.7413, time 283.30ms, mfu 3.48%\n",
      "iter 320: loss 2.7486, time 292.05ms, mfu 3.48%\n",
      "iter 321: loss 2.7680, time 283.82ms, mfu 3.49%\n",
      "iter 322: loss 2.7345, time 285.11ms, mfu 3.49%\n",
      "iter 323: loss 2.7317, time 291.78ms, mfu 3.49%\n",
      "iter 324: loss 2.7303, time 279.25ms, mfu 3.50%\n",
      "iter 325: loss 2.7213, time 287.91ms, mfu 3.50%\n",
      "iter 326: loss 2.6825, time 287.98ms, mfu 3.50%\n",
      "iter 327: loss 2.7380, time 285.53ms, mfu 3.50%\n",
      "iter 328: loss 2.7010, time 286.93ms, mfu 3.50%\n",
      "iter 329: loss 2.7209, time 287.83ms, mfu 3.50%\n",
      "iter 330: loss 2.7558, time 285.09ms, mfu 3.51%\n",
      "iter 331: loss 2.6896, time 285.25ms, mfu 3.51%\n",
      "iter 332: loss 2.7017, time 287.73ms, mfu 3.51%\n",
      "iter 333: loss 2.7118, time 288.19ms, mfu 3.51%\n",
      "iter 334: loss 2.7073, time 287.00ms, mfu 3.51%\n",
      "iter 335: loss 2.7232, time 290.04ms, mfu 3.51%\n",
      "iter 336: loss 2.6772, time 286.39ms, mfu 3.51%\n",
      "iter 337: loss 2.7223, time 288.64ms, mfu 3.51%\n",
      "iter 338: loss 2.7026, time 292.04ms, mfu 3.50%\n",
      "iter 339: loss 2.6716, time 289.56ms, mfu 3.50%\n",
      "iter 340: loss 2.6940, time 284.64ms, mfu 3.50%\n",
      "iter 341: loss 2.6937, time 293.77ms, mfu 3.50%\n",
      "iter 342: loss 2.6632, time 292.32ms, mfu 3.49%\n",
      "iter 343: loss 2.6724, time 279.53ms, mfu 3.50%\n",
      "iter 344: loss 2.6557, time 293.36ms, mfu 3.50%\n",
      "iter 345: loss 2.6987, time 291.35ms, mfu 3.49%\n",
      "iter 346: loss 2.6678, time 285.35ms, mfu 3.50%\n",
      "iter 347: loss 2.6797, time 287.81ms, mfu 3.50%\n",
      "iter 348: loss 2.6579, time 281.66ms, mfu 3.51%\n",
      "iter 349: loss 2.6569, time 284.02ms, mfu 3.51%\n",
      "iter 350: loss 2.6721, time 291.00ms, mfu 3.51%\n",
      "iter 351: loss 2.6398, time 284.73ms, mfu 3.51%\n",
      "iter 352: loss 2.6596, time 298.85ms, mfu 3.50%\n",
      "iter 353: loss 2.6519, time 286.43ms, mfu 3.50%\n",
      "iter 354: loss 2.6112, time 287.83ms, mfu 3.50%\n",
      "iter 355: loss 2.6421, time 298.47ms, mfu 3.49%\n",
      "iter 356: loss 2.6065, time 278.04ms, mfu 3.50%\n",
      "iter 357: loss 2.6658, time 286.44ms, mfu 3.50%\n",
      "iter 358: loss 2.6658, time 289.08ms, mfu 3.50%\n",
      "iter 359: loss 2.6570, time 286.17ms, mfu 3.50%\n",
      "iter 360: loss 2.6279, time 295.65ms, mfu 3.49%\n",
      "iter 361: loss 2.6288, time 292.64ms, mfu 3.49%\n",
      "iter 362: loss 2.6410, time 290.25ms, mfu 3.49%\n",
      "iter 363: loss 2.6463, time 289.47ms, mfu 3.49%\n",
      "iter 364: loss 2.6197, time 287.74ms, mfu 3.49%\n",
      "iter 365: loss 2.6420, time 295.52ms, mfu 3.48%\n",
      "iter 366: loss 2.6315, time 294.69ms, mfu 3.47%\n",
      "iter 367: loss 2.6319, time 288.54ms, mfu 3.48%\n",
      "iter 368: loss 2.6383, time 297.59ms, mfu 3.47%\n",
      "iter 369: loss 2.6198, time 291.96ms, mfu 3.47%\n",
      "iter 370: loss 2.6061, time 291.73ms, mfu 3.47%\n",
      "iter 371: loss 2.6254, time 291.75ms, mfu 3.46%\n",
      "iter 372: loss 2.6164, time 292.15ms, mfu 3.46%\n",
      "iter 373: loss 2.6173, time 287.33ms, mfu 3.47%\n",
      "iter 374: loss 2.6444, time 290.72ms, mfu 3.47%\n",
      "iter 375: loss 2.5660, time 292.03ms, mfu 3.47%\n",
      "iter 376: loss 2.6100, time 297.07ms, mfu 3.46%\n",
      "iter 377: loss 2.5998, time 289.36ms, mfu 3.46%\n",
      "iter 378: loss 2.5887, time 283.78ms, mfu 3.47%\n",
      "iter 379: loss 2.6331, time 291.42ms, mfu 3.47%\n",
      "iter 380: loss 2.5798, time 290.44ms, mfu 3.47%\n",
      "iter 381: loss 2.5744, time 289.38ms, mfu 3.47%\n",
      "iter 382: loss 2.5887, time 290.98ms, mfu 3.47%\n",
      "iter 383: loss 2.5878, time 290.01ms, mfu 3.47%\n",
      "iter 384: loss 2.5827, time 290.99ms, mfu 3.47%\n",
      "iter 385: loss 2.5869, time 300.03ms, mfu 3.46%\n",
      "iter 386: loss 2.5791, time 294.41ms, mfu 3.46%\n",
      "iter 387: loss 2.5572, time 292.75ms, mfu 3.45%\n",
      "iter 388: loss 2.5579, time 289.38ms, mfu 3.46%\n",
      "iter 389: loss 2.5756, time 295.21ms, mfu 3.45%\n",
      "iter 390: loss 2.5693, time 284.37ms, mfu 3.46%\n",
      "iter 391: loss 2.5770, time 290.53ms, mfu 3.46%\n",
      "iter 392: loss 2.5731, time 288.57ms, mfu 3.47%\n",
      "iter 393: loss 2.5640, time 293.37ms, mfu 3.46%\n",
      "iter 394: loss 2.5557, time 289.36ms, mfu 3.47%\n",
      "iter 395: loss 2.5549, time 293.66ms, mfu 3.46%\n",
      "iter 396: loss 2.5186, time 279.86ms, mfu 3.48%\n",
      "iter 397: loss 2.5407, time 286.94ms, mfu 3.48%\n",
      "iter 398: loss 2.5337, time 289.88ms, mfu 3.48%\n",
      "iter 399: loss 2.5475, time 285.43ms, mfu 3.49%\n",
      "iter 400: loss 2.5232, time 288.27ms, mfu 3.49%\n",
      "iter 401: loss 2.5404, time 284.55ms, mfu 3.49%\n",
      "iter 402: loss 2.5262, time 286.62ms, mfu 3.49%\n",
      "iter 403: loss 2.5372, time 290.69ms, mfu 3.49%\n",
      "iter 404: loss 2.5124, time 288.08ms, mfu 3.49%\n",
      "iter 405: loss 2.5406, time 293.10ms, mfu 3.49%\n",
      "iter 406: loss 2.5387, time 278.83ms, mfu 3.50%\n",
      "iter 407: loss 2.5335, time 283.31ms, mfu 3.51%\n",
      "iter 408: loss 2.5124, time 285.91ms, mfu 3.51%\n",
      "iter 409: loss 2.5091, time 281.31ms, mfu 3.52%\n",
      "iter 410: loss 2.5132, time 296.11ms, mfu 3.50%\n",
      "iter 411: loss 2.4929, time 271.93ms, mfu 3.52%\n",
      "iter 412: loss 2.4815, time 285.85ms, mfu 3.53%\n",
      "iter 413: loss 2.5061, time 283.60ms, mfu 3.53%\n",
      "iter 414: loss 2.5005, time 295.01ms, mfu 3.52%\n",
      "iter 415: loss 2.4784, time 271.77ms, mfu 3.54%\n",
      "iter 416: loss 2.4743, time 281.39ms, mfu 3.54%\n",
      "iter 417: loss 2.5020, time 289.70ms, mfu 3.53%\n",
      "iter 418: loss 2.4809, time 279.20ms, mfu 3.54%\n",
      "iter 419: loss 2.4579, time 288.44ms, mfu 3.54%\n",
      "iter 420: loss 2.4862, time 281.05ms, mfu 3.54%\n",
      "iter 421: loss 2.4649, time 281.25ms, mfu 3.55%\n",
      "iter 422: loss 2.4736, time 281.16ms, mfu 3.55%\n",
      "iter 423: loss 2.4807, time 287.50ms, mfu 3.55%\n",
      "iter 424: loss 2.4803, time 289.82ms, mfu 3.54%\n",
      "iter 425: loss 2.4786, time 274.71ms, mfu 3.55%\n",
      "iter 426: loss 2.4628, time 283.16ms, mfu 3.55%\n",
      "iter 427: loss 2.4509, time 282.99ms, mfu 3.55%\n",
      "iter 428: loss 2.4533, time 282.64ms, mfu 3.56%\n",
      "iter 429: loss 2.4268, time 283.89ms, mfu 3.56%\n",
      "iter 430: loss 2.4362, time 291.84ms, mfu 3.55%\n",
      "iter 431: loss 2.4538, time 273.95ms, mfu 3.56%\n",
      "iter 432: loss 2.4621, time 282.11ms, mfu 3.56%\n",
      "iter 433: loss 2.4303, time 286.25ms, mfu 3.56%\n",
      "iter 434: loss 2.4486, time 282.91ms, mfu 3.56%\n",
      "iter 435: loss 2.4488, time 286.66ms, mfu 3.55%\n",
      "iter 436: loss 2.4179, time 282.78ms, mfu 3.55%\n",
      "iter 437: loss 2.4337, time 284.64ms, mfu 3.55%\n",
      "iter 438: loss 2.4378, time 281.55ms, mfu 3.56%\n",
      "iter 439: loss 2.4550, time 282.86ms, mfu 3.56%\n",
      "iter 440: loss 2.4224, time 280.95ms, mfu 3.56%\n",
      "iter 441: loss 2.4344, time 283.08ms, mfu 3.56%\n",
      "iter 442: loss 2.4013, time 283.21ms, mfu 3.56%\n",
      "iter 443: loss 2.4182, time 283.07ms, mfu 3.56%\n",
      "iter 444: loss 2.4323, time 283.81ms, mfu 3.56%\n",
      "iter 445: loss 2.4345, time 286.06ms, mfu 3.56%\n",
      "iter 446: loss 2.4424, time 289.81ms, mfu 3.55%\n",
      "iter 447: loss 2.4175, time 276.10ms, mfu 3.56%\n",
      "iter 448: loss 2.4077, time 295.46ms, mfu 3.54%\n",
      "iter 449: loss 2.3978, time 266.65ms, mfu 3.57%\n",
      "iter 450: loss 2.4360, time 283.29ms, mfu 3.57%\n",
      "iter 451: loss 2.4323, time 282.60ms, mfu 3.57%\n",
      "iter 452: loss 2.4230, time 284.85ms, mfu 3.56%\n",
      "iter 453: loss 2.4131, time 283.02ms, mfu 3.56%\n",
      "iter 454: loss 2.4231, time 282.04ms, mfu 3.57%\n",
      "iter 455: loss 2.4060, time 300.15ms, mfu 3.54%\n",
      "iter 456: loss 2.3899, time 278.49ms, mfu 3.55%\n",
      "iter 457: loss 2.4202, time 290.13ms, mfu 3.54%\n",
      "iter 458: loss 2.4447, time 281.94ms, mfu 3.55%\n",
      "iter 459: loss 2.3789, time 300.84ms, mfu 3.53%\n",
      "iter 460: loss 2.4017, time 265.69ms, mfu 3.55%\n",
      "iter 461: loss 2.3833, time 285.24ms, mfu 3.55%\n",
      "iter 462: loss 2.4080, time 286.18ms, mfu 3.55%\n",
      "iter 463: loss 2.4009, time 294.13ms, mfu 3.54%\n",
      "iter 464: loss 2.4017, time 287.82ms, mfu 3.53%\n",
      "iter 465: loss 2.3862, time 284.94ms, mfu 3.53%\n",
      "iter 466: loss 2.3834, time 288.17ms, mfu 3.53%\n",
      "iter 467: loss 2.3787, time 284.49ms, mfu 3.53%\n",
      "iter 468: loss 2.3830, time 283.91ms, mfu 3.53%\n",
      "iter 469: loss 2.3695, time 286.48ms, mfu 3.53%\n",
      "iter 470: loss 2.4003, time 285.98ms, mfu 3.53%\n",
      "iter 471: loss 2.4073, time 285.20ms, mfu 3.53%\n",
      "iter 472: loss 2.3798, time 287.49ms, mfu 3.53%\n",
      "iter 473: loss 2.3966, time 283.32ms, mfu 3.53%\n",
      "iter 474: loss 2.3560, time 290.15ms, mfu 3.53%\n",
      "iter 475: loss 2.4087, time 282.35ms, mfu 3.53%\n",
      "iter 476: loss 2.3622, time 288.23ms, mfu 3.53%\n",
      "iter 477: loss 2.3861, time 289.27ms, mfu 3.52%\n",
      "iter 478: loss 2.3570, time 282.79ms, mfu 3.53%\n",
      "iter 479: loss 2.3601, time 289.19ms, mfu 3.52%\n",
      "iter 480: loss 2.3366, time 283.45ms, mfu 3.53%\n",
      "iter 481: loss 2.3702, time 279.38ms, mfu 3.54%\n",
      "iter 482: loss 2.3655, time 293.29ms, mfu 3.53%\n",
      "iter 483: loss 2.3407, time 277.08ms, mfu 3.54%\n",
      "iter 484: loss 2.3244, time 283.60ms, mfu 3.54%\n",
      "iter 485: loss 2.3392, time 290.37ms, mfu 3.53%\n",
      "iter 486: loss 2.3705, time 282.40ms, mfu 3.54%\n",
      "iter 487: loss 2.3800, time 282.15ms, mfu 3.54%\n",
      "iter 488: loss 2.3395, time 283.89ms, mfu 3.54%\n",
      "iter 489: loss 2.3462, time 281.78ms, mfu 3.54%\n",
      "iter 490: loss 2.3454, time 282.42ms, mfu 3.55%\n",
      "iter 491: loss 2.3508, time 284.47ms, mfu 3.55%\n",
      "iter 492: loss 2.3522, time 282.95ms, mfu 3.55%\n",
      "iter 493: loss 2.3493, time 282.73ms, mfu 3.55%\n",
      "iter 494: loss 2.3414, time 284.24ms, mfu 3.55%\n",
      "iter 495: loss 2.3227, time 282.25ms, mfu 3.55%\n",
      "iter 496: loss 2.3355, time 279.97ms, mfu 3.56%\n",
      "iter 497: loss 2.3244, time 287.09ms, mfu 3.55%\n",
      "iter 498: loss 2.3166, time 276.14ms, mfu 3.56%\n",
      "iter 499: loss 2.3271, time 289.62ms, mfu 3.55%\n",
      "iter 500: loss 2.3092, time 279.68ms, mfu 3.56%\n",
      "iter 501: loss 2.3191, time 286.50ms, mfu 3.56%\n",
      "iter 502: loss 2.3085, time 281.56ms, mfu 3.56%\n",
      "iter 503: loss 2.3367, time 280.18ms, mfu 3.56%\n",
      "iter 504: loss 2.3203, time 278.28ms, mfu 3.57%\n",
      "iter 505: loss 2.3137, time 277.94ms, mfu 3.57%\n",
      "iter 506: loss 2.3037, time 286.00ms, mfu 3.57%\n",
      "iter 507: loss 2.2993, time 280.03ms, mfu 3.57%\n",
      "iter 508: loss 2.3398, time 275.91ms, mfu 3.58%\n",
      "iter 509: loss 2.3206, time 282.82ms, mfu 3.58%\n",
      "iter 510: loss 2.3058, time 285.60ms, mfu 3.57%\n",
      "iter 511: loss 2.3127, time 275.16ms, mfu 3.58%\n",
      "iter 512: loss 2.3050, time 279.38ms, mfu 3.59%\n",
      "iter 513: loss 2.2908, time 281.44ms, mfu 3.59%\n",
      "iter 514: loss 2.3038, time 276.97ms, mfu 3.59%\n",
      "iter 515: loss 2.3039, time 281.45ms, mfu 3.59%\n",
      "iter 516: loss 2.2931, time 278.41ms, mfu 3.59%\n",
      "iter 517: loss 2.3193, time 278.44ms, mfu 3.60%\n",
      "iter 518: loss 2.2873, time 277.69ms, mfu 3.60%\n",
      "iter 519: loss 2.2601, time 281.54ms, mfu 3.60%\n",
      "iter 520: loss 2.2985, time 281.84ms, mfu 3.60%\n",
      "iter 521: loss 2.2759, time 274.71ms, mfu 3.60%\n",
      "iter 522: loss 2.2790, time 285.04ms, mfu 3.60%\n",
      "iter 523: loss 2.2794, time 282.96ms, mfu 3.59%\n",
      "iter 524: loss 2.2592, time 274.73ms, mfu 3.60%\n",
      "iter 525: loss 2.2814, time 279.88ms, mfu 3.60%\n",
      "iter 526: loss 2.2518, time 280.14ms, mfu 3.60%\n",
      "iter 527: loss 2.2678, time 286.45ms, mfu 3.59%\n",
      "iter 528: loss 2.2568, time 269.99ms, mfu 3.61%\n",
      "iter 529: loss 2.2728, time 284.13ms, mfu 3.60%\n",
      "iter 530: loss 2.2552, time 273.45ms, mfu 3.61%\n",
      "iter 531: loss 2.2420, time 281.52ms, mfu 3.61%\n",
      "iter 532: loss 2.2583, time 278.48ms, mfu 3.61%\n",
      "iter 533: loss 2.2199, time 277.97ms, mfu 3.61%\n",
      "iter 534: loss 2.2691, time 287.95ms, mfu 3.60%\n",
      "iter 535: loss 2.2528, time 269.14ms, mfu 3.61%\n",
      "iter 536: loss 2.2510, time 280.74ms, mfu 3.61%\n",
      "iter 537: loss 2.2776, time 288.82ms, mfu 3.60%\n",
      "iter 538: loss 2.2113, time 273.06ms, mfu 3.61%\n",
      "iter 539: loss 2.2272, time 277.82ms, mfu 3.61%\n",
      "iter 540: loss 2.2173, time 283.48ms, mfu 3.61%\n",
      "iter 541: loss 2.2273, time 273.37ms, mfu 3.61%\n",
      "iter 542: loss 2.2390, time 280.15ms, mfu 3.61%\n",
      "iter 543: loss 2.2367, time 277.06ms, mfu 3.61%\n",
      "iter 544: loss 2.2084, time 279.01ms, mfu 3.61%\n",
      "iter 545: loss 2.2559, time 282.09ms, mfu 3.61%\n",
      "iter 546: loss 2.2017, time 279.61ms, mfu 3.61%\n",
      "iter 547: loss 2.2423, time 284.98ms, mfu 3.60%\n",
      "iter 548: loss 2.1975, time 273.52ms, mfu 3.61%\n",
      "iter 549: loss 2.2178, time 276.27ms, mfu 3.62%\n",
      "iter 550: loss 2.2307, time 280.99ms, mfu 3.61%\n",
      "iter 551: loss 2.2357, time 281.22ms, mfu 3.61%\n",
      "iter 552: loss 2.2045, time 281.12ms, mfu 3.61%\n",
      "iter 553: loss 2.2163, time 288.53ms, mfu 3.60%\n",
      "iter 554: loss 2.2046, time 276.91ms, mfu 3.60%\n",
      "iter 555: loss 2.2088, time 280.53ms, mfu 3.60%\n",
      "iter 556: loss 2.2042, time 280.43ms, mfu 3.60%\n",
      "iter 557: loss 2.1994, time 288.63ms, mfu 3.59%\n",
      "iter 558: loss 2.2457, time 280.41ms, mfu 3.59%\n",
      "iter 559: loss 2.1945, time 280.56ms, mfu 3.59%\n",
      "iter 560: loss 2.1928, time 282.96ms, mfu 3.59%\n",
      "iter 561: loss 2.2066, time 282.70ms, mfu 3.59%\n",
      "iter 562: loss 2.1977, time 287.97ms, mfu 3.58%\n",
      "iter 563: loss 2.1972, time 278.66ms, mfu 3.58%\n",
      "iter 564: loss 2.2087, time 286.44ms, mfu 3.57%\n",
      "iter 565: loss 2.1564, time 278.64ms, mfu 3.58%\n",
      "iter 566: loss 2.1878, time 291.30ms, mfu 3.57%\n",
      "iter 567: loss 2.1415, time 282.59ms, mfu 3.57%\n",
      "iter 568: loss 2.2198, time 285.76ms, mfu 3.56%\n",
      "iter 569: loss 2.1985, time 290.37ms, mfu 3.55%\n",
      "iter 570: loss 2.1738, time 281.66ms, mfu 3.56%\n",
      "iter 571: loss 2.1834, time 283.79ms, mfu 3.56%\n",
      "iter 572: loss 2.2217, time 280.72ms, mfu 3.56%\n",
      "iter 573: loss 2.1650, time 283.73ms, mfu 3.56%\n",
      "iter 574: loss 2.1602, time 282.26ms, mfu 3.56%\n",
      "iter 575: loss 2.1762, time 281.18ms, mfu 3.56%\n",
      "iter 576: loss 2.1610, time 286.78ms, mfu 3.56%\n",
      "iter 577: loss 2.1821, time 289.66ms, mfu 3.55%\n",
      "iter 578: loss 2.1983, time 287.86ms, mfu 3.55%\n",
      "iter 579: loss 2.1676, time 269.25ms, mfu 3.57%\n",
      "iter 580: loss 2.1839, time 281.15ms, mfu 3.57%\n",
      "iter 581: loss 2.1393, time 283.75ms, mfu 3.57%\n",
      "iter 582: loss 2.1102, time 284.72ms, mfu 3.56%\n",
      "iter 583: loss 2.1596, time 276.23ms, mfu 3.57%\n",
      "iter 584: loss 2.1686, time 288.02ms, mfu 3.57%\n",
      "iter 585: loss 2.1591, time 285.83ms, mfu 3.56%\n",
      "iter 586: loss 2.1641, time 273.77ms, mfu 3.57%\n",
      "iter 587: loss 2.1686, time 281.11ms, mfu 3.57%\n",
      "iter 588: loss 2.1381, time 277.09ms, mfu 3.58%\n",
      "iter 589: loss 2.1479, time 281.76ms, mfu 3.58%\n",
      "iter 590: loss 2.1511, time 279.16ms, mfu 3.58%\n",
      "iter 591: loss 2.1121, time 275.05ms, mfu 3.59%\n",
      "iter 592: loss 2.1349, time 280.66ms, mfu 3.59%\n",
      "iter 593: loss 2.1545, time 280.71ms, mfu 3.59%\n",
      "iter 594: loss 2.1108, time 280.87ms, mfu 3.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 283\u001b[0m\n\u001b[1;32m    281\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# if we choose to clip gradients, we need to unscale the gradients first to clip the right gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import pickle\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import time\n",
    "# 1. load data\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, 'data/shakespeare_char')\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "print(f'train_data shape: \\n\\t{train_data.shape}')\n",
    "print(f'train_data shape: \\n\\t{val_data.shape}')\n",
    "# 2.model config\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    # seq_len\n",
    "    block_size: int = 2048\n",
    "    # vocab_size\n",
    "    vocab_size: int = 100\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embed: int = 4096\n",
    "    bias: bool = False\n",
    "    dropout: float = 0.0\n",
    "    compile = True\n",
    "    def __post_init__(self):\n",
    "        self.padded_vocab_size = self.vocab_size + 64 - (self.vocab_size % 64) if self.padded_vocab_size is None else self.vocab_size\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str):\n",
    "        return cls(**llama_configs[name])\n",
    "\n",
    "\n",
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embed=4096),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embed=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embed=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embed=8192),\n",
    "    \"baby_llama\": dict(n_layer=2, n_head=8, n_embed=128)\n",
    "}\n",
    "## specific parameter settings\n",
    "# For shakespeare, choose smaller block size than vanilla LLaMA\n",
    "out_dir = os.path.join(work_dir, 'results')\n",
    "backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "block_size = 1024\n",
    "batch_size = 12\n",
    "# max_iters = 600000\n",
    "max_iters = 1000\n",
    "log_interval = 1\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "iter_num = 0\n",
    "best_val_loss = 1e-9\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# optimizer\n",
    "weight_decay = 1e-1\n",
    "learning_rate = 6e-4\n",
    "decay_lr = True\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = 6e-5\n",
    "\n",
    "model_config = LLaMAConfig.from_name(\"baby_llama\")\n",
    "model_config.block_size = block_size\n",
    "model_config.vocab_size = 100\n",
    "print(f'model config: \\n\\t{model_config}')\n",
    "# clip gradients at this value, or disable if == 0.0\n",
    "grad_clip = 1.0\n",
    "\n",
    "# evaluate\n",
    "eval_interval = 2000\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "\n",
    "# 3. load model\n",
    "model = LLaMA(model_config)\n",
    "print(f'model arc: \\n\\t{model}')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False)\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "print('config_keys: \\n\\t', config_keys)\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "print('config: \\n\\t', config)\n",
    "\n",
    "# 4. ddp training\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    # init process group, backend: nccl or gloo or mpi\n",
    "    init_process_group(backend=backend)\n",
    "    # rank is the GPU index, 1 GPU will be 0, 2 GPUs will be 0, 1 on global\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    # local rank is the GPU index on one node, 1 GPU will be 0, 2 GPUs will be 0, 1 on one node\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    # world size is the total number of GPUs * nodes\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device=device)\n",
    "    # # this process will do logging, checkpointing etc.\n",
    "    master_process = ddp_rank == 0\n",
    "    # each process gets a different seed\n",
    "    seed_offset = ddp_rank\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f'gradient_accumulation_steps: {gradient_accumulation_steps}')\n",
    "print(f'ddp_world_size: {ddp_world_size}')\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'block_size: {block_size}')\n",
    "print(f'tokens per iter: {tokens_per_iter}')\n",
    "\n",
    "# ddp: 0; non-ddp: True\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(42 + seed_offset)\n",
    "# allow tf32 on matmul\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# allow tf32 on cudnn\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "# for later use in torch.autocast\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "# automatic mixed precison,amp, ofter blend with gradscaler\n",
    "# when we train the model with mixed precison, we may need a gradscaler to shrink gradients to avoid Gradient Underflow\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# 5. load vocab\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "print(f'meta data: {meta_path}')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f'found vocab_size = {meta_vocab_size} (inside {meta_path})')\n",
    "\n",
    "# 6. ddp model settings\n",
    "model.to(device)\n",
    "# unwrap DDP container if needed\n",
    "raw_model = model.module if ddp else model\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizer(\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    betas=(beta1, beta2),\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = None\n",
    "if compile:\n",
    "    print('Compiling the model>>>')\n",
    "    unoptimizer_model = model\n",
    "    # require torch >= 2.0\n",
    "    model = torch.compile(model)\n",
    "    model.to(device)\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "#### 7.prepare batch\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # sample from range(len(data) - block_size) and the shape is (batch_size,)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    # shift right as label\n",
    "    y = torch.stack([torch.from_numpy((data[i+1 : i + block_size + 1]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device=device, non_blocking = True), y.pin_memory().to(device=device, non_blocking = True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "print(f'X shape: {X.shape}, X: {X[0,:10]}')\n",
    "print(f'Y shape: {Y.shape}, Y: {Y[0,:10]}')\n",
    "\n",
    "#### 8. training record\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "\n",
    "## 9.learning rate settings\n",
    "def get_lr(iter):\n",
    "    # 1. linear warmup for warmup_iters steps\n",
    "    if iter < warmup_iters:\n",
    "        return learning_rate * (iter / warmup_iters)\n",
    "    # 2. set the minimum lr when the iter more than decay iters\n",
    "    if iter > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3. in between, use cosine decay\n",
    "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    # coeff ranges from 0-1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "#### 4.4 training loop\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # mixed precison need to be closed\n",
    "            with ctx:\n",
    "                 logits, loss = model(X, Y)\n",
    "            # save the loss during each iter\n",
    "            losses[k] = loss.item()\n",
    "        # get the mean loss across one eval iteration\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    # you may notice that this is the pre-checking step rather than training itself\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # if loss lower than best loss we set, we will update the loss to current loss\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_config,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,                    \n",
    "                }\n",
    "\n",
    "                print(f'saving checkpoint to {out_dir}')\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable.\n",
    "            \n",
    "            # only sync gradients at last\n",
    "            # it means that if in ddp, we only need to forward pass when last step\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1 )\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        # if we choose to clip gradients, we need to unscale the gradients first to clip the right gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        # grad clip is the max gradients during training to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "\n",
    "    # if not in ddp, we use this logging\n",
    "    if iter_num  % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            # 0.9 and 0.1 can dynamticly adjust to monitor training flop\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1    \n",
    "    # this notebool is just for presentation, so the max_iters will set to 10000, in raw project, it will be set to 600000.\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "    \n",
    "# ddp needs init and destory\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model raw state dict: \n",
      "\t('_orig_mod.lm_head.weight', tensor([[-0.0165, -0.1323,  0.1174,  ..., -0.1137, -0.0753,  0.0217],\n",
      "        [-0.1176, -0.0574,  0.0647,  ..., -0.1205, -0.0097,  0.1088],\n",
      "        [-0.0411, -0.0398,  0.0155,  ...,  0.0061, -0.0903, -0.0474],\n",
      "        ...,\n",
      "        [-0.0172,  0.0158, -0.0632,  ..., -0.0277,  0.0944, -0.1004],\n",
      "        [-0.0146, -0.0777, -0.0558,  ...,  0.0378,  0.0180, -0.0529],\n",
      "        [ 0.0328, -0.0672, -0.0782,  ...,  0.0088,  0.1077, -0.0218]],\n",
      "       device='cuda:0'))\n",
      "get dataset meta alphabet information: \n",
      "\t{'vocab_size': 65, 'itos': {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}, 'stoi': {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}}\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ', 'I']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ', 'I', 'C']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ', 'I', 'C', 'H']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ', 'I', 'C', 'H', 'e']\n",
      "-------------------------\n",
      "['I', ' ', 'l', 'o', 'v', 'e', ',', '\\n', 'O', ':', ' ', 'I', 'C', 'H', 'e', 'n']\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we will not start from stratch like nanoGPT instead of inference with our trained model above.\n",
    "# But one thing you need to konw is when we save the model.state_dict\n",
    "# the module name would have a prefix '_orig_mod_'\n",
    "show_state_dict = model.state_dict().items()\n",
    "# just like: \n",
    "print(f'model raw state dict: \\n\\t{list(show_state_dict)[0]}')\n",
    "# so when you load a model you trained before, you may remove the prefix and the detail is in nanoGPT repertory -> sample.py\n",
    "\n",
    "#### 1. load encode and decode\n",
    "# This step means we need to convert word into ids, such as: 'I love you' -> '<SOS> 5 2 0 <EOS>'\n",
    "# there are two ways to complement that:\n",
    "# 1. load meta\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb+') as f:\n",
    "        meta = pickle.load(f)\n",
    "        print(f'get dataset meta alphabet information: \\n\\t{meta}')\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: [itos[i] for i in l]\n",
    "else:\n",
    "    # 2. load from tiktoken\n",
    "    # ok let's assume LLaMA-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming LLaMA encodings...\")\n",
    "    enc = LlamaTokenizer.from_pretrained(\"facebook/llama\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "#### 2. inference\n",
    "# assuming that we start at the very beginning\n",
    "start = 'I love'\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None,...])\n",
    "\n",
    "# inference config\n",
    "# assuming that we need to generate 2 responses for a sample\n",
    "sample_num = 10\n",
    "# max tokens generate for a sample\n",
    "max_new_tokens = 1\n",
    "# temperature 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "temperature = 0.8\n",
    "# top_k retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "top_k = 10\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(sample_num):\n",
    "            # NOTE you may notice that the nanoGPT `generate` method is in the GPT model\n",
    "            # but I will put it as a independent method for better understanding\n",
    "            \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "            \"\"\"\n",
    "            for _ in range(max_new_tokens):\n",
    "                # if the sequence context is growing too long we must crop it at block_size\n",
    "                idx_cond = x if x.size(1) <= model_config.block_size else x[:, -model_config.block_size:]\n",
    "                # forward the model to get the logits for the index in the sequence\n",
    "                logits, _ = model(idx_cond)\n",
    "                # pluck the logits at the final step and scale by desired temperature\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # optionally crop the logits to only the top k options\n",
    "                if top_k is not None:\n",
    "                    # torch.topk returns two variables: v means value and _ means indices\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    # v[:, [-1]] means select values in logits that all lower than v last dimension\n",
    "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                # apply softmax to convert logits to (normalized) probabilities\n",
    "                probs = F.softmax(logits, dim = -1)\n",
    "                # sample from the distribution, num_samples means sample a idx once and NOT the max prob idx must be sampled\n",
    "                # to make sure diversity!\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                # append sampled index to the running sequence and continue\n",
    "                x = torch.cat((x, idx_next), dim = -1)\n",
    "\n",
    "            #### 3.output\n",
    "            print(decode(x[0].tolist()))\n",
    "            print('-------------------------')         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
