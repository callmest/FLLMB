{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA\n",
    "**Welcome** to the llama notebook. And I must say that this charpter will have several challenging points you need to pay attention to.\n",
    "1. RMS-Norm\n",
    "2. SwiGLU\n",
    "3. Rope\n",
    "4. KVcache\n",
    "5. Grouped-Query-Attention\n",
    "\n",
    "But feel free to get over with them. Cause Studying itself is just fun. So, enjoy it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import pacakge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd23/miniconda3/envs/rbp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import math\n",
    "import inspect\n",
    "import tiktoken\n",
    "\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLaMA model parameters\n",
    "Not all of parameters are setting in the first place, some of them are added during the model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLaMAconfig:\n",
    "    # one-head dimension\n",
    "    n_embedding: int = 128\n",
    "    block_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LLaMA model\n",
    "we will implement the LLaMA model step by step. But you should know that in this part, I will NOT blend one module with another, which means I will not infer to KV cache when debug attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 RMS-Norm (Root Mean Square Layer Normalization)\n",
    "Question: What is the difference between **pre-norm** and **post-norm**? (It will be answered in the end of this part.)\n",
    "\n",
    "Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. \n",
    "\n",
    "BSD 3-Clause License: https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "\n",
    "see also: https://blog.csdn.net/yjw123456/article/details/138139970\n",
    "\n",
    "<img src=\"./image/RMSNorm.png\" alt=\"RMSNorm and LayerNorm\" style=\"width: 350px; height: 200px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        # this is the W\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        \n",
    "        # calculate the root of 1/H * sigma{(x_i)**2}\n",
    "        norm_x = torch.mean(x * x, dim = self.dim, keepdim = True)\n",
    "        # calculate rms_x, eps is applied to avoid devided 0.\n",
    "        rms_x = torch.rsqrt(norm_x + self.eps)\n",
    "        # calculate the normed_x\n",
    "        normed_x = x * rms_x\n",
    "        # attach the learning params\n",
    "        scaled_x = normed_x * self.scale\n",
    "        return scaled_x\n",
    "\n",
    "# note: rmsnorm is designed to reduce computation caused by mean in layernorm, that can improve computational effiency and precision simultaneously.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 SwiGLU\n",
    "\n",
    "SwiGLU means a learning gated object function, element wise.\n",
    "\n",
    "<img src=\"./image/SwiGLU.png\" alt=\"SwiGLU\" style=\"width: 450px; height: 260px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.silu(x): x * sigmoid(x)\n",
    "\n",
    "def mlp_silu(x):\n",
    "    # here, the embedding dim will not change for showing and we will discuss this latter.\n",
    "    fc1 = nn.Linear(x.size(-1), x.size(-1))\n",
    "    fc2 = nn.Linear(x.size(-1), x.size(-1))\n",
    "    # proj fc2 with SiLU\n",
    "    x = fc1(x)\n",
    "    gated_x = F.silu(fc2(x))\n",
    "    # element wise multiply matrix\n",
    "    output = x * gated_x\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 ROPE\n",
    "One about rope you need to pay attention is that rope encodes the **absolute position** with a rotation matrix and meanwhile incorporates the explicit **relative positive** dependency in **self-attention** formulation. --rope paper\n",
    "\n",
    "We can try to understand sentence above with a logical line: absolute position -> self-attention -> relative position information\n",
    "\n",
    "⭐there are **two** aspects:\n",
    "- ✅ Definition of RoPE.\n",
    "- ✅ Extention of RoPE to a long context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Definition of RoPE\n",
    "reference 1: https://blog.csdn.net/weixin_43646592/article/details/130924280\n",
    "\n",
    "reference 2: https://oi-wiki.org/math/complex/\n",
    "\n",
    "⭐⭐⭐ (recommended) reference 3: https://blog.csdn.net/v_JULY_v/article/details/134085503 \n",
    "\n",
    "NOTE: I had beed finished the math explanation of the RoPE, you can find it in the `Addition` part of my github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta shape: \n",
      " torch.Size([64])\n",
      "theta: \n",
      " tensor([1.0000, 0.8660, 0.7499, 0.6494, 0.5623])\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-compute rope -> to preduce the cos and sin matrix\n",
    "batch_size = 32\n",
    "seq_len = 1024\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "# base is used to calculate \\theta\n",
    "base = 10000\n",
    "# \\theta is the rotary angle\n",
    "theta = 1.0 / (base ** (torch.arange(0, n_embed, 2) / n_embed))\n",
    "print('theta shape: \\n', theta.shape)\n",
    "print('theta: \\n', theta[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_idx shape: \n",
      " torch.Size([1024])\n",
      "id_theta shape: \n",
      " torch.Size([1024, 64])\n",
      "id_theta: \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.8660, 0.7499, 0.6494, 0.5623]])\n"
     ]
    }
   ],
   "source": [
    "seq_idx = torch.arange(seq_len)\n",
    "print('seq_idx shape: \\n', seq_idx.shape)\n",
    "# outer is the dot computation, idx_theta is the m\\theta is the math equation.\n",
    "idx_theta = torch.outer(seq_idx, theta).float()\n",
    "print('id_theta shape: \\n', idx_theta.shape)\n",
    "print('id_theta: \\n', idx_theta[:2, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache shape: \n",
      " torch.Size([1024, 64, 2])\n",
      "cos: \n",
      " tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.6479,  0.7318,  0.7965,  0.8460],\n",
      "        [-0.4161, -0.1604,  0.0709,  0.2687,  0.4315],\n",
      "        [-0.9900, -0.8558, -0.6279, -0.3685, -0.1160],\n",
      "        [-0.6536, -0.9485, -0.9899, -0.8556, -0.6277]])\n",
      "sin: \n",
      " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.7617,  0.6816,  0.6047,  0.5332],\n",
      "        [ 0.9093,  0.9870,  0.9975,  0.9632,  0.9021],\n",
      "        [ 0.1411,  0.5173,  0.7783,  0.9296,  0.9933],\n",
      "        [-0.7568, -0.3167,  0.1415,  0.5176,  0.7785]])\n",
      "cache: \n",
      " tensor([[[ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000],\n",
      "         [ 1.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5403,  0.8415],\n",
      "         [ 0.6479,  0.7617],\n",
      "         [ 0.7318,  0.6816],\n",
      "         [ 0.7965,  0.6047],\n",
      "         [ 0.8460,  0.5332]],\n",
      "\n",
      "        [[-0.4161,  0.9093],\n",
      "         [-0.1604,  0.9870],\n",
      "         [ 0.0709,  0.9975],\n",
      "         [ 0.2687,  0.9632],\n",
      "         [ 0.4315,  0.9021]],\n",
      "\n",
      "        [[-0.9900,  0.1411],\n",
      "         [-0.8558,  0.5173],\n",
      "         [-0.6279,  0.7783],\n",
      "         [-0.3685,  0.9296],\n",
      "         [-0.1160,  0.9933]],\n",
      "\n",
      "        [[-0.6536, -0.7568],\n",
      "         [-0.9485, -0.3167],\n",
      "         [-0.9899,  0.1415],\n",
      "         [-0.8556,  0.5176],\n",
      "         [-0.6277,  0.7785]]])\n"
     ]
    }
   ],
   "source": [
    "cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim= -1)\n",
    "print('cache shape: \\n', cache.shape)\n",
    "# we need to explain this transformation\n",
    "cos = torch.cos(idx_theta)\n",
    "print('cos: \\n', cos[:5, :5])\n",
    "sin = torch.sin(idx_theta)\n",
    "print('sin: \\n', sin[:5, :5])\n",
    "print('cache: \\n', cache[:5, :5, :])\n",
    "# In the cache last dimension, the first column is cos(m\\theta), the second column is sin(m\\theta). m is the position and theta is the angle with dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([32, 1024, 8, 128])\n",
      "x_ shape: \n",
      " torch.Size([32, 1024, 8, 64, 2])\n",
      "rope_cache shape: \n",
      " torch.Size([1, 1024, 1, 64, 2])\n",
      "x_0 shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "rope_cache_0 shape: \n",
      " torch.Size([1, 1024, 1, 64])\n",
      "x_even shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "x_odd shape: \n",
      " torch.Size([32, 1024, 8, 64])\n",
      "out_put shape: \n",
      " torch.Size([32, 1024, 8, 64, 2])\n",
      "out_put shape: \n",
      " torch.Size([32, 1024, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# 2. apply rope -> there if an input q, and we transit it to right form and then multiply it with cache above.\n",
    "## we can construct an input when debug\n",
    "## an input may look like: ( batch_size, seq_len, num_head, dim)\n",
    "x = torch.randn((batch_size, seq_len, n_head, n_embed))\n",
    "print('x shape: \\n', x.shape)\n",
    "## 1. trucate to avoid length out of range\n",
    "seq_len = x.size(1)\n",
    "cache = cache[:seq_len]\n",
    "## 2. reshape the x -> let the last two dim -> (-1, 2) -> (128) -> (64, 2)\n",
    "x_ = x.reshape(*x.shape[:-1], -1, 2)\n",
    "print('x_ shape: \\n', x_.shape)\n",
    "rope_cache = cache.view(1, x_.size(1), 1, x_.size(3), 2)\n",
    "print('rope_cache shape: \\n', rope_cache.shape)\n",
    "\n",
    "## then we compute the rope output according to the formulation, ... means all dimension except the pointed dimension.\n",
    "x_0 = x_[..., 0]\n",
    "x_1 = x_[..., 1]\n",
    "print('x_0 shape: \\n', x_0.shape)\n",
    "rope_cache_0 = rope_cache[..., 0]\n",
    "rope_cache_1 = rope_cache[..., 1]\n",
    "print('rope_cache_0 shape: \\n', rope_cache_0.shape)\n",
    "\n",
    "## * is element-wise matrix-multiplying\n",
    "# In even dimension: 0,2,...\n",
    "x_even = x_0 * rope_cache_0 - x_1 * rope_cache_1\n",
    "print('x_even shape: \\n', x_even.shape)\n",
    "# In odds dimension: 1, 3,...\n",
    "x_odd = x_1 * rope_cache_0 + x_0 * rope_cache_1\n",
    "print('x_odd shape: \\n', x_odd.shape)\n",
    "out_put = torch.stack([x_even, x_odd], dim = -1)\n",
    "print('out_put shape: \\n', out_put.shape)\n",
    "# reshape from the third dimension\n",
    "out_put = out_put.flatten(3)\n",
    "print('out_put shape: \\n', out_put.shape)\n",
    "\n",
    "# output shape is indentity with inputshape\n",
    "# you may also see RoPE notebook to see more information.\n",
    "\n",
    "# we can conclude code above\n",
    "def bulid_rope_cache(\n",
    "        seq_len:int,\n",
    "        n_embed: int,\n",
    "        dtype: torch.dtype,\n",
    "        device: torch.device,\n",
    "        base: int = 10000\n",
    "):\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_embed, 2, dtype=dtype, device=device) / n_embed))\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half() \n",
    "    return cache\n",
    "         \n",
    "def apply_rope(x: torch.tensor, rope_cache):\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "\n",
    "    x_out = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out = x_out.flatten(3)\n",
    "    \n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 KVcache ⭐⭐⭐\n",
    "KV cache is used to reduce computational resources during **inference**. In general, we need to calculate self-attention every time when we pass the next token to raw sentences, then calculate next next token.\n",
    "\n",
    "The idea is that we can store all the previous k v in self-attention calculating so that we can reduce repeat computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([1, 10])\n",
      "embed_x shape: \n",
      " torch.Size([1, 10, 128])\n",
      "Q shape: \n",
      " torch.Size([1, 10, 128])\n",
      "K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "score shape: \n",
      " torch.Size([1, 10, 10])\n",
      "attn shape: \n",
      " torch.Size([1, 10, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 10, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 10, 100])\n",
      "output: \n",
      " tensor(56)\n",
      "new_x shape: \n",
      " torch.Size([1, 11])\n",
      "Q shape: \n",
      " torch.Size([1, 11, 128])\n",
      "K shape: \n",
      " torch.Size([1, 11, 128])\n",
      "V shape: \n",
      " torch.Size([1, 11, 128])\n",
      "score shape: \n",
      " torch.Size([1, 11, 11])\n",
      "attn shape: \n",
      " torch.Size([1, 11, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 11, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 11, 100])\n",
      "output: \n",
      " tensor(26)\n"
     ]
    }
   ],
   "source": [
    "# First, we construct an input, shape is: (1, input_len), assumpt that embedding dim is 128, vocab_size is 100\n",
    "input_len = 10\n",
    "vocab_size = 100\n",
    "embedding_size = 128\n",
    "x = torch.randint(0, 100, (1, input_len))\n",
    "print('x shape: \\n', x.shape)\n",
    "# **WITHOUT KVCAHCE**\n",
    "# 1. first, we need to see the raw attention **WITHOUT KVCAHCE**\n",
    "# When inference, we passed the inputs into the model.\n",
    "## 1.1 pos embedding, here we just nn.embedding\n",
    "### 1. first iteration\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "embed_x = embedding(x)\n",
    "print('embed_x shape: \\n', embed_x.shape)\n",
    "wq = nn.Linear(embedding_size, embedding_size)\n",
    "wk = nn.Linear(embedding_size, embedding_size)\n",
    "wv = nn.Linear(embedding_size, embedding_size)\n",
    "Q = wq(embed_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "# we discard all the other procudures\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "### 2. second iteration/generation\n",
    "new_x = torch.cat((x, output.unsqueeze(0).view(1, -1)), dim=-1)\n",
    "print('new_x shape: \\n', new_x.shape)\n",
    "# then we repeat operations above\n",
    "embed_new_x = embedding(new_x)\n",
    "Q = wq(embed_new_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_new_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_new_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in attention calculation, iteration 1 and 2 both calculate the whole attention, and we can see in iteration 1 and 2, the wq and wk are all the same！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: \n",
      " torch.Size([1, 10])\n",
      "embed_x shape: \n",
      " torch.Size([1, 10, 128])\n",
      "Q shape: \n",
      " torch.Size([1, 10, 128])\n",
      "K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "cache_K shape: \n",
      " torch.Size([1, 10, 128])\n",
      "cache_V shape: \n",
      " torch.Size([1, 10, 128])\n",
      "score shape: \n",
      " torch.Size([1, 10, 10])\n",
      "attn shape: \n",
      " torch.Size([1, 10, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 10, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 10, 100])\n",
      "output: \n",
      " tensor(12)\n",
      "new_x shape: \n",
      " torch.Size([1, 1])\n",
      "Q shape: \n",
      " torch.Size([1, 1, 128])\n",
      "K shape: \n",
      " torch.Size([1, 1, 128])\n",
      "V shape: \n",
      " torch.Size([1, 1, 128])\n",
      "cached_K shape: \n",
      " torch.Size([1, 11, 128])\n",
      "cached_V shape: \n",
      " torch.Size([1, 11, 128])\n",
      "score shape: \n",
      " torch.Size([1, 1, 11])\n",
      "attn shape: \n",
      " torch.Size([1, 1, 128])\n",
      "lm_x shape: \n",
      " torch.Size([1, 1, 100])\n",
      "lm_x_softm shape: \n",
      " torch.Size([1, 1, 100])\n",
      "output: \n",
      " tensor(91)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **WITH KVCAHCE**\n",
    "## In KVcache, we can just store the first calculated k and v, and then concat new k an v to generate a complete whole length k and v\n",
    "input_len = 10\n",
    "vocab_size = 100\n",
    "embedding_size = 128\n",
    "x = torch.randint(0, 100, (1, input_len))\n",
    "print('x shape: \\n', x.shape)\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "embed_x = embedding(x)\n",
    "print('embed_x shape: \\n', embed_x.shape)\n",
    "wq = nn.Linear(embedding_size, embedding_size)\n",
    "wk = nn.Linear(embedding_size, embedding_size)\n",
    "wv = nn.Linear(embedding_size, embedding_size)\n",
    "Q = wq(embed_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "####################KVCACHE##########################\n",
    "cache_K = K\n",
    "print('cache_K shape: \\n', cache_K.shape)\n",
    "cache_V = V\n",
    "print('cache_V shape: \\n', cache_V.shape)\n",
    "####################KVCACHE##########################\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "# we discard all the other procudures\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "### 2. second iteration/generation\n",
    "#############x is different######################\n",
    "new_x = output.unsqueeze(0).view(1, -1)\n",
    "print('new_x shape: \\n', new_x.shape)\n",
    "# then we repeat operations above\n",
    "## and notice the QKV shape\n",
    "embed_new_x = embedding(new_x)\n",
    "Q = wq(embed_new_x)\n",
    "print('Q shape: \\n', Q.shape)\n",
    "K = wk(embed_new_x)\n",
    "print('K shape: \\n', K.shape)\n",
    "V = wv(embed_new_x)\n",
    "print('V shape: \\n', V.shape)\n",
    "###################concat cache####################\n",
    "K = torch.concat((cache_K, K),dim=1)\n",
    "print('cached_K shape: \\n', K.shape)\n",
    "V = torch.concat((cache_V, V),dim=1)\n",
    "print('cached_V shape: \\n', V.shape)\n",
    "cache_K = K\n",
    "cache_V = V\n",
    "###################concat cache####################\n",
    "score = Q @ K.transpose(2, 1)\n",
    "print('score shape: \\n', score.shape)\n",
    "attn = score @ V\n",
    "print('attn shape: \\n', attn.shape)\n",
    "lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "lm_x = lm_head(attn)\n",
    "print('lm_x shape: \\n', lm_x.shape)\n",
    "lm_x_softm = nn.Softmax(dim=-1)(lm_x)\n",
    "print('lm_x_softm shape: \\n', lm_x_softm.shape)\n",
    "output = torch.argmax(lm_x_softm[:,-1,:])\n",
    "print('output: \\n', output)\n",
    "\n",
    "# In conclusion, the computation is reduced from Q[L+1,D] @ K[L+1,D] @ V[L+1,D] to  Q[1,D] @ K[L+1,D] @ V[L+1,D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Grouped-Query-Attention\n",
    "The KV cache can significantly reduce inference computational burdens. But the ocupation of GPU was massive. And there are mainly four ways to make kvcache effiency: \n",
    "- cut length; \n",
    "- reduce self-attention head-nums - MQA/GQA; \n",
    "- quantization of kvcache;\n",
    "- paged attention \n",
    "\n",
    "In lit-LLaMA, adopted cut length by rolling; And we will introduce another method GQA.\n",
    "\n",
    "In general, there are several heads during self-attention and we grouped them, such as: 8 heads -> 2 grouped heads, means that there are 4 heads in one group. In one group, the kv just save once and others were copied from this saved kv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/GQA.png\" alt=\"GQA\" style=\"width: 500px; height: 450px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.1 parameters\n",
    "we assumed the embedding dim is 18, and head num is 6, it means there are 3 channels in one head. Hence that:\n",
    "- Q dim 18, 6 heads -> (Q1, Q2, Q3, Q4, Q5, Q6)\n",
    "\n",
    "- K dim 18, 2 grouped, -> (grouped_K1, grouped_K2) \n",
    "-> (grouped_K1_copy1, grouped_K1_copy2, grouped_K1_copy3,\n",
    "grouped_K2_copy1, grouped_K2_copy2,grouped_K2_copy3)\n",
    "- grouped_K1 + grouped_K2, dim: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 18\n",
    "    # attention layers\n",
    "    n_layers: int = 1\n",
    "    # q heads\n",
    "    n_heads: int = 6\n",
    "    # kv grouped heads\n",
    "    n_kv_heads: int =  2\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 10  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    # rms norm eps\n",
    "    norm_eps: float = 1e-5\n",
    "    # theta bese\n",
    "    rope_theta: float = 500000\n",
    "    max_batch_size: int = 2\n",
    "    max_seq_len: int = 17\n",
    "    model_parallel_size = 1\n",
    "\n",
    "config = ModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 GQA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated k shape: \n",
      "torch.Size([1, 7, 6, 3])\n",
      "k: \n",
      "tensor([[-1.2014, -1.9226,  0.9098],\n",
      "        [ 0.3263,  0.1853,  1.1418]])\n",
      "repeated k: \n",
      "tensor([[-1.2014, -1.9226,  0.9098],\n",
      "        [-1.2014, -1.9226,  0.9098],\n",
      "        [-1.2014, -1.9226,  0.9098],\n",
      "        [ 0.3263,  0.1853,  1.1418],\n",
      "        [ 0.3263,  0.1853,  1.1418],\n",
      "        [ 0.3263,  0.1853,  1.1418]])\n"
     ]
    }
   ],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    # x[:, :, :, None, :] means that add one new dim in None dim\n",
    "    # x: [b, s, h, d] -> [b, s, h, 1, d]\n",
    "    # expand operation will expand vector dims to desired shape\n",
    "    # BUT you may notice that it may NOT copy data substanially\n",
    "    # As a fact, it just looks like the vector is copied by torch broadcast\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim) # \n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "# debug\n",
    "k = torch.randn(1, 7, 2, 3)\n",
    "repeat_k = repeat_kv(k, 3)\n",
    "print(f'repeated k shape: \\n{repeat_k.shape}')\n",
    "print(f'k: \\n{k[0, 0, :, :]}')\n",
    "print(f'repeated k: \\n{repeat_k[0, 0, :, :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wq_shape: \n",
      "\ttorch.Size([18, 18])\n",
      "wk_shape: \n",
      "\ttorch.Size([6, 18])\n",
      "wv_shape: \n",
      "\ttorch.Size([6, 18])\n",
      "wo_shape: \n",
      "\ttorch.Size([18, 18])\n",
      "attn shape with GQA: \n",
      "\tAttention(\n",
      "  (wq): Linear(in_features=18, out_features=18, bias=False)\n",
      "  (wk): Linear(in_features=18, out_features=6, bias=False)\n",
      "  (wv): Linear(in_features=18, out_features=6, bias=False)\n",
      "  (wo): Linear(in_features=18, out_features=18, bias=False)\n",
      ")\n",
      "x_src shape: \n",
      "\ttorch.Size([2, 17, 18])\n",
      "q shape: \n",
      "\ttorch.Size([2, 17, 6, 3])\n",
      "keys shape: \n",
      "\ttorch.Size([2, 17, 2, 3])\n",
      "values shape: \n",
      "\ttorch.Size([2, 17, 2, 3])\n",
      "q shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "repeated_keys shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "repeated_values shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "output shape: \n",
      "\ttorch.Size([2, 6, 17, 3])\n",
      "concated shape: \n",
      "\ttorch.Size([2, 17, 18])\n",
      "y shape: \n",
      "\ttorch.Size([2, 17, 18])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # this is the parallel parameter\n",
    "        model_parallel_size = args.model_parallel_size\n",
    "        # local heads means that total heads are distributed into several nodes\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(in_features=args.dim, out_features=args.n_heads * self.head_dim,bias=False)\n",
    "        self.wk = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wo = nn.Linear(in_features=args.n_heads * self.head_dim, out_features=args.dim,bias=False)\n",
    "\n",
    "        print(f'wq_shape: \\n\\t{self.wq.weight.shape}')\n",
    "        print(f'wk_shape: \\n\\t{self.wk.weight.shape}')\n",
    "        print(f'wv_shape: \\n\\t{self.wv.weight.shape}')\n",
    "        print(f'wo_shape: \\n\\t{self.wo.weight.shape}')\n",
    "\n",
    "        # kvcache, since that we grouped the kv, so we just need to store one grouped k and v\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        # notice that the shape of k and v\n",
    "        # we can find that 'Grouped' DOES NOT mean that calculate the whole k and v first then split them into groups\n",
    "        # while calculate one grouped k and v, then expand them into whole length\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        # you may notice the position of applying rope\n",
    "        # calculate qkv -> apply rope -> cache kv -> expand\n",
    "        # xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # ignore RoPE\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        # By default, this is the inference environment\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv \n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        print(f'q shape: \\n\\t{xq.shape}')\n",
    "        print(f'keys shape: \\n\\t{keys.shape}')\n",
    "        print(f'values shape: \\n\\t{values.shape}')\n",
    "\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        print(f'q shape: \\n\\t{xq.shape}')\n",
    "        print(f'repeated_keys shape: \\n\\t{keys.shape}')\n",
    "        print(f'repeated_values shape: \\n\\t{values.shape}')\n",
    "\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values) \n",
    "        print(f'output shape: \\n\\t{output.shape}')\n",
    "        # (b, h, l, h_d) -> (b, l, d)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        print(f'concated shape: \\n\\t{output.shape}')\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "# debug\n",
    "attn = Attention(config)\n",
    "print(f'attn shape with GQA: \\n\\t{attn}')\n",
    "batch_size = config.max_batch_size\n",
    "seq_len = config.max_seq_len\n",
    "embedding_dim = config.dim\n",
    "x_src = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "print(f'x_src shape: \\n\\t{x_src.shape}')\n",
    "y = attn(x_src, start_pos = 0, freqs_cis=None, mask=None)\n",
    "print(f'y shape: \\n\\t{y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 LLAMA Model Construction\n",
    "In this step, we will assemble a llama model using modules just like other chapters before.\n",
    "\n",
    "<img src=\"./image/LLaMA.png\" alt=\"LLaMA\" style=\"width: 600px; height: 400px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of LLaMA implements lit-llama\n",
    "##### 2.5.1 LLAMA Config\n",
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    # seq_len\n",
    "    block_size: int = 2048\n",
    "    # vocab_size\n",
    "    vocab_size: int = 100\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embed: int = 4096\n",
    "    \n",
    "##### 2.5.2 CausalSelfAttention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config:LLaMAConfig):\n",
    "        super().__init__()\n",
    "        # check head\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "\n",
    "        # qkv in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "\n",
    "        # output proj\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "\n",
    "        # settings\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.tensor,\n",
    "                rope: torch.tensor,\n",
    "                mask: torch.tensor,\n",
    "                max_seq_length: int,\n",
    "                input_pos: Optional[torch.tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None,\n",
    "                ):\n",
    "            # (Batch_size, seq_len, embedding_dim)\n",
    "            B, T, C = x.size()\n",
    "            # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "            q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "\n",
    "            head_size = C // self.n_head\n",
    "            k = k.view(B, T, self.n_head, head_size)\n",
    "            q = q.view(B, T, self.n_head, head_size)\n",
    "            v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "            q = apply_rope(q, rope)\n",
    "            k = apply_rope(k, rope)\n",
    "\n",
    "            k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "            q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "            v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "            # kvcache:\n",
    "            if kv_cache is not None:\n",
    "                cache_k, cache_v = kv_cache\n",
    "                if input_pos[-1] >= max_seq_length:\n",
    "                    input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                    # torch.roll: https://pytorch.org/docs/stable/generated/torch.roll.html\n",
    "                    cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                    cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "                # index_copy: https://blog.csdn.net/hjxu2016/article/details/130161239\n",
    "                # means insert k into cache_k with input_pos in dim 2\n",
    "                k = cache_k.index_copy(2, input_pos, k)\n",
    "                v = cache_v.index_copy(2, input_pos, v)\n",
    "                kv_cache = k, v\n",
    "            \n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "            y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            y = self.c_proj(y)\n",
    "            return y, kv_cache\n",
    "    \n",
    "##### 2.5.3 silu mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config:LLaMAConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embed\n",
    "        # 2/3 hidden_dim\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        # let n_hidden is multiple 256\n",
    "        if n_hidden % 256 != 0:\n",
    "            n_hidden = n_hidden + 256 - (n_hidden % 256)\n",
    "        \n",
    "        self.c_fc1 = nn.Linear(config.n_embed, n_hidden, bias = False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embed, n_hidden, bias = False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embed, bias = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "##### 2.5.3 LLaMA block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig):\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embed)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ):\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x),\n",
    "                                    rope,\n",
    "                                    mask,\n",
    "                                    max_seq_length,\n",
    "                                    input_pos,\n",
    "                                    kv_cache)\n",
    "        # short cut 1\n",
    "        x = x + h\n",
    "        # short cut 2\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache\n",
    "    \n",
    "##### 2.5.3 LLaMA model\n",
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig):\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embed),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embed),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            idx: torch.Tensor,\n",
    "            targets: torch.tensor = None,\n",
    "            max_seq_length: Optional[int] = None,\n",
    "            input_pos: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        B, T = idx.size()\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            # index_select: https://pytorch.org/docs/stable/generated/torch.index_select.html\n",
    "            # rope: (T, n_embed/2, 2)\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            # mask: (1, 1, :T, :T)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "        \n",
    "        x = self.transformer.wte(idx)\n",
    "        # during training, we will not use qvcache\n",
    "        if input_pos is None:\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            # kvcaches: [layer1:(k,v), layer2:(k,v), layer3:(k,v)]\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embed // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                # initial kv_cache\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # we only need to calculate the next token\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "         # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused = True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f'using fused adamW: {use_fused}')\n",
    "        return optimizer\n",
    "    \n",
    "    def get_num_params(self, non_embedding = True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wte.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops (Floating Point Operations) utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        # get the attn shape\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embed // cfg.n_head, cfg.block_size\n",
    "        # per token calculated\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        # fwd and pwd need to iter every token\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "    \n",
    "    @classmethod\n",
    "    def from_name(cls, name: str):\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "    \n",
    "    def build_rope_cache(self, idx: torch.tensor):\n",
    "        return bulid_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_embed=self.config.n_embed // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx):\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        # make a tril angle matrix -> (1, 1, block_size, block_size)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: \n",
      "\t(1003854,)\n",
      "train_data shape: \n",
      "\t(111540,)\n",
      "model config: \n",
      "\tLLaMAConfig(block_size=1024, vocab_size=100, padded_vocab_size=128, n_layer=2, n_head=8, n_embed=128, bias=False, dropout=0.0)\n",
      "model arc: \n",
      "\tLLaMA(\n",
      "  (lm_head): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(128, 128)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_fc2): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      ")\n",
      "config_keys: \n",
      "\t ['batch_size', 'seq_len', 'n_embed', 'n_head', 'base', 'input_len', 'vocab_size', 'embedding_size', 'embedding_dim', 'work_dir', 'data_dir', 'out_dir', 'backend', 'gradient_accumulation_steps', 'block_size', 'max_iters', 'log_interval', 'device', 'dtype', 'iter_num', 'best_val_loss', 'always_save_checkpoint', 'weight_decay', 'learning_rate', 'decay_lr', 'beta1', 'beta2', 'warmup_iters', 'lr_decay_iters', 'min_lr', 'grad_clip', 'eval_interval', 'eval_iters', 'eval_only']\n",
      "config: \n",
      "\t {'batch_size': 12, 'seq_len': 17, 'n_embed': 128, 'n_head': 8, 'base': 10000, 'input_len': 10, 'vocab_size': 100, 'embedding_size': 128, 'embedding_dim': 18, 'work_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA', 'data_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA/data/shakespeare_char', 'out_dir': '/public/share/sd23/d2l/AI_study/LLaMA/LLaMA/results', 'backend': 'nccl', 'gradient_accumulation_steps': 40, 'block_size': 1024, 'max_iters': 1000, 'log_interval': 1, 'device': 'cuda', 'dtype': 'bfloat16', 'iter_num': 0, 'best_val_loss': 1e-09, 'always_save_checkpoint': False, 'weight_decay': 0.1, 'learning_rate': 0.0006, 'decay_lr': True, 'beta1': 0.9, 'beta2': 0.95, 'warmup_iters': 2000, 'lr_decay_iters': 1000, 'min_lr': 6e-05, 'grad_clip': 1.0, 'eval_interval': 2000, 'eval_iters': 200, 'eval_only': False}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import pickle\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import time\n",
    "# 1. load data\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, 'data/shakespeare_char')\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "print(f'train_data shape: \\n\\t{train_data.shape}')\n",
    "print(f'train_data shape: \\n\\t{val_data.shape}')\n",
    "# 2.model config\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    # seq_len\n",
    "    block_size: int = 2048\n",
    "    # vocab_size\n",
    "    vocab_size: int = 100\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embed: int = 4096\n",
    "    bias: bool = False\n",
    "    dropout: float = 0.0\n",
    "    compile = True\n",
    "    def __post_init__(self):\n",
    "        self.padded_vocab_size = self.vocab_size + 64 - (self.vocab_size % 64) if self.padded_vocab_size is None else self.vocab_size\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str):\n",
    "        return cls(**llama_configs[name])\n",
    "\n",
    "\n",
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embed=4096),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embed=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embed=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embed=8192),\n",
    "    \"baby_llama\": dict(n_layer=2, n_head=8, n_embed=128)\n",
    "}\n",
    "## specific parameter settings\n",
    "# For shakespeare, choose smaller block size than vanilla LLaMA\n",
    "out_dir = os.path.join(work_dir, 'results')\n",
    "backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "block_size = 1024\n",
    "batch_size = 12\n",
    "# max_iters = 600000\n",
    "max_iters = 1000\n",
    "log_interval = 1\n",
    "device = 'cuda'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "iter_num = 0\n",
    "best_val_loss = 1e-9\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# optimizer\n",
    "weight_decay = 1e-1\n",
    "learning_rate = 6e-4\n",
    "decay_lr = True\n",
    "beta1, beta2 = 0.9, 0.95\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = 6e-5\n",
    "\n",
    "model_config = LLaMAConfig.from_name(\"baby_llama\")\n",
    "model_config.block_size = block_size\n",
    "model_config.vocab_size = 100\n",
    "print(f'model config: \\n\\t{model_config}')\n",
    "# clip gradients at this value, or disable if == 0.0\n",
    "grad_clip = 1.0\n",
    "\n",
    "# evaluate\n",
    "eval_interval = 2000\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "\n",
    "# 3. load model\n",
    "model = LLaMA(model_config)\n",
    "print(f'model arc: \\n\\t{model}')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False)\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "print('config_keys: \\n\\t', config_keys)\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "print('config: \\n\\t', config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. ddp training\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    # init process group, backend: nccl or gloo or mpi\n",
    "    init_process_group(backend=backend)\n",
    "    # rank is the GPU index, 1 GPU will be 0, 2 GPUs will be 0, 1 on global\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    # local rank is the GPU index on one node, 1 GPU will be 0, 2 GPUs will be 0, 1 on one node\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    # world size is the total number of GPUs * nodes\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device=device)\n",
    "    # # this process will do logging, checkpointing etc.\n",
    "    master_process = ddp_rank == 0\n",
    "    # each process gets a different seed\n",
    "    seed_offset = ddp_rank\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f'gradient_accumulation_steps: {gradient_accumulation_steps}')\n",
    "print(f'ddp_world_size: {ddp_world_size}')\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'block_size: {block_size}')\n",
    "print(f'tokens per iter: {tokens_per_iter}')\n",
    "\n",
    "# ddp: 0; non-ddp: True\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(42 + seed_offset)\n",
    "# allow tf32 on matmul\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# allow tf32 on cudnn\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "# for later use in torch.autocast\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "# automatic mixed precison,amp, ofter blend with gradscaler\n",
    "# when we train the model with mixed precison, we may need a gradscaler to shrink gradients to avoid Gradient Underflow\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# 5. load vocab\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "print(f'meta data: {meta_path}')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f'found vocab_size = {meta_vocab_size} (inside {meta_path})')\n",
    "\n",
    "# 6. ddp model settings\n",
    "model.to(device)\n",
    "# unwrap DDP container if needed\n",
    "raw_model = model.module if ddp else model\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizer(\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    betas=(beta1, beta2),\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = None\n",
    "if compile:\n",
    "    print('Compiling the model>>>')\n",
    "    unoptimizer_model = model\n",
    "    # require torch >= 2.0\n",
    "    model = torch.compile(model)\n",
    "    model.to(device)\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "#### 7.prepare batch\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # sample from range(len(data) - block_size) and the shape is (batch_size,)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    # shift right as label\n",
    "    y = torch.stack([torch.from_numpy((data[i+1 : i + block_size + 1]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device=device, non_blocking = True), y.pin_memory().to(device=device, non_blocking = True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "print(f'X shape: {X.shape}, X: {X[0,:10]}')\n",
    "print(f'Y shape: {Y.shape}, Y: {Y[0,:10]}')\n",
    "\n",
    "#### 8. training record\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "\n",
    "## 9.learning rate settings\n",
    "def get_lr(iter):\n",
    "    # 1. linear warmup for warmup_iters steps\n",
    "    if iter < warmup_iters:\n",
    "        return learning_rate * (iter / warmup_iters)\n",
    "    # 2. set the minimum lr when the iter more than decay iters\n",
    "    if iter > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3. in between, use cosine decay\n",
    "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    # coeff ranges from 0-1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "#### 4.4 training loop\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # mixed precison need to be closed\n",
    "            with ctx:\n",
    "                 logits, loss = model(X, Y)\n",
    "            # save the loss during each iter\n",
    "            losses[k] = loss.item()\n",
    "        # get the mean loss across one eval iteration\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    # you may notice that this is the pre-checking step rather than training itself\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # if loss lower than best loss we set, we will update the loss to current loss\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_config,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,                    \n",
    "                }\n",
    "\n",
    "                print(f'saving checkpoint to {out_dir}')\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable.\n",
    "            \n",
    "            # only sync gradients at last\n",
    "            # it means that if in ddp, we only need to forward pass when last step\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1 )\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        # if we choose to clip gradients, we need to unscale the gradients first to clip the right gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        # grad clip is the max gradients during training to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "\n",
    "    # if not in ddp, we use this logging\n",
    "    if iter_num  % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            # 0.9 and 0.1 can dynamticly adjust to monitor training flop\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1    \n",
    "    # this notebool is just for presentation, so the max_iters will set to 10000, in raw project, it will be set to 600000.\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "    \n",
    "# ddp needs init and destory\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we will not start from stratch like nanoGPT instead of inference with our trained model above.\n",
    "# But one thing you need to konw is when we save the model.state_dict\n",
    "# the module name would have a prefix '_orig_mod_'\n",
    "show_state_dict = model.state_dict().items()\n",
    "# just like: \n",
    "print(f'model raw state dict: \\n\\t{list(show_state_dict)[0]}')\n",
    "# so when you load a model you trained before, you may remove the prefix and the detail is in nanoGPT repertory -> sample.py\n",
    "\n",
    "#### 1. load encode and decode\n",
    "# This step means we need to convert word into ids, such as: 'I love you' -> '<SOS> 5 2 0 <EOS>'\n",
    "# there are two ways to complement that:\n",
    "# 1. load meta\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb+') as f:\n",
    "        meta = pickle.load(f)\n",
    "        print(f'get dataset meta alphabet information: \\n\\t{meta}')\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: [itos[i] for i in l]\n",
    "else:\n",
    "    # 2. load from tiktoken\n",
    "    # ok let's assume LLaMA-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming LLaMA encodings...\")\n",
    "    enc = LlamaTokenizer.from_pretrained(\"facebook/llama\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "#### 2. inference\n",
    "# assuming that we start at the very beginning\n",
    "start = 'I love'\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None,...])\n",
    "\n",
    "# inference config\n",
    "# assuming that we need to generate 2 responses for a sample\n",
    "sample_num = 10\n",
    "# max tokens generate for a sample\n",
    "max_new_tokens = 1\n",
    "# temperature 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "temperature = 0.8\n",
    "# top_k retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "top_k = 10\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(sample_num):\n",
    "            # NOTE you may notice that the nanoGPT `generate` method is in the GPT model\n",
    "            # but I will put it as a independent method for better understanding\n",
    "            \"\"\"\n",
    "            Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "            the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "            Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "            \"\"\"\n",
    "            for _ in range(max_new_tokens):\n",
    "                # if the sequence context is growing too long we must crop it at block_size\n",
    "                idx_cond = x if x.size(1) <= model_config.block_size else x[:, -model_config.block_size:]\n",
    "                # forward the model to get the logits for the index in the sequence\n",
    "                logits, _ = model(idx_cond)\n",
    "                # pluck the logits at the final step and scale by desired temperature\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # optionally crop the logits to only the top k options\n",
    "                if top_k is not None:\n",
    "                    # torch.topk returns two variables: v means value and _ means indices\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    # v[:, [-1]] means select values in logits that all lower than v last dimension\n",
    "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                # apply softmax to convert logits to (normalized) probabilities\n",
    "                probs = F.softmax(logits, dim = -1)\n",
    "                # sample from the distribution, num_samples means sample a idx once and NOT the max prob idx must be sampled\n",
    "                # to make sure diversity!\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                # append sampled index to the running sequence and continue\n",
    "                x = torch.cat((x, idx_next), dim = -1)\n",
    "\n",
    "            #### 3.output\n",
    "            print(decode(x[0].tolist()))\n",
    "            print('-------------------------')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA\n",
    "LoRA: Low-Rank Adaptation. PEFT\n",
    "\n",
    "             ┌───────────────────┐\n",
    "             ┆         h         ┆\n",
    "             └───────────────────┘\n",
    "                       ▲\n",
    "                       |\n",
    "                       +\n",
    "                    /     \\\n",
    "    ┌─────────────────┐    ╭───────────────╮     Matrix initialization:\n",
    "    ┆                 ┆     \\      B      /      B = 0\n",
    "    ┆   pretrained    ┆      \\    r*d    /       A = N(0, sigma^2)\n",
    "    ┆    weights      ┆       ╰─────────╯\n",
    "    ┆                 ┆       |    r    |        r - rank\n",
    "    ┆   W e R^(d*d)   ┆       | ◀─────▶ |\n",
    "    ┆                 ┆       ╭─────────╮\n",
    "    └─────────────────┘      /     A     \\\n",
    "              ▲             /     d*r     \\\n",
    "               \\           ╰───────────────╯\n",
    "                \\                ▲\n",
    "                 \\              /\n",
    "                  \\            /\n",
    "             ┌───────────────────┐\n",
    "             ┆         x         ┆\n",
    "             └───────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfig:\n",
    "    r: float = 0.0\n",
    "    alpha: float = 1.0\n",
    "    dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(self,\n",
    "                 r: int,\n",
    "                 lora_alpha: int,\n",
    "                 lora_dropout: float,\n",
    "                 merge_weights: bool,):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "        else:\n",
    "            # if lora_dropout is 0, we will use a lambda function to return the input\n",
    "            self.lora_dropout = lambda x: x\n",
    "\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedLinear(nn.Linear, LoRALayer):\n",
    "    def __init__(\n",
    "            self,\n",
    "\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "\n",
    "            r: int = 0,\n",
    "            lora_alpha: int = 1,\n",
    "            lora_dropout: float = 0.0,\n",
    "            # enable_lora is a list of bools, which means that we can decide which feature will be used in LoRA\n",
    "            enable_lora: List[bool] = [False],\n",
    "            fan_in_fan_out: bool = False,\n",
    "            merge_weights: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features=in_features, out_features=out_features, **kwargs)\n",
    "        LoRALayer.__init__(self,r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n",
    "        assert out_features % len(enable_lora) == 0, \\\n",
    "            f\"out_features must be divisible by {len(enable_lora)} for LoRA to work\"\n",
    "        \n",
    "        if r > 0 and any(enable_lora):\n",
    "            # A matrix\n",
    "            self.lora_A = nn.Parameter(\n",
    "                self.weight.new_zeros((r, in_features))\n",
    "            )\n",
    "            # B matrix\n",
    "            self.lora_B = nn.Parameter(\n",
    "                self.weight.new_zeros((out_features, r))\n",
    "            )\n",
    "            # scaling factor\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            self.weight.requires_grad = False\n",
    "            # dicided which feature will be used in LoRA\n",
    "            # example:\n",
    "            # out_features = 6, enable_lora = [True, False]\n",
    "            # self.lora_ind = [False, False, False, False, False, False]\n",
    "            # self.lora_ind = [[False, False, False], [False, False, False]]\n",
    "            # self.lora_ind = [[True, True, True], [False, False, False]]\n",
    "            # self.lora_ind = [True, True, True, False, False, False]\n",
    "\n",
    "            self.lora_ind = self.weight.new_zeros(\n",
    "                (out_features,), dtype=torch.bool\n",
    "            ).view(len(enable_lora), -1)\n",
    "            self.lora_ind[enable_lora,:] = True\n",
    "            self.lora_ind = self.lora_ind.view(-1)\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.T\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            nn.Linear.reset_parameters(self=self)\n",
    "            if hasattr(self, 'lora_A'):\n",
    "                nn.init.kaiming_normal_(self.lora_A, a=math.sqrt(5))\n",
    "                nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        def train(\n",
    "                self,\n",
    "                mode: bool = True\n",
    "        ):\n",
    "            def T(w):\n",
    "                return w.T if self.fan_in_fan_out else w\n",
    "            nn.Linear.train(self, mode)\n",
    "            should = self.merged if mode else not self.merged\n",
    "            if self.merge_weights and should:\n",
    "                if self.r > 0 and any(self.enable_lora):\n",
    "                    delta_w = F.conv1d(\n",
    "                        self.lora_A.data.unsqueeze(0),\n",
    "                        self.lora_B.data.unsqueeze(-1),\n",
    "                    ).squeeze(0)\n",
    "                    sign = -1 if mode else 1\n",
    "                    self.weight.data += sign * self.zero_pad(T(delta_w * self.scaling))\n",
    "                self.merged = not mode\n",
    "\n",
    "        def forward(self, x):\n",
    "            def T(w):\n",
    "                return w.T if self.fan_in_fan_out else w\n",
    "            if self.merged:\n",
    "                return F.linear(x, T(self.weight), bias=self.bias)  \n",
    "            else:\n",
    "                result = F.linear(x, T(self.weight), bias=self.bias)  \n",
    "                if self.r > 0:\n",
    "                    # adjustment = (x ⋅ A) ⋅ B\n",
    "                    # final output=output + adjustment⋅scaling\n",
    "                    after_A = F.linear(self.lora_dropout(x), self.lora_A)  \n",
    "                    # after A: () -> (r, in_features)\n",
    "                    # （batch size, channels, length）\n",
    "                    after_B = F.conv1d(\n",
    "                        after_A.transpose(-2, -1), \n",
    "                        self.lora_B.unsqueeze(-1),  \n",
    "                    ).transpose(-2, -1) \n",
    "                    result += after_B * self.scaling  \n",
    "                return result          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the LoRA model\n",
    "\n",
    "class LoRACausalSelfAttention(CausalSelfAttention):\n",
    "    lora_config = None\n",
    "\n",
    "    def __init__(self, config: LLaMAConfig):\n",
    "        nn.Module.__init__(self)\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = MergedLinear(\n",
    "            in_features = config.n_embd,\n",
    "            out_features = 3 * config.n_embd,\n",
    "            r = self.lora_config.r,\n",
    "            lora_alpha = self.lora_config.alpha,\n",
    "            lora_dropout = self.lora_config.dropout,\n",
    "            enable_lora = [True, False, False],\n",
    "            fan_in_fan_out = False,\n",
    "            merge_weights = True,\n",
    "            bias = False\n",
    "        )\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        self.rope_cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lit_llama.model as llama\n",
    "@contextmanager\n",
    "def lora(\n",
    "    r, alpha, dropout, enabled = True\n",
    "):\n",
    "    if not enabled:\n",
    "        yield\n",
    "        return\n",
    "    # load the LoRA config\n",
    "    LoRACausalSelfAttention.lora_config = LoRAConfig(r=r, alpha=alpha, dropout=dropout)\n",
    "    # load raw model self attention\n",
    "    # causal_self_attention = llama.transformer.h[0].attn\n",
    "    causal_self_attention = llama.CausalSelfAttention\n",
    "    # replace the self attention with LoRA self attention\n",
    "    # llama.transformer.h[0].attn = LoRACausalSelfAttention()\n",
    "    llama.CausalSelfAttention = LoRACausalSelfAttention\n",
    "    # yield means when we use this context manager, we will run the code in the block, and then return to the original state\n",
    "    yield\n",
    "    # llama.transformer.h[0].attn = causal_self_attention\n",
    "    llama.CausalSelfAttention = causal_self_attention\n",
    "    LoRACausalSelfAttention.lora_config = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA(\n",
      "  (lm_head): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(128, 128)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): LoRACausalSelfAttention(\n",
      "          (c_attn): MergedLinear(\n",
      "            in_features=128, out_features=384, bias=False\n",
      "            (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_fc2): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = \"./data/model/iter-000200-ckpt.pth\"\n",
    "out_dir = \"/results\"\n",
    "config = LLaMAConfig.from_name(\"baby_llama\")\n",
    "config.block_size = 1024\n",
    "config.vocab_size = 128\n",
    "config.padded_vocab_size = 128\n",
    "config.n_embd = 128\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "with lora(r=lora_r, alpha=lora_alpha, dropout=lora_dropout, enabled=True):\n",
    "    model_lora = llama.LLaMA(config)\n",
    "    model_lora.load_state_dict(checkpoint, strict=False)\n",
    "    print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "bias = 'none'\n",
    "if bias == 'none':\n",
    "    print(bias)\n",
    "elif bias == 'all':\n",
    "    for n, p in model_lora.named_parameters():\n",
    "        if 'bias' in n:\n",
    "            print(\"bias true grad : \", n)\n",
    "            p.requires_grad = True\n",
    "elif bias == 'lora_only':\n",
    "    print('lora_only:')\n",
    "    for m in model_lora.modules():\n",
    "        if isinstance(m, LoRALayer) and \\\n",
    "            hasattr(m, 'bias') and \\\n",
    "            m.bias is not None:\n",
    "                m.bias.requires_grad = True\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------LLaMA without LoRA--------\n",
      "grad =  True \t[LLaMA] lm_head.weight\n",
      "grad =  True \t[LLaMA] transformer.wte.weight\n",
      "grad =  True \t[LLaMA] transformer.h.0.rms_1.scale\n",
      "grad =  True \t[LLaMA] transformer.h.0.attn.c_attn.weight\n",
      "grad =  True \t[LLaMA] transformer.h.0.attn.c_attn.bias\n",
      "grad =  True \t[LLaMA] transformer.h.0.attn.c_proj.weight\n",
      "grad =  True \t[LLaMA] transformer.h.0.attn.c_proj.bias\n",
      "grad =  True \t[LLaMA] transformer.h.0.rms_2.scale\n",
      "grad =  True \t[LLaMA] transformer.h.0.mlp.c_fc1.weight\n",
      "grad =  True \t[LLaMA] transformer.h.0.mlp.c_fc2.weight\n",
      "grad =  True \t[LLaMA] transformer.h.0.mlp.c_proj.weight\n",
      "grad =  True \t[LLaMA] transformer.h.1.rms_1.scale\n",
      "grad =  True \t[LLaMA] transformer.h.1.attn.c_attn.weight\n",
      "grad =  True \t[LLaMA] transformer.h.1.attn.c_attn.bias\n",
      "grad =  True \t[LLaMA] transformer.h.1.attn.c_proj.weight\n",
      "grad =  True \t[LLaMA] transformer.h.1.attn.c_proj.bias\n",
      "grad =  True \t[LLaMA] transformer.h.1.rms_2.scale\n",
      "grad =  True \t[LLaMA] transformer.h.1.mlp.c_fc1.weight\n",
      "grad =  True \t[LLaMA] transformer.h.1.mlp.c_fc2.weight\n",
      "grad =  True \t[LLaMA] transformer.h.1.mlp.c_proj.weight\n",
      "grad =  True \t[LLaMA] transformer.ln_f.scale\n"
     ]
    }
   ],
   "source": [
    "print('------------LLaMA without LoRA--------')\n",
    "for n, p in model.named_parameters():\n",
    "        if 'lora_' not in n:\n",
    "            print('grad = ', p.requires_grad, \"\\t[LLaMA]\", n)\n",
    "#             p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------LLaMA with LoRA---------\n",
      "grad =  False \t[LLaMA] lm_head.weight\n",
      "grad =  False \t[LLaMA] transformer.wte.weight\n",
      "grad =  False \t[LLaMA] transformer.h.0.rms_1.scale\n",
      "grad =  False \t[LLaMA] transformer.h.0.attn.c_attn.weight\n",
      "grad =  True \t[lora]: transformer.h.0.attn.c_attn.lora_A\n",
      "grad =  True \t[lora]: transformer.h.0.attn.c_attn.lora_B\n",
      "grad =  False \t[LLaMA] transformer.h.0.attn.c_proj.weight\n",
      "grad =  False \t[LLaMA] transformer.h.0.rms_2.scale\n",
      "grad =  False \t[LLaMA] transformer.h.0.mlp.c_fc1.weight\n",
      "grad =  False \t[LLaMA] transformer.h.0.mlp.c_fc2.weight\n",
      "grad =  False \t[LLaMA] transformer.h.0.mlp.c_proj.weight\n",
      "grad =  False \t[LLaMA] transformer.h.1.rms_1.scale\n",
      "grad =  False \t[LLaMA] transformer.h.1.attn.c_attn.weight\n",
      "grad =  True \t[lora]: transformer.h.1.attn.c_attn.lora_A\n",
      "grad =  True \t[lora]: transformer.h.1.attn.c_attn.lora_B\n",
      "grad =  False \t[LLaMA] transformer.h.1.attn.c_proj.weight\n",
      "grad =  False \t[LLaMA] transformer.h.1.rms_2.scale\n",
      "grad =  False \t[LLaMA] transformer.h.1.mlp.c_fc1.weight\n",
      "grad =  False \t[LLaMA] transformer.h.1.mlp.c_fc2.weight\n",
      "grad =  False \t[LLaMA] transformer.h.1.mlp.c_proj.weight\n",
      "grad =  False \t[LLaMA] transformer.ln_f.scale\n"
     ]
    }
   ],
   "source": [
    "print('-----------LLaMA with LoRA---------')\n",
    "for n, p in model_lora.named_parameters():\n",
    "        if 'lora_' not in n:\n",
    "#             print(\"[LLaMA]\", n)\n",
    "            p.requires_grad = False\n",
    "            print('grad = ', p.requires_grad, \"\\t[LLaMA]\", n)\n",
    "        else:\n",
    "            print('grad = ', p.requires_grad, \"\\t[lora]:\", n)\n",
    "#             print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----llama attention---- \n",
      " CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "  (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "\n",
      "----llama attention with LoRA---- \n",
      " LoRACausalSelfAttention(\n",
      "  (c_attn): MergedLinear(\n",
      "    in_features=128, out_features=384, bias=False\n",
      "    (lora_dropout): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llama_attn = model.transformer['h'][0].attn\n",
    "print('\\n----llama attention---- \\n',llama_attn)\n",
    "\n",
    "lora_attn = model_lora.transformer['h'][0].attn\n",
    "print('\\n----llama attention with LoRA---- \\n',lora_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------LoRA linear weight shape----------\n",
      "lora rank:  8\n",
      "lora alpha:  16\n",
      "lora_scaling: 2.0\n",
      "Pretrained Weight: torch.Size([384, 128])\n",
      "lora_A Weight: torch.Size([8, 128])\n",
      "lora_B Weight: torch.Size([384, 8])\n",
      "Pretrained Weight: torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "lora_attn = model_lora.transformer['h'][0].attn  # LLaMA Attention with LoRA\n",
    "lora_linear = lora_attn.c_attn               # Attention embedding->c_attn:linear(+LoRA)->QKV \n",
    "\n",
    "print('----------LoRA linear weight shape----------')\n",
    "print(\"lora rank: \", lora_linear.r)\n",
    "print(\"lora alpha: \", lora_linear.lora_alpha)\n",
    "print(\"lora_scaling:\", lora_linear.scaling)\n",
    "print('Pretrained Weight:',lora_linear.weight.shape)\n",
    "print('lora_A Weight:',lora_linear.lora_A.shape)\n",
    "print('lora_B Weight:',lora_linear.lora_B.shape)\n",
    "print('Pretrained Weight:',lora_linear.lora_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output size: \n",
      " torch.Size([16, 1024, 128])\n",
      "vocab size: \n",
      " 128\n",
      "loss: \n",
      " tensor(2.7823)\n"
     ]
    }
   ],
   "source": [
    "input = torch.load('input.pt')\n",
    "target = torch.load('target.pt')\n",
    "logits = model_lora(input)\n",
    "print('model output size: \\n', logits.shape)\n",
    "print('vocab size: \\n', config.vocab_size)\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1), ignore_index=-1)\n",
    "print('loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is llama embedding layer\n",
      "llama input shape \n",
      " torch.Size([16, 1024])\n",
      "llama embed shape \n",
      " torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "# llama-embedding\n",
    "print('This is llama embedding layer' )\n",
    "print('llama input shape \\n',input.shape)\n",
    "x = model_lora.transformer.wte(input)\n",
    "print('llama embed shape \\n',x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
