{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 原理解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qlearning 与 SARSA 不同的地方只在于 TD target, 在 SARSA 中, 我们会收集 (now_state, now_action, reward, next_state, next_action) 的数据，这里的 next_action 是通过 random.choice 选取出来的，但是在 Qlearning 中，是直接利用贪心的策略，不需要输入 next_action 的数据， 而是在直接选择 next_state 下，所有动作的 action_value 的最大值。\n",
    "\n",
    "同时需要注意的是，Qlearning 本身是一个 off-policy 的算法， 因为 Qlearning 的目的是想要得到 optimal policy , 这和我们 当前采取的 policy 是不同的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 off policy (classical ver.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from GridWorld import GridWorld\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Grid World\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "Initial Policy\n",
      "⬅️⬇️🔄🔄➡️\n",
      "⬅️⏩️⏪➡️➡️\n",
      "⬅️⬅️🔄⬇️⬇️\n",
      "⬅️⏪✅⏫️🔄\n",
      "➡️⏫️➡️➡️🔄\n",
      "Initial State Value: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "Initial Action Value: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "rows = 5\n",
    "columns = 5\n",
    "\n",
    "gridworld  = GridWorld(forbiddenAreaReward=-10, reward=1, desc=[\".....\", \".##..\", \"..#..\", \".#T#.\", \".#...\"])\n",
    "print('Initial Grid World')\n",
    "gridworld.show()\n",
    "\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*columns))] \n",
    "print('Initial Policy')\n",
    "gridworld.show_policy_matirx(policy)\n",
    "\n",
    "state_value = np.zeros((rows * columns))\n",
    "print(f'Initial State Value: {state_value}')\n",
    "\n",
    "action_value = np.zeros((rows * columns, 5))\n",
    "print(f'Initial Action Value: {action_value}')\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 999 \\ 1000\n",
      "episode end, trajectory length: 8\n",
      "state value updated: \n",
      "[1.82116797 2.02768054 2.25589071 2.50868309 2.78898621 1.6312208\n",
      " 1.60034826 1.89171341 1.95175734 3.10038853 1.44321933 1.06824428\n",
      " 4.8340822  2.81955038 3.44609244 1.13984092 4.86200633 4.73039236\n",
      " 4.89368464 3.83000398 0.65329565 4.4304419  5.2566311  4.73027429\n",
      " 4.2565327 ]\n",
      "policy updated\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "Final Policy\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "Final State Value\n",
      "[1.82116797 2.02768054 2.25589071 2.50868309 2.78898621 1.6312208\n",
      " 1.60034826 1.89171341 1.95175734 3.10038853 1.44321933 1.06824428\n",
      " 4.8340822  2.81955038 3.44609244 1.13984092 4.86200633 4.73039236\n",
      " 4.89368464 3.83000398 0.65329565 4.4304419  5.2566311  4.73027429\n",
      " 4.2565327 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(num_episodes):\n",
    "    clear_output(wait=True)\n",
    "    print(f'episode: {episode} \\ {num_episodes}')\n",
    "        # 定义epsilon-greedy策略\n",
    "    # greedy_action_prob = 1 - epsilon * (4 / 5)\n",
    "    # non_greedy_action_prob = epsilon / 5\n",
    "    # action_dict = { 1: greedy_action_prob,\n",
    "    #                0: non_greedy_action_prob}\n",
    "    # # 这一步是根据epsilon-greedy策略赋予每个状态动作的概率\n",
    "    # policy_epsilon_greedy = np.vectorize(action_dict.get)(policy)\n",
    "    # 检查每个状态被访问的次数\n",
    "    state_visited = [0 for _ in range(rows * columns)]\n",
    "    # 随机选取一个状态和动作, 根据书中off-policy的伪代码，不需要epsilon-greedy策略\n",
    "    now_state = random.choice(range(rows * columns))\n",
    "    now_action = random.choice(range(5))\n",
    "\n",
    "    # 根据伪代码，在一个 episode 下，我们根据先有的策略，生成一条轨迹，获取数据\n",
    "    trajectory = gridworld.get_episode_score(\n",
    "        now_state=now_state,\n",
    "        action=now_action,\n",
    "        policy=policy,\n",
    "        steps=-1,\n",
    "        stop_when_reach_target=True\n",
    "    )\n",
    "    print(f'episode end, trajectory length: {len(trajectory)}')\n",
    "\n",
    "    # 现在需要qlearning来更新action_value，注意这里是反向更新, 需要给len(trajectory)减去1，因为length如果是1，那么会循环2次\n",
    "    for k in range(len(trajectory) - 1, -1, -1):\n",
    "        last_state, last_action, reward, next_state, next_action = trajectory[k]\n",
    "        state_visited[last_state] += 1\n",
    "        # qlearning的更新公式: Q(s, a)(t+1) = Q(s, a)(t) - alpha * (Q(s, a)(t) - (r + gamma * max_a' Q(s, a)(t+1)))\n",
    "        # 注意这里是选择了下一个状态的最大action value 的动作，而不是next_action\n",
    "        TD_target = reward + gamma * np.max(action_value[next_state])\n",
    "        TD_error = action_value[last_state][last_action] - TD_target\n",
    "        # 更新action_value\n",
    "        action_value[last_state][last_action] -= alpha * TD_error\n",
    "        # 更新policy\n",
    "        policy[last_state] = np.eye(5)[np.argmax(action_value[last_state])]\n",
    "\n",
    "    # 更新state value\n",
    "    state_value = np.max(action_value, axis=1)\n",
    "    print(f'state value updated: \\n{state_value}')\n",
    "    print('policy updated')\n",
    "    gridworld.show_policy_matirx(policy)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print('Final Policy')\n",
    "gridworld.show_policy_matirx(policy)\n",
    "print('Final State Value')\n",
    "print(state_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBP-TSTL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
