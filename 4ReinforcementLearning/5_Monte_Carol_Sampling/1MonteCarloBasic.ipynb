{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Basic\n",
    "\n",
    "蒙特卡洛主要是利用了随机采样产生数据，通过产生的数据来更新策略。本质上类似于从数据估计出模型的参数。\n",
    "\n",
    "蒙特卡洛算法是对`policy iteration`算法的修改，把里面需要模型的部分(这里的模型指的就是在某个`state`我采取某个`action`的概率是多少，本质上就是策略)用先采样数据然后自己估计出模型来替换了。\n",
    "\n",
    "具体做法就是我遍历每一个`state`的每个`action`然后产生很多`trajactory/episode`，再求期望获取`action value`，采取最大的`action value`作为当前状态的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from GridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "🚫⬜️⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "🚫✅⬜️⬜️🚫\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9 \n",
    "rows = 5\n",
    "cols = 5\n",
    "# 加载网格世界\n",
    "grid_world = GridWorld(rows, cols, forbiddenAreaReward= -10)\n",
    "grid_world.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️➡️🔄🔄⬇️\n",
      "🔄⬆️⬅️⬅️⬇️\n",
      "⏫️🔄🔄⬅️⬆️\n",
      "⬅️⬅️⬇️⬆️⬅️\n",
      "⏪✅⬅️⬅️⏩️\n"
     ]
    }
   ],
   "source": [
    "# 定义episode length\n",
    "episode_length = 100\n",
    "\n",
    "# state value, 初始化为0， 表示每个state的value\n",
    "value = np.zeros(rows*cols) \n",
    "# action value, 初始化为0, 表示每个state的5个action的value\n",
    "qtable = np.zeros((rows*cols, 5)) \n",
    "\n",
    "# 蒙特卡洛方法一开始是从一个随机的policy开始的，这里我们定义一个随机的policy\n",
    "# np.random.seed(50)\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*cols))] \n",
    "grid_world.show_policy_matirx(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Random Policy...\n",
      "done!\n",
      "Initial Q Table\n",
      "done!\n",
      "Start Q Value Update...\n",
      "-----------------------------------\n",
      "q value update start at 0[125.0]\n",
      "now policy: \n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⬆️⬆️⬆️⬆️\n",
      "⏫️⬆️⬇️⬆️⬆️\n",
      "⬆️⬇️⬇️⬇️⬆️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 0[178116.11412857752]\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "q value update start at 1[178116.11412857752]\n",
      "now policy: \n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⬆️⬇️⬆️⬆️\n",
      "⏫️⬇️⬇️⬇️⬆️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 1[170158.66223275958]\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "q value update start at 2[170158.66223275958]\n",
      "now policy: \n",
      "➡️➡️⬇️➡️⬇️\n",
      "⬆️⬇️⬇️⬇️⬆️\n",
      "⏩️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 2[2435.6935224546905]\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "q value update start at 3[2435.6935224546905]\n",
      "now policy: \n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 3[1402.5967726007984]\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "q value update start at 4[1402.5967726007984]\n",
      "now policy: \n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 4[436.2953142781263]\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "q value update start at 5[436.2953142781263]\n",
      "now policy: \n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 5[0.0]\n",
      "-----------------------------------\n",
      "Optimal Policy Found!\n",
      "Final Policy\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "Final Q Table\n",
      "[[-4.09533905  6.56076095  6.56076095 -4.09533905  5.90466095]\n",
      " [-3.43923905  5.90466095  7.28976095  5.90466095  6.56076095]\n",
      " [-4.09533905  5.31417095  6.56076095  6.56076095  5.90466095]\n",
      " [-4.68582905  4.78272995  5.90466095  5.90466095  5.31417095]\n",
      " [-5.21727005 -5.21727005  5.31417095  5.31417095  4.78272995]\n",
      " [ 5.90466095  7.28976095 -2.71023905 -3.43923905  6.56076095]\n",
      " [ 6.56076095  6.56076095  8.09976095  6.56076095  7.28976095]\n",
      " [ 5.90466095  5.90466095  7.28976095  7.28976095  6.56076095]\n",
      " [ 5.31417095  5.31417095  6.56076095  6.56076095  5.90466095]\n",
      " [ 4.78272995 -4.68582905  5.90466095  5.90466095  5.31417095]\n",
      " [ 6.56076095  8.09976095  8.09976095 -2.71023905 -2.71023905]\n",
      " [ 7.28976095  7.28976095  8.99976095 -2.71023905  8.09976095]\n",
      " [ 6.56076095  6.56076095  8.09976095  8.09976095  7.28976095]\n",
      " [ 5.90466095  5.90466095  7.28976095  7.28976095  6.56076095]\n",
      " [ 5.31417095 -4.09533905  6.56076095  6.56076095  5.90466095]\n",
      " [-2.71023905  8.99976095 -1.00023905 -1.90023905  8.09976095]\n",
      " [ 8.09976095  8.09976095  9.99976095  8.09976095  8.99976095]\n",
      " [ 7.28976095  7.28976095  8.99976095  8.99976095  8.09976095]\n",
      " [ 6.56076095  6.56076095  8.09976095  8.09976095  7.28976095]\n",
      " [ 5.90466095 -3.43923905 -2.71023905  7.28976095  6.56076095]\n",
      " [ 8.09976095  9.99976095 -1.00023905 -1.00023905 -1.00023905]\n",
      " [ 8.99976095  8.99976095 -1.00023905 -1.00023905  9.99976095]\n",
      " [ 8.09976095  8.09976095 -1.00023905  9.99976095  8.99976095]\n",
      " [ 7.28976095 -2.71023905 -1.90023905  8.99976095  8.09976095]\n",
      " [ 6.56076095 -2.71023905 -2.71023905  8.09976095 -2.71023905]]\n"
     ]
    }
   ],
   "source": [
    "# 通过采样的方法计算action value\n",
    "# 这里和policy iteration不同的地方是，在policy iteration中，我们是通过迭代的方法来计算value，而在蒙特卡洛方法中，我们是通过采样的方法来计算value\n",
    "# 在policy iteration中，我们是已知一个固定策略的\n",
    "\n",
    "print('Generate Random Policy...')\n",
    "qtable = np.zeros((rows*cols, 5))\n",
    "print('done!')\n",
    "print('Initial Q Table')\n",
    "pre_qtabel = qtable.copy() + 1\n",
    "print('done!')\n",
    "threshold = 0.001\n",
    "print('Start Q Value Update...')\n",
    "cut = 0\n",
    "cut_threshold = 1000\n",
    "while np.sum((pre_qtabel-qtable)**2) > threshold and cut < cut_threshold:\n",
    "    print('-----------------------------------')\n",
    "    print(f'q value update start at {cut}[{np.sum((pre_qtabel-qtable)**2)}]')\n",
    "    pre_qtabel = qtable.copy()\n",
    "    # 通过采样的方法计算action value\n",
    "    # 遍历每一个状态\n",
    "    for i in range(rows*cols):\n",
    "        # 遍历每一个action\n",
    "        for j in range(5):\n",
    "            # 下面函数的返回值是一个元组列表，每一个元组包含一个episode的信息，包括state, action, reward\n",
    "            episode = grid_world.get_episode_score(\n",
    "                now_state=i,\n",
    "                action=j,\n",
    "                policy=policy,\n",
    "                steps=episode_length,\n",
    "            )\n",
    "            # 然后我们需要计算这一条episode的所对应的action value\n",
    "            # action value的计算方法是：Gt = Rt+1 + gamma*Rt+2 + gamma^2*Rt+3 + ... + gamma^(T-1)*Rt+T\n",
    "            # q(s,a) = E[Gt|St=s, At=a]\n",
    "            # temp取的是episode的最后一个状态的reward\n",
    "            # 这里是从后往前计算的,最后展开就是上面对应的式子\n",
    "            temp_reward = episode[episode_length][2]\n",
    "            for k in range(episode_length-1, -1, -1):\n",
    "                temp_reward = episode[k][2] + gamma*temp_reward\n",
    "            # 更新qtable\n",
    "            qtable[i][j] = temp_reward\n",
    "    # 选取最大的action value的action作为policy\n",
    "    policy = np.eye(5)[np.argmax(qtable, axis=1)]\n",
    "    print('now policy: ')\n",
    "    grid_world.show_policy_matirx(policy)\n",
    "    print(f'q value update end at : {cut}[{np.sum((pre_qtabel-qtable)**2)}]')\n",
    "    cut += 1\n",
    "    print('-----------------------------------')\n",
    "print('Optimal Policy Found!')\n",
    "print('Final Policy')\n",
    "grid_world.show_policy_matirx(policy)\n",
    "print('Final Q Table')\n",
    "print(qtable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBP-TSTL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
