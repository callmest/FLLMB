{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Exploring Starts\n",
    "\n",
    "蒙特卡洛Basic的算法的缺点是需要遍历每一个`state`的每一个`action`，得到`action value`\n",
    "\n",
    "但在经过每一个`episode`的时候，中间会有很多重复的步骤，称之为`visit`，或许可以只根据我在一条`episode`探索的时候，中间获得的`visit`来作为当前状态动作的`action value`\n",
    "\n",
    "当然这样是不精确的，怎么样用`visit`来做`action value` 会在下一小节提到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\callmest\\.conda\\envs\\RBP-TSTL\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from GridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "🚫⬜️⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "🚫✅⬜️⬜️🚫\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9 \n",
    "rows = 5\n",
    "cols = 5\n",
    "# 加载网格世界\n",
    "grid_world = GridWorld(rows, cols, forbiddenAreaReward= -10)\n",
    "grid_world.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️⬇️🔄⬅️⬇️\n",
      "➡️🔄⬆️🔄🔄\n",
      "🔄🔄⬅️⬅️➡️\n",
      "⬆️⬆️⬇️⬆️⬆️\n",
      "⏩️✅⬅️⬅️⏪\n"
     ]
    }
   ],
   "source": [
    "# 定义episode length\n",
    "episode_length = 100\n",
    "\n",
    "# state value, 初始化为0， 表示每个state的value\n",
    "value = np.zeros(rows*cols) \n",
    "# action value, 初始化为0, 表示每个state的5个action的value\n",
    "qtable = np.zeros((rows*cols, 5)) \n",
    "\n",
    "# 蒙特卡洛方法一开始是从一个随机的policy开始的，这里我们定义一个随机的policy\n",
    "# np.random.seed(50)\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*cols))] \n",
    "grid_world.show_policy_matirx(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Random Policy...\n",
      "done!\n",
      "Initial Q Table\n",
      "done!\n",
      "Start Q Value Update...\n",
      "q value update start at 0[125.0]\n",
      "now policy: \n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⬆️⬆️⬆️⬆️\n",
      "⏫️⬆️⬇️⬆️⬆️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 0[168171.90793243307]\n",
      "q value update start at 1[168171.90793243307]\n",
      "now policy: \n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⬆️⬇️⬆️⬆️\n",
      "⏬⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 1[167335.0540029216]\n",
      "q value update start at 2[167335.0540029216]\n",
      "now policy: \n",
      "➡️➡️⬇️➡️⬇️\n",
      "⬆️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬅️⬅️⬅️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 2[1198.494657015979]\n",
      "q value update start at 3[1198.494657015979]\n",
      "now policy: \n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏬⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 3[1310.7814712809559]\n",
      "q value update start at 4[1310.7814712809559]\n",
      "now policy: \n",
      "⬇️⬇️⬅️⬅️⬅️\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "⏩️⬇️⬅️⬅️⬅️\n",
      "➡️⬇️⬇️⬇️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 4[44.616643355203166]\n",
      "q value update start at 5[44.616643355203166]\n",
      "now policy: \n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏬⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "q value update end at : 5[9.830201680304264e-08]\n",
      "Optimal Policy Found!\n",
      "Final Policy\n",
      "➡️⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏬⬇️⬇️⬇️⬇️\n",
      "➡️⬇️⬅️⬅️⬅️\n",
      "⏩️✅⬅️⬅️⏪\n",
      "Final Q Table\n",
      "[[-4.09533905  6.56076095  6.56073439 -4.09533905  5.90466095]\n",
      " [-3.43923905  5.90466095  7.28963565  5.90466095  6.56076095]\n",
      " [-4.09533905  5.31417095  6.56076095  6.56067208  5.90466095]\n",
      " [-4.68582905  4.78272995  5.90466095  5.90460487  5.31417095]\n",
      " [-5.21727005 -5.21727005  5.31417095  5.31414439  4.78272995]\n",
      " [ 5.90466095  7.28973439 -2.71023905 -3.43923905  6.56076095]\n",
      " [ 6.56076095  6.56076095  8.09973439  6.56076095  7.28976095]\n",
      " [ 5.90466095  5.90466095  7.28973439  7.28976095  6.56076095]\n",
      " [ 5.31417095  5.31417095  6.56073439  6.56076095  5.90466095]\n",
      " [ 4.78272995 -4.68582905  5.90463439  5.90466095  5.31417095]\n",
      " [ 6.56076095  8.09973439  8.09976095 -2.71023905 -2.71023905]\n",
      " [ 7.28976095  7.28976095  8.99963565 -2.71023905  8.09976095]\n",
      " [ 6.56076095  6.56076095  8.09976095  8.09967208  7.28976095]\n",
      " [ 5.90466095  5.90466095  7.28976095  7.28970487  6.56076095]\n",
      " [ 5.31417095 -4.09533905  6.56076095  6.56073439  5.90466095]\n",
      " [-2.71023905  8.99973439 -1.00023905 -1.90023905  8.09976095]\n",
      " [ 8.09976095  8.09976095  9.99973439  8.09976095  8.99976095]\n",
      " [ 7.28976095  7.28976095  8.99973439  8.99976095  8.09976095]\n",
      " [ 6.56076095  6.56076095  8.09970487  8.09976095  7.28976095]\n",
      " [ 5.90466095 -3.43923905 -2.71023905  7.28973439  6.56076095]\n",
      " [ 8.09976095  9.99973439 -1.00023905 -1.00023905 -1.00023905]\n",
      " [ 8.99976095  8.99976095 -1.00023905 -1.00023905  9.15628416]\n",
      " [ 8.09976095  8.09976095 -1.00023905  9.99967208  8.99976095]\n",
      " [ 7.28976095 -2.71023905 -1.90023905  8.99970487  8.09976095]\n",
      " [ 6.56076095 -2.71023905 -2.71023905  8.09973439 -2.71023905]]\n"
     ]
    }
   ],
   "source": [
    "# 通过采样的方法计算action value\n",
    "# 这里和policy iteration不同的地方是，在policy iteration中，我们是通过迭代的方法来计算value，而在蒙特卡洛方法中，我们是通过采样的方法来计算value\n",
    "# 在policy iteration中，我们是已知一个固定策略的\n",
    "\n",
    "print('Generate Random Policy...')\n",
    "qtable = np.zeros((rows*cols, 5))\n",
    "print('done!')\n",
    "print('Initial Q Table')\n",
    "pre_qtabel = qtable.copy() + 1\n",
    "print('done!')\n",
    "threshold = 0.001\n",
    "print('Start Q Value Update...')\n",
    "cut = 0\n",
    "cut_threshold = 1000\n",
    "while np.sum((pre_qtabel-qtable)**2) > threshold and cut < cut_threshold:\n",
    "    print('-----------------------------------')\n",
    "    print(f'q value update start at {cut}[{np.sum((pre_qtabel-qtable)**2)}]')\n",
    "    pre_qtabel = qtable.copy()\n",
    "    # 通过采样的方法计算action value\n",
    "    # 遍历每一个状态\n",
    "    for i in range(rows*cols):\n",
    "        # 遍历每一个action\n",
    "        for j in range(5):\n",
    "            # 初始化qtable_rewards和qtable_counts\n",
    "\n",
    "            qtable_rewards = [[0 for _ in range(5)] for _ in range(rows*cols)]\n",
    "            qtable_counts = [[0 for _ in range(5)] for _ in range(rows*cols)]\n",
    "            # 下面函数的返回值是一个元组列表，每一个元组包含一个episode的信息，包括state, action, reward\n",
    "            episode = grid_world.get_episode_score(\n",
    "                now_state=i,\n",
    "                action=j,\n",
    "                policy=policy,\n",
    "                steps=episode_length,\n",
    "            )\n",
    "\n",
    "            reward = episode[episode_length][2]\n",
    "            for k in range(episode_length-1, -1, -1):\n",
    "                # 需要提取出每一个episode的信息，包括state, action, reward\n",
    "                temp_state = episode[k][0]\n",
    "                temp_action = episode[k][1]\n",
    "                temp_reward = episode[k][2]\n",
    "                # 先计算当前的action value\n",
    "                reward = temp_reward + gamma * reward\n",
    "                # 更新qtable_rewards和qtable_counts\n",
    "                # 将episode中的reward加入到qtable_rewards中\n",
    "                qtable_rewards[temp_state][temp_action] += reward\n",
    "                # 将episode中的count加入到qtable_counts中\n",
    "                qtable_counts[temp_state][temp_action] += 1  \n",
    "                # 这里采用的是平均值的方法来更新qtable， 即every visit\n",
    "                qtable[temp_state][temp_action] = qtable_rewards[temp_state][temp_action] / qtable_counts[temp_state][temp_action]\n",
    "\n",
    "                # first visit\n",
    "                # if qtable_counts[temp_state][temp_action] == 0:\n",
    "                #     qtable_rewards[temp_state][temp_action] = reward\n",
    "                #     qtable[temp_state][temp_action] = qtable_rewards[temp_state][temp_action] / qtable_counts[temp_state][temp_action]\n",
    "                #     qtable_counts[temp_state][temp_action] += 1 \n",
    "    \n",
    "    # 选取最大的action value的action作为policy\n",
    "    policy = np.eye(5)[np.argmax(qtable, axis=1)]\n",
    "    print('now policy: ')\n",
    "    grid_world.show_policy_matirx(policy)\n",
    "    print(f'q value update end at : {cut}[{np.sum((pre_qtabel-qtable)**2)}]')\n",
    "    cut += 1\n",
    "    print('-----------------------------------')\n",
    "print('Optimal Policy Found!')\n",
    "print('Final Policy')\n",
    "grid_world.show_policy_matirx(policy)\n",
    "print('Final Q Table')\n",
    "print(qtable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBP-TSTL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
