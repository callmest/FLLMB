{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: \n",
      "\t{(1, 2): 1, (2, 3): 2, (3, 4): 2, (4, 6): 1, (6, 2): 1}\n"
     ]
    }
   ],
   "source": [
    "# get different pair counts\n",
    "def get_stats(ids, counts = None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# debug\n",
    "example = [1, 2, 3, 4, 6, 2, 3, 4]\n",
    "counts = get_stats(example)\n",
    "print(f'counts: \\n\\t{counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newids: \n",
      "\t[4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# merge according to stats, merge results\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i] == pair[0] and ids[i+1] == pair[1] and i < len(ids) - 1:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# debug\n",
    "ids=[1, 2, 3, 1, 2]\n",
    "pair=(1, 2)\n",
    "newids = merge(ids, pair, 4)\n",
    "print(f'newids: \\n\\t{newids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "def build_vocab():\n",
    "    merges = {}\n",
    "    vocab = {idx: bytes([idx]) for idx in range(8)}\n",
    "    for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'n '\n",
      "b'in'\n",
      "b'an'\n",
      "b's '\n",
      "b'ed'\n",
      "b'en'\n",
      "b'on '\n",
      "b'er'\n",
      "b'e '\n",
      "b'ou'\n",
      "b't '\n",
      "b'ti'\n",
      "b'fo'\n",
      "b'for'\n",
      "b'ar'\n",
      "b'od'\n",
      "b'is '\n",
      "b'eed'\n",
      "b'an '\n",
      "b'ba'\n",
      "b'tion '\n",
      "b're'\n",
      "b'ing'\n",
      "b'ing '\n",
      "b'  '\n",
      "b'ge '\n",
      "b'ua'\n",
      "b'al'\n",
      "b'l '\n",
      "b'l y'\n",
      "b'l you'\n",
      "b'l you '\n",
      "b'l you n'\n",
      "b'l you need'\n",
      "b'ha'\n",
      "b'can '\n",
      "b', '\n",
      "b'tt'\n",
      "b'si'\n",
      "b'Tr'\n",
      "b'Tran'\n",
      "b'Trans'\n",
      "b'Transfor'\n",
      "b'Transform'\n",
      "b'Transformer'\n",
      "b'Transformers'\n",
      "b'tr'\n",
      "b'Re'\n",
      "b'infor'\n",
      "b'inforc'\n",
      "b'inforce'\n",
      "b'inforcem'\n",
      "b'inforcemen'\n",
      "b'inforcement '\n",
      "b' re'\n",
      "b' res'\n",
      "b'on'\n",
      "b'   '\n",
      "b'   L'\n",
      "b'   Lar'\n",
      "b'   Large '\n",
      "b'   Large L'\n",
      "b'   Large Lan'\n",
      "b'   Large Lang'\n",
      "b'   Large Langua'\n",
      "b'   Large Language '\n",
      "b'   Large Language M'\n",
      "b'   Large Language Mod'\n",
      "b'   Large Language Mode'\n",
      "b'   Large Language Model'\n",
      "b'   Large Language Models '\n",
      "b'   Large Language Models is '\n",
      "b'   Large Language Models is al'\n",
      "b'   Large Language Models is all you need'\n",
      "b'   Large Language Models is all you need,'\n",
      "b'   Large Language Models is all you need,w'\n",
      "b'   Large Language Models is all you need,wha'\n",
      "b'   Large Language Models is all you need,what '\n",
      "b'   Large Language Models is all you need,what can '\n",
      "b'   Large Language Models is all you need,what can i'\n",
      "b'   Large Language Models is all you need,what can i '\n",
      "b'   Large Language Models is all you need,what can i s'\n",
      "b'   Large Language Models is all you need,what can i sa'\n",
      "b'   Large Language Models is all you need,what can i say'\n",
      "b'   Large Language Models is all you need,what can i say, '\n",
      "b'   Large Language Models is all you need,what can i say, m'\n",
      "b'   Large Language Models is all you need,what can i say, man'\n",
      "b'   Large Language Models is all you need,what can i say, manba'\n",
      "b'   Large Language Models is all you need,what can i say, manba '\n",
      "b'   Large Language Models is all you need,what can i say, manba ou'\n",
      "b'   Large Language Models is all you need,what can i say, manba out'\n",
      "{(110, 32): 256, (105, 110): 257, (97, 110): 258, (115, 32): 259, (101, 100): 260, (101, 110): 261, (111, 256): 262, (101, 114): 263, (101, 32): 264, (111, 117): 265, (116, 32): 266, (116, 105): 267, (102, 111): 268, (268, 114): 269, (97, 114): 270, (111, 100): 271, (105, 259): 272, (101, 260): 273, (97, 256): 274, (98, 97): 275, (267, 262): 276, (114, 101): 277, (257, 103): 278, (278, 32): 279, (32, 32): 280, (103, 264): 281, (117, 97): 282, (97, 108): 283, (108, 32): 284, (284, 121): 285, (285, 265): 286, (286, 32): 287, (287, 110): 288, (288, 273): 289, (104, 97): 290, (99, 274): 291, (44, 32): 292, (116, 116): 293, (115, 105): 294, (84, 114): 295, (295, 258): 296, (296, 115): 297, (297, 269): 298, (298, 109): 299, (299, 263): 300, (300, 115): 301, (116, 114): 302, (82, 101): 303, (257, 269): 304, (304, 99): 305, (305, 101): 306, (306, 109): 307, (307, 261): 308, (308, 266): 309, (32, 277): 310, (310, 115): 311, (111, 110): 312, (280, 32): 313, (313, 76): 314, (314, 270): 315, (315, 281): 316, (316, 76): 317, (317, 258): 318, (318, 103): 319, (319, 282): 320, (320, 281): 321, (321, 77): 322, (322, 271): 323, (323, 101): 324, (324, 108): 325, (325, 259): 326, (326, 272): 327, (327, 283): 328, (328, 289): 329, (329, 44): 330, (330, 119): 331, (331, 290): 332, (332, 266): 333, (333, 291): 334, (334, 105): 335, (335, 32): 336, (336, 115): 337, (337, 97): 338, (338, 121): 339, (339, 292): 340, (340, 109): 341, (341, 258): 342, (342, 275): 343, (343, 32): 344, (344, 265): 345, (345, 116): 346}\n"
     ]
    }
   ],
   "source": [
    "# BPE\n",
    "class BasicTokenizer():\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.vocab = self.build_vocab()\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        return vocab\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        text_bytes = text.encode(\"utf-8\") \n",
    "        ids = list(text_bytes) \n",
    "\n",
    "        merges = {} \n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            pair = max(stats, key=stats.get) \n",
    "            idx = 256 + i\n",
    "            # returned ids is the new token where corresponding elements were replaced by pair idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # merges is the new merged alphabet\n",
    "            merges[pair] = idx\n",
    "            # vocab is the total alphabet\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "            self.merges = merges\n",
    "            self.vocab = vocab\n",
    "            \n",
    "    def text2id(self, text):\n",
    "        text = \"N\".encode('utf-8')\n",
    "        for k, p in self.vocab.items():\n",
    "            if p == text:\n",
    "                return k\n",
    "            else:\n",
    "                return 10086\n",
    "# debug\n",
    "bpe = BasicTokenizer()\n",
    "text = '''   \n",
    "Large Language Models is all you need,\n",
    "what can i say, manba out. \n",
    "Attention is All you need.\n",
    "Vision Transformers, \n",
    "Generative Pretrained Transformers,\n",
    "Reinforcement leraning from human feedback\n",
    "chain of thought is basic resoning tool.\n",
    "LLMs can evaluate NLP results.\n",
    "Richard Sutton Refinforcement Learning Introduction edition 2.\n",
    "encoder-only\n",
    "'''\n",
    "vocab_size = len(list(text.encode('utf-8')))\n",
    "bpe.train(text.replace('\\n', ''), vocab_size = vocab_size)\n",
    "for i in range(256,vocab_size,1):\n",
    "    print(bpe.vocab[i])\n",
    "print(bpe.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 110)\n",
      "b'a' b'n'\n",
      "(101, 114)\n",
      "b'e' b'r'\n",
      "(101, 32)\n",
      "b'e' b' '\n",
      "(116, 114)\n",
      "b't' b'r'\n",
      "(105, 32)\n",
      "b'i' b' '\n",
      "[105, 32, 108, 111, 118, 264, 302, 258, 115, 102, 114, 111, 109, 263, 115]\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "# utf-8 token ids\n",
    "text = 'i love transfromers'\n",
    "text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "\n",
    "# bpe token ids\n",
    "ids = list(text_bytes) # list of integers in range 0..255\n",
    "while len(ids) >= 2:\n",
    "    stats = get_stats(ids)\n",
    "    # ids = (2,3,4,5)\n",
    "    #    2\n",
    "    # 2, 3\n",
    "    # 3, 4\n",
    "    # 4, 5\n",
    "    # 5\n",
    "    # statsï¼š pair(2,3), (3,4), (4,5)\n",
    "    # pair (2,3)~268, (3,4)~269 ...\n",
    "  \n",
    "    # bpe.merges.get([3,4]) = 268\n",
    "    # bpe.merges.get([3,5]) = 289\n",
    "    # bpe.merges.get([4,5]) = inf\n",
    "    # min means most frequently in 'merge'\n",
    "    pair = min(stats, key=lambda p: bpe.merges.get(p, float(\"inf\"))) \n",
    "    print(pair)\n",
    "    print(bpe.vocab[pair[0]], bpe.vocab[pair[1]])\n",
    "    if pair not in bpe.merges:\n",
    "        break \n",
    "    idx = bpe.merges[pair] # (3,4) -> 268\n",
    "    ids = merge(ids, pair, idx) # (2,3,4,5) -> (2, 268, 5)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love transfromers\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "text_bytes = b\"\".join(bpe.vocab[idx] for idx in ids)\n",
    "decode_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "print(decode_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
