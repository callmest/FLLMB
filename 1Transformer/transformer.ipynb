{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d993145",
   "metadata": {},
   "source": [
    "## 0. parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7965c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from torch import optim\n",
    "# 定义模型的参数\n",
    "enc_voc_size = 5893\n",
    "dec_voc_size = 7853\n",
    "d_model = 512\n",
    "max_len = 256\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d8d1f",
   "metadata": {},
   "source": [
    "## 1. transformer module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf4d5d",
   "metadata": {},
   "source": [
    "## 1.1 Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45ed1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Linear(in_features=4, out_features=5, bias=True)\n",
      "b Embedding(4, 5)\n",
      "a.weight.shape torch.Size([5, 4])\n",
      "b.weight.shape torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 构造一个线性层\n",
    "a = torch.nn.Linear(4,5)\n",
    "# 构造一个embedding层\n",
    "b = torch.nn.Embedding(4,5)\n",
    "\n",
    "print('a', a)\n",
    "print('b', b)\n",
    "print('a.weight.shape', a.weight.shape)\n",
    "print('b.weight.shape', b.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08983c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([14, 512])\n",
      "输入数据 torch.Size([3, 10])\n",
      "输入数据的embedding torch.Size([3, 10, 512])\n",
      "embd_layer 4 weight: tensor([ 1.5772,  0.6077,  0.5554, -0.3491,  0.5295, -0.8412,  0.0257,  0.2443,\n",
      "        -0.1485, -0.4112], grad_fn=<SliceBackward0>)\n",
      "embedding 4 ouptput: tensor([ 1.5772,  0.6077,  0.5554, -0.3491,  0.5295, -0.8412,  0.0257,  0.2443,\n",
      "        -0.1485, -0.4112], grad_fn=<SliceBackward0>)\n",
      "embedding.weight torch.Size([14, 512])\n",
      "error: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "# 这个部分是探究embedding layer 编码之后， 词表内相同的元素的embedding都是相同的\n",
    "\n",
    "# 构建一个词表大小为14，embedding维度为512的embedding层\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "# 构造输入数据\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,13,3, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "\n",
    "embedding = embd_layer(input_id)\n",
    "print(\"输入数据\",input_id.shape)\n",
    "print(\"输入数据的embedding\", embedding.shape)\n",
    "# 取 embd_layer 的 第四个 和 input_id 中 4 数字的编码\n",
    "print('embd_layer 4 weight:', embd_layer.weight[4,:10])\n",
    "print('embedding 4 ouptput:',embedding[0][1][:10])\n",
    "\n",
    "# 注意：embedding的词表大小维度必须包含input_id中的不同id，否则会报错\n",
    "# 再次构建一个只有14个词的embedding layer\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "# 构造输入数据\n",
    "# 但是现在input_id中有15种不同id\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,14,15, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "try:\n",
    "    embedding = embd_layer(input_id)\n",
    "except Exception as e:\n",
    "    print('error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb93be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_src_tokne:  TokenEmbedding(5893, 512, padding_idx=1)\n",
      "test_trg_token:  TokenEmbedding(7853, 512, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "# 创建tokenembedding类\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # 这里是继承了nn.embedding 中的 init方法\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "\n",
    "test_src_token = TokenEmbedding(enc_voc_size, d_model)\n",
    "test_trg_token = TokenEmbedding(dec_voc_size, d_model)\n",
    "print('test_src_tokne: ', test_src_token)\n",
    "print('test_trg_token: ', test_trg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fe7bb",
   "metadata": {},
   "source": [
    "### 1.2 positional Encoding\n",
    "采用正余弦函数 主要是为了通过周期函数的组合来表示相对位置信息\n",
    "可以参考：https://blog.csdn.net/m0_37605642/article/details/132866365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f90f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pos_encoding:  torch.Size([256, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 构建position embedding\n",
    "        self.embedding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.embedding.requires_grad = False\n",
    "\n",
    "        # 构建位置向量\n",
    "        # [max_len] -> [max_len,1]\n",
    "        pos = torch.arange(0, max_len, device=device).float().unsqueeze(dim = 1)\n",
    "\n",
    "        # 构建维度向量\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        # 计算位置编码\n",
    "        # x:y:z 指的是从x到y，每隔z个位置取一次\n",
    "        self.embedding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.embedding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        # 从这里可以看出 positional embedding 构建好之后 其实是定值了\n",
    "        return self.embedding[:seq_len, :]\n",
    "\n",
    "# 测试\n",
    "test_pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
    "print('test_pos_encoding: ',test_pos_encoding.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3827d",
   "metadata": {},
   "source": [
    "### 1.3 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40dc90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_layerNorm gamma shape:  torch.Size([512])\n",
      "test_layerNorm beta shape:  torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model, eps = 1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layernorm 是在最后一维的归一化 通俗的来说，就是对一个样本的所有特征值进行归一化\n",
    "        # 1. 计算均值\n",
    "        mean = x.mean(-1, keepdim = True)\n",
    "        # 2. 计算方差\n",
    "        var = x.var(-1, keepdim = True, unbiased = False)\n",
    "        # 3. 计算标准化x\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # 4. 计算归一化 用可学习的参数拟合\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "\n",
    "test_layer_norm = LayerNorm(d_model)\n",
    "print('test_layerNorm gamma shape: ', test_layer_norm.gamma.shape)\n",
    "print('test_layerNorm beta shape: ', test_layer_norm.beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db81fe2",
   "metadata": {},
   "source": [
    "### 1.4 scaled dot product attention\n",
    "单头注意力机制 并且不带mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ce1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaleDotProductAttention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim = -1) # 在最后一维进行softmax\n",
    "\n",
    "    def forward(self, q, k, v, mask = None, e = 1e-10):\n",
    "        # 1. 得到q,k,v的维度, 这里是单头注意力机制，所以head_num = 1\n",
    "        batch_size, head_num, seq_len, d_model = k.size()\n",
    "        # 2. 计算q,k的点积 并进行scale，进行scale的目的是如果数值过大 进行softmax后梯度都会很小\n",
    "        score = (q @ k.transpose(2, 3)) / math.sqrt(d_model)\n",
    "        # 3. mask fill\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score)\n",
    "        # 4. 计算 v\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb3e55",
   "metadata": {},
   "source": [
    "### 1.5 position wise feed forward\n",
    "其实就是mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db3190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661e2d4",
   "metadata": {},
   "source": [
    "### 1.6 multi-head attention\n",
    "把单头注意力改成多头注意力 本质上相当于cnn的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c11a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        # 确定头的数量\n",
    "        self.n_head = n_head\n",
    "        # 注意力机制仍然是一样的算法\n",
    "        self.attention = scaleDotProductAttention()\n",
    "        # 用线性层得到q 注意在上面的attention计算中是没有qkv的产生过程的\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 多头注意力concat后经过一个线性层\n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # 传入的qkv应该是都是x\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        # 将qkv进行拆分\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        # 计算attention\n",
    "        out, attention = self.attention(q, k, v, mask = mask)\n",
    "        # 再将out拼接在一起\n",
    "        out = self.concat(out)\n",
    "        # 经过一次线性变换后输出\n",
    "        out = self.concat_linear(out)\n",
    "        return out\n",
    "\n",
    "    def split(self, tensor):\n",
    "        batch_size, seq_len, d_model = tensor.size()\n",
    "        # 这里其实要注意是否会整除不尽\n",
    "        d_tensor = d_model // self.n_head\n",
    "        # 这里要将头拿到前面来\n",
    "        tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.size()\n",
    "        d_model = n_head * d_tensor\n",
    "        # 拼接，这里的 contiguous 意思是连续 是为了保证在 transpose 后能够进行view。参考：https://zhuanlan.zhihu.com/p/64551412\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ea074",
   "metadata": {},
   "source": [
    "## 2. transformer model\n",
    " 接下来 我们会将上面的module 逐步组合成 transformer 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd964f",
   "metadata": {},
   "source": [
    "### 2.1 transformer embedding 层\n",
    "embedding层就是将input_id 的编码和 位置编码 `加和` 在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ce3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.position_emb = PositionalEncoding(d_model=d_model, max_len=max_len, device=device)\n",
    "        self.drop_out = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_embeding = self.token_emb(x)\n",
    "        position_embedding = self.position_emb(x)\n",
    "        # 相加后然后还要drop\n",
    "        embedding = self.drop_out(token_embeding + position_embedding)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed66f6",
   "metadata": {},
   "source": [
    "### 2.2 encoder layer\n",
    "编码后输入encoding层\n",
    "这里需要注意一些措辞上的区别： encode layer或者 encode block 堆叠在一起 形成 一个 transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4580193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionWiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        # 输入的x是经过位置编码后的\n",
    "        # 1. 保留x 作为后面的short cut\n",
    "        _x = x\n",
    "        # 2. 先经过一次attention的计算\n",
    "        x = self.attention(q = x, k = x, v = x, mask = s_mask)\n",
    "        # 3. add and norm (要先drop)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        # 4. ffn 保留一次short cut\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        # 5. 再次add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61e75c",
   "metadata": {},
   "source": [
    "### 2.3 decoder layer\n",
    "注意 在构建decoder layer 的时候 一个是每个decoder layer都有接受来自于encoder的输出 实际上是k和v 其次是decoder当中会有mask的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head = n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "        # decoder 当中还会涉及到一个attention encoder和decoder的交叉注意力机制 使用的是encoder的K和V decoder的Q 但实际上指的就是encoder的输出经过俩个线性变化后成了K和V\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p = drop_prob)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        # 1. 先计算self attention 并保留short cut\n",
    "        _x = dec\n",
    "        # 2. 这里的t_mask 是下三角矩阵 decoder的目的是预测next token 所以要屏蔽下一个词\n",
    "        x = self.self_attention(q = dec, k = dec, v = dec, mask = t_mask)\n",
    "\n",
    "        # 3. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 4. cross attention\n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q = x, k = enc, v = enc, mask = s_mask)\n",
    "\n",
    "            # add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. ffn\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231aaa0c",
   "metadata": {},
   "source": [
    "### 2.4 encoder\n",
    "许多个encoder layer组在一起形成encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b0695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(\n",
    "            d_model=d_model,\n",
    "            max_len=max_len,\n",
    "            vocab_size=enc_voc_size,\n",
    "            drop_prob=drop_prob,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_head,\n",
    "                    drop_prob=drop_prob\n",
    "                ) \n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15176a98",
   "metadata": {},
   "source": [
    "### 2.5 decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b04bfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 注意 decoder的embedding 层 和 encoder 的只有 vocab size 不一致\n",
    "        self.emb = TransformerEmbedding(\n",
    "            d_model=d_model,\n",
    "            max_len=max_len,\n",
    "            vocab_size=dec_voc_size,\n",
    "            drop_prob=drop_prob,\n",
    "            device=device\n",
    "        )\n",
    "                \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_head,\n",
    "                    drop_prob=drop_prob\n",
    "                ) \n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        # 注意 线性层输出是dec的词汇表大小 实际上是每一个字 属于词表当中每个字的概率\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        # 对于decoder每一层来说 都有 encoder 的输入到cross attention当中\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        out = self.linear(trg)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddeb375",
   "metadata": {},
   "source": [
    "### 2.6 构建transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678f46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len, ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        # 三个mask对应着 三个attention的计算： encode self-attention decode self attention计算 cross attention 计算\n",
    "        # 先生成src的mask 用于src的attention计算\n",
    "        src_mask  = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        # 注意这里是trg在前 因为用的是trg的q\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "        # 这是将pad 和 下三角矩阵融合 因为一方面要找出pad 另一方面要掩盖下一个词\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * self.make_no_peak_mask(trg, trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        # 得到q, k 的长度\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # 此时输入的qk都是input_id 也就是根据词表转化过来的，会有pad\n",
    "        # 这一步就是根据pad 生成mask 因为pad是不进行attention的计算的\n",
    "        # 如果等于pad 那么 就会返回0 则 不是pad的位置都是1\n",
    "        # 所以生成的k的形状是(batch, 1, 1, len_k)\n",
    "        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "        # & 是位与操作 只有当两位上都是1的时候才是1\n",
    "        mask = (k & q).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # torch.tril 是用来形成下三角矩阵, 详见：https://blog.csdn.net/qq_38406029/article/details/122059507\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e34bb",
   "metadata": {},
   "source": [
    "### 3. 调试transformer数据流\n",
    "参考资料：https://blog.csdn.net/zhaohongfei_358/article/details/125858248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d33357",
   "metadata": {},
   "source": [
    "#### 3.1 创建transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c158403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1158792/1787143414.py:25: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
       "      (position_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): scaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (concat_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionWiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): TokenEmbedding(7853, 512, padding_idx=1)\n",
       "      (position_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): scaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (concat_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): scaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (concat_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionWiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=7853, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 2\n",
    "ffn_hidden = 2048\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "drop_prob = 0.1\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "# 使用kaiming_uniform对model初始化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070d8ed",
   "metadata": {},
   "source": [
    "#### 3.2 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13ad8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape:  torch.Size([128, 27])\n",
      "trg shape:  torch.Size([128, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src = torch.load('tensor_src.pt').to(device)\n",
    "trg = torch.load('tensor_trg.pt').to(device)\n",
    "print('src shape: ', src.shape)\n",
    "print('trg shape: ', trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db647b16",
   "metadata": {},
   "source": [
    "#### 3.3 创建mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685976f",
   "metadata": {},
   "source": [
    "mask是为了在计算attention的时候将pad的位置的score去除掉 这样不会影响 z 的计算。一般用一个非常大的负数来表示\n",
    "\n",
    "mask的过程实际上是模拟了attention计算的过程 即`q@k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb757578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask:  torch.Size([128, 1, 27, 27])\n",
      "src_trg_mask:  torch.Size([128, 1, 28, 27])\n",
      "trg_mask:  torch.Size([128, 1, 28, 28])\n",
      "src:  tensor([  2,  48,  53, 127,  36,  71,  18,  11,   8,   4, 268,   5,   3,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "src mask:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
      "==============================================================split line==============================================================\n",
      "src_trg_mask:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
      "==============================================================split line==============================================================\n",
      "trg:  tensor([  2,  43, 103,  80,  52,  47,  10,  12,   6, 320,   4,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "trg mask:  tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "src_mask = model.make_pad_mask(src, src, src_pad_idx, src_pad_idx)\n",
    "src_trg_mask = model.make_pad_mask(trg, src, trg_pad_idx, src_pad_idx)\n",
    "trg_mask = model.make_pad_mask(trg, trg, trg_pad_idx, trg_pad_idx) * model.make_no_peak_mask(trg, trg)\n",
    "print('src_mask: ', src_mask.shape)\n",
    "print('src_trg_mask: ', src_trg_mask.shape)\n",
    "print('trg_mask: ', trg_mask.shape)\n",
    "\n",
    "# 取出mask的值观察\n",
    "# mask 在计算qkscore后 会以mask_filled 的方式填入，所以形状上必须符合qkscore的情况\n",
    "# trg mask是个下三角矩阵 trg mask 取倒数第二维 就对应 trg中的一句话\n",
    "print('src: ', src[0])\n",
    "print('src mask: ', src_mask[0][0][0].int())\n",
    "print('==============================================================split line==============================================================')\n",
    "print('src_trg_mask: ', src_trg_mask[0][0][0].int())\n",
    "print('==============================================================split line==============================================================')\n",
    "print('trg: ', trg[0])\n",
    "print('trg mask: ', trg_mask[0][0][0].int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970ba57",
   "metadata": {},
   "source": [
    "#### 3.4 调试embedding\n",
    "input_id -> input embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52ad5892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb: \n",
      " TokenEmbedding(5893, 512, padding_idx=1)\n",
      "input_tok_emb:\n",
      " torch.Size([128, 27, 512])\n",
      "input_tok_emb[0]:\n",
      " tensor([ 0.0659, -0.0807,  0.0928,  0.0404, -0.0871,  0.0316,  0.0440, -0.0285,\n",
      "         0.0706, -0.0998], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "position_emb: \n",
      " PositionalEncoding()\n",
      "input_position_emb:\n",
      " torch.Size([27, 512])\n",
      "input_position_emb[0]:\n",
      " tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')\n",
      "input_emb: \n",
      " torch.Size([128, 27, 512])\n",
      "input_emb[0]:\n",
      " tensor([ 0.0659,  0.9193,  0.0928,  1.0404, -0.0871,  1.0316,  0.0440,  0.9715,\n",
      "         0.0706,  0.9002], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 提取两个embedding层\n",
    "tok_emb = model.encoder.emb.token_emb\n",
    "print('token_emb: \\n', tok_emb)\n",
    "input_tok_emb = tok_emb(src)\n",
    "print('input_tok_emb:\\n', input_tok_emb.shape)\n",
    "print('input_tok_emb[0]:\\n', input_tok_emb[0][0][:10])\n",
    "position_emb = model.encoder.emb.position_emb\n",
    "print('position_emb: \\n', position_emb)\n",
    "input_position_emb = position_emb(src)\n",
    "# 这里对positional embedding 做了一次广播机制\n",
    "print('input_position_emb:\\n', input_position_emb.shape)\n",
    "print('input_position_emb[0]:\\n', input_position_emb[0][:10])\n",
    "input_emb = input_tok_emb + input_position_emb\n",
    "print('input_emb: \\n', input_emb.shape)\n",
    "print('input_emb[0]:\\n', input_emb[0][0][:10])\n",
    "input_emb = model.encoder.emb.drop_out(input_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb7ebe",
   "metadata": {},
   "source": [
    "#### 3.5 调试encoder\n",
    "input embedding -> encoder layers -> output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e7c0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_emb shape: \n",
      " torch.Size([128, 27, 512])\n",
      "input_emb: \n",
      " tensor([ 0.0732,  1.0215,  0.1031,  1.1560, -0.0968,  1.1462,  0.0489,  1.0794,\n",
      "         0.0785,  1.0002], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "q: \n",
      " tensor([-0.6794, -1.6800,  2.2284, -0.2421, -0.5112,  0.0564, -1.5653, -2.3242,\n",
      "        -0.1343, -1.2660], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "k: \n",
      " tensor([-0.7530, -1.3307,  1.3520, -0.7096, -0.8103,  0.6370,  0.3142,  1.0733,\n",
      "         1.8861, -3.4619], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "v: \n",
      " tensor([ 1.1294, -0.3531,  0.3073, -1.6134,  0.2602,  0.1717,  2.8820, -1.1430,\n",
      "         1.6829,  1.6666], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "multi-head q: \n",
      " torch.Size([128, 8, 27, 64])\n",
      "attention temp shape: \n",
      " torch.Size([128, 8, 27, 27])\n",
      "attention temp: \n",
      " tensor([ 6.2526,  7.1083, 10.3776, 11.2454, 13.3260,  4.1047,  6.2703,  7.6537,\n",
      "        11.2654, 13.0507], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 8, 27, 27])\n",
      "score: \n",
      " tensor([0.2763, 0.3141, 0.4586, 0.4970, 0.5889, 0.1814, 0.2771, 0.3383, 0.4979,\n",
      "        0.5768], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "masked score shape: \n",
      " torch.Size([128, 8, 27, 27])\n",
      "masked score: \n",
      " tensor([ 2.7633e-01,  3.1415e-01,  4.5863e-01,  4.9698e-01,  5.8893e-01,\n",
      "         1.8140e-01,  2.7711e-01,  3.3825e-01,  4.9786e-01,  5.7677e-01,\n",
      "         4.6950e-01,  3.9740e-01,  3.7152e-02, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 8, 27, 27])\n",
      "score: \n",
      " tensor([0.0687, 0.0714, 0.0824, 0.0857, 0.0939, 0.0625, 0.0688, 0.0731, 0.0857,\n",
      "        0.0928, 0.0834, 0.0776, 0.0541, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 8, 27, 64])\n",
      "score: \n",
      " tensor([-0.2000, -0.3785, -0.0549, -0.3762,  0.0300,  0.1583,  2.1810, -0.7454,\n",
      "         0.4050,  0.6607], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 27, 512])\n",
      "score: \n",
      " tensor([-0.2000, -0.1887, -0.1836, -0.1885, -0.1739, -0.1996, -0.1879, -0.1786,\n",
      "        -0.1773, -0.1852], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 27, 512])\n",
      "score: \n",
      " tensor([ 0.8956,  0.8686,  0.6087,  1.8087, -0.8066, -0.3585,  1.1016, -1.2058,\n",
      "        -0.8836,  0.0814], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "可以发现输入的input emb的形状和计算完attention后的形状是一致的\n"
     ]
    }
   ],
   "source": [
    "# 获取encoder当中的一个层 调试计算流程\n",
    "encoder_layer = model.encoder.layers[0]\n",
    "\n",
    "# 1. 先保留short cut 再经过attention 计算\n",
    "    # short cut\n",
    "_input_emb = input_emb\n",
    "    # 注意力机制\n",
    "    # 通过linear层 得到qkv\n",
    "w_q = encoder_layer.attention.w_q\n",
    "w_k = encoder_layer.attention.w_k\n",
    "w_v = encoder_layer.attention.w_v\n",
    "\n",
    "    # 输入input embedding，分别得到qkv\n",
    "q = w_q(input_emb)\n",
    "k = w_k(input_emb)\n",
    "v = w_v(input_emb)\n",
    "\n",
    "    # 以上就是经过线性变换 使得qkv不同\n",
    "print('input_emb shape: \\n', input_emb.shape)\n",
    "print('input_emb: \\n', input_emb[0][0][:10])\n",
    "print('q: \\n', q[0][0][:10])\n",
    "print('k: \\n', k[0][0][:10])\n",
    "print('v: \\n', v[0][0][:10])\n",
    "\n",
    "    # 然后我们需要将qkv拆分成多头, 这里因为qkv形状相等 所以以q为例\n",
    "batch_size, seq_len, d_model = q.size()\n",
    "    # 这里要注意是否能被整除\n",
    "try:\n",
    "    d_tensor = d_model // n_heads\n",
    "except Exception as e:\n",
    "    print('Wrong when split heads: \\n', e)\n",
    "    # batch_size, seq_len, d_model -> batch_size, n_heads, seq_len, d_tensor\n",
    "q = q.view(batch_size, seq_len, n_heads, d_tensor).transpose(1, 2)\n",
    "k = k.view(batch_size, seq_len, n_heads, d_tensor).transpose(1, 2)\n",
    "v = v.view(batch_size, seq_len, n_heads, d_tensor).transpose(1, 2)\n",
    "\n",
    "print('multi-head q: \\n', q.shape)\n",
    "\n",
    "    # self-attention计算\n",
    "temp = q @ k.transpose(2,3)\n",
    "print('attention temp shape: \\n', temp.shape)\n",
    "print('attention temp: \\n', temp[0][0][0][:10])\n",
    "    # scale 操作\n",
    "score = temp / math.sqrt(d_model)\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][0][:10])\n",
    "    # mask 操作 目的是为了mask掉其中padding的部分\n",
    "score = score.masked_fill(src_mask == 0, -1e10)\n",
    "print('masked score shape: \\n', score.shape)\n",
    "print('masked score: \\n', score[0][0][0])\n",
    "    # softmax 可以观察到 mask后的地方都变成了0 \n",
    "    # 关于softmax：https://openatomworkshop.csdn.net/66470371b12a9d168eb6e9c9.html\n",
    "score = nn.Softmax(dim=-1)(score)\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][0])\n",
    "    # 计算v\n",
    "score = score @ v\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][0][:10])\n",
    "    # 将多头再拼接回来\n",
    "batch_size, n_head, seq_len, d_tensor = score.size()\n",
    "d_model = n_head * d_tensor\n",
    "score = score.transpose(2, 3).contiguous().view(batch_size, seq_len, d_model)\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][:10])\n",
    "# multi-head attention 和 self-attention 不同的地方在于 需要在输出的地方再乘以一个权重\n",
    "score = encoder_layer.attention.concat_linear(score)\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][:10])\n",
    "print('可以发现输入的input emb的形状和计算完attention后的形状是一致的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "483ef42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean shape: \n",
      " torch.Size([128, 27, 1])\n",
      "mean: \n",
      " tensor(0.4363, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "var shape: \n",
      " torch.Size([128, 27, 1])\n",
      "var: \n",
      " tensor(1.9876, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: \n",
      " torch.Size([128, 27, 512])\n",
      "out: \n",
      " tensor(0.4483, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 27, 512])\n",
      "score: \n",
      " tensor(0.4483, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. add and norm\n",
    "score = encoder_layer.dropout1(score)\n",
    "    # 1) add\n",
    "score = score + _input_emb\n",
    "    # 2) layerNorm: 可以看小冬瓜AIGC小红书有详细解释\n",
    "    # 定义参数 这些参数都是可以训练的\n",
    "gamma = nn.Parameter(torch.ones(d_model)).to(device)\n",
    "beta = nn.Parameter(torch.zeros(d_model)).to(device)\n",
    "eps = 1e-12\n",
    "    # 在特征维度上计算均值\n",
    "mean = score.mean(dim=-1, keepdim=True)\n",
    "print('mean shape: \\n', mean.shape)\n",
    "print('mean: \\n', mean[0][0][0])\n",
    "    # 在特征维度上计算方差\n",
    "var = score.var(dim = -1, keepdim=True, unbiased = False)\n",
    "print('var shape: \\n', var.shape)\n",
    "print('var: \\n', var[0][0][0])\n",
    "    # 计算统计学上的归一化\n",
    "out = (score - mean) / torch.sqrt(var + eps)\n",
    "print('out shape: \\n', out.shape)\n",
    "print('out: \\n', out[0][0][0])\n",
    "score = gamma * out + beta\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91bc6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score shape: \n",
      " torch.Size([128, 27, 512])\n",
      "score: \n",
      " tensor(-0.2670, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ffn\n",
    "# 先来一次short cut\n",
    "_score = score\n",
    "\n",
    "# 经过前向传播 比较简单 不再调试到每一步\n",
    "score = encoder_layer.ffn(score)\n",
    "score = encoder_layer.dropout2(score)\n",
    "\n",
    "# 再进行一次 layerNorm\n",
    "score = encoder_layer.norm2( _score + score)\n",
    "print('score shape: \\n', score.shape)\n",
    "print('score: \\n', score[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9c296",
   "metadata": {},
   "source": [
    "#### 3.6 调试decoder\n",
    "decoder操作稍微复杂一点 主要是经过两次attention 一次是self-attention 另外一次是交叉注意力机制 是和encoder一起的\n",
    "\n",
    "decoder的self-attention不再赘述 操作方式和encoder差不多 这里会详细debug 交叉注意力层\n",
    "\n",
    "前面我们已经获取到了encoder 的输出 score， decoder 的注意力分数我们用 x 来代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e16ad4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trg_emb shape: \n",
      " torch.Size([128, 28, 512])\n",
      "trg_emb: \n",
      " tensor(-0.0381, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "x shape: \n",
      " torch.Size([128, 28, 512])\n",
      "x: \n",
      " tensor([ 0.8951,  1.6015,  0.8205, -0.8543, -1.7190, -1.9628, -4.4652,  0.7496,\n",
      "        -0.0703, -1.4640], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "multi-head q: \n",
      " torch.Size([128, 8, 28, 64])\n",
      "multi-head k: \n",
      " torch.Size([128, 8, 27, 64])\n",
      "multi-head v: \n",
      " torch.Size([128, 8, 27, 64])\n",
      "attention temp shape: \n",
      " torch.Size([128, 8, 28, 27])\n",
      "attention temp: \n",
      " tensor([ -4.3989,  -1.7746,  -2.8873, -14.5719,  -0.9342,   5.7105, -16.0071,\n",
      "        -12.5798, -16.5159,   3.6476], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "scaled score shape: \n",
      " torch.Size([128, 8, 28, 27])\n",
      "scaled score: \n",
      " tensor([-0.1944, -0.0784, -0.1276, -0.6440, -0.0413,  0.2524, -0.7074, -0.5560,\n",
      "        -0.7299,  0.1612], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "masked score shape: \n",
      " torch.Size([128, 8, 28, 27])\n",
      "masked score: \n",
      " tensor([-1.9441e-01, -7.8429e-02, -1.2760e-01, -6.4399e-01, -4.1286e-02,\n",
      "         2.5237e-01, -7.0742e-01, -5.5595e-01, -7.2991e-01,  1.6120e-01,\n",
      "         1.4866e+00, -1.0636e+00,  4.8963e-01, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
      "        -1.0000e+10, -1.0000e+10], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "softmax score shape: \n",
      " torch.Size([128, 8, 28, 27])\n",
      "softmax score: \n",
      " tensor([0.0567, 0.0637, 0.0606, 0.0362, 0.0661, 0.0886, 0.0339, 0.0395, 0.0332,\n",
      "        0.0809, 0.3045, 0.0238, 0.1124, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "output score shape: \n",
      " torch.Size([128, 8, 28, 64])\n",
      "output score: \n",
      " tensor([ 0.1850,  0.2872,  0.8855,  0.1362, -0.8842, -0.4326, -0.1744,  1.1390,\n",
      "         0.5525, -1.1599], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "concat score shape: \n",
      " torch.Size([128, 28, 512])\n",
      "concat score: \n",
      " tensor([0.1850, 0.2662, 0.2309, 0.2115, 0.2467, 0.2234, 0.1957, 0.2742, 0.3354,\n",
      "        0.3999], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "score shape: \n",
      " torch.Size([128, 28, 512])\n",
      "score: \n",
      " tensor([ 1.8792,  0.0680,  0.5090, -0.3326, -0.6612,  0.2561,  1.0289,  0.1449,\n",
      "        -1.9737,  0.5408], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "可以发现输入的input emb的形状和计算完attention后的形状是一致的\n",
      "需要注意交叉注意力计算的时候 矩阵大小的变化\n"
     ]
    }
   ],
   "source": [
    "# 先对trg进行位置编码 这些和encoder操作都是一致的\n",
    "# note： encoder的输出是score\n",
    "trg_emb = model.decoder.emb(trg)\n",
    "print('trg_emb shape: \\n', trg_emb.shape)\n",
    "print('trg_emb: \\n', trg_emb[0][0][0])\n",
    "decoder_layer = model.decoder.layers[0]\n",
    "\n",
    "# 先进行short cut 换个名字\n",
    "_x = trg_emb\n",
    "x = trg_emb\n",
    "# 计算self-attention\n",
    "x = decoder_layer.self_attention(q = x, k = x, v = x, mask = trg_mask)\n",
    "print('x shape: \\n', x.shape)\n",
    "print('x: \\n', x[0][0][:10])\n",
    "x = decoder_layer.dropout1(x)\n",
    "x = decoder_layer.norm1(x + _x)\n",
    "\n",
    "# short cut第二次\n",
    "_x = x\n",
    "# 进行交叉注意力的计算 注意：attention的计算方式还是一致的 不同的是qkv和mask\n",
    "    # 通过linear层 得到qkv\n",
    "w_q = decoder_layer.enc_dec_attention.w_q\n",
    "w_k = decoder_layer.enc_dec_attention.w_k\n",
    "w_v = decoder_layer.enc_dec_attention.w_v\n",
    "\n",
    "    # 我们用decoder的q 来查询 encoder的k和v\n",
    "q = w_q(x)\n",
    "k = w_k(score)\n",
    "v = w_v(score)\n",
    "\n",
    "    # 拆分成多头，由于前面debug过详细内容 这里不再赘述\n",
    "split = decoder_layer.enc_dec_attention.split\n",
    "q, k, v = split(q), split(k), split(v)\n",
    "print('multi-head q: \\n', q.shape)\n",
    "print('multi-head k: \\n', k.shape)\n",
    "print('multi-head v: \\n', v.shape)\n",
    "\n",
    "    # 注意这里的q和kv的形状是不同的\n",
    "    # self-attention计算\n",
    "temp = q @ k.transpose(2,3)\n",
    "print('attention temp shape: \\n', temp.shape)\n",
    "print('attention temp: \\n', temp[0][0][0][:10])\n",
    "    # scale 操作\n",
    "x = temp / math.sqrt(d_model)\n",
    "print('scaled score shape: \\n', x.shape)\n",
    "print('scaled score: \\n', x[0][0][0][:10])\n",
    "    # mask 操作 目的是为了mask掉其中padding的部分\n",
    "x = x.masked_fill(src_trg_mask == 0, -1e10)\n",
    "print('masked score shape: \\n', x.shape)\n",
    "print('masked score: \\n', x[0][0][0])\n",
    "    # softmax 可以观察到 mask后的地方都变成了0 \n",
    "    # 关于softmax：https://openatomworkshop.csdn.net/66470371b12a9d168eb6e9c9.html\n",
    "x = nn.Softmax(dim=-1)(x)\n",
    "print('softmax score shape: \\n', x.shape)\n",
    "print('softmax score: \\n', x[0][0][0])\n",
    "    # 计算v\n",
    "x = x @ v\n",
    "print('output score shape: \\n', x.shape)\n",
    "print('output score: \\n', x[0][0][0][:10])\n",
    "    # 将多头再拼接回来\n",
    "batch_size, n_head, seq_len, d_tensor = x.size()\n",
    "d_model = n_head * d_tensor\n",
    "x = x.transpose(2, 3).contiguous().view(batch_size, seq_len, d_model)\n",
    "print('concat score shape: \\n', x.shape)\n",
    "print('concat score: \\n', x[0][0][:10])\n",
    "# multi-head attention 和 self-attention 不同的地方在于 需要在输出的地方再乘以一个权重\n",
    "x = decoder_layer.enc_dec_attention.concat_linear(x)\n",
    "print('score shape: \\n', x.shape)\n",
    "print('score: \\n', x[0][0][:10])\n",
    "print('可以发现输入的input emb的形状和计算完attention后的形状是一致的')\n",
    "print('需要注意交叉注意力计算的时候 矩阵大小的变化')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4edbe77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer output shape: \n",
      " torch.Size([128, 28, 512])\n",
      "transformer output: \n",
      " tensor(0.7944, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "decoder trg input shape: \n",
      " torch.Size([128, 28, 512])\n",
      "decoder trg input: \n",
      " tensor(-0.0381, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 接下来就是add and norm 和ffn了\n",
    "x = decoder_layer.dropout2(x)\n",
    "x = decoder_layer.norm2(x + _x)\n",
    "\n",
    "_x = x\n",
    "x = decoder_layer.ffn(x)\n",
    "\n",
    "x = decoder_layer.dropout3(x)\n",
    "x = decoder_layer.norm3(x + _x)\n",
    "# 此时得到的x 就是transformer模型的输出\n",
    "print('transformer output shape: \\n', x.shape)\n",
    "print('transformer output: \\n', x[0][0][0])\n",
    "# 比较一下 和 输入的 差别\n",
    "print('decoder trg input shape: \\n', trg_emb.shape)\n",
    "print('decoder trg input: \\n', trg_emb[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02c32d",
   "metadata": {},
   "source": [
    "### 4. transformer 训练和推理流程\n",
    "模型在训练和推理时候的流程会有所区别 训练会一次性输入trg 但是 推理 是next token generation的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77277e41",
   "metadata": {},
   "source": [
    "#### 4.1 loss\n",
    "训练的时候需要注意损失函数的计算， 尤其要注意input output和label的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f37af329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src input shape: \n",
      " torch.Size([128, 27])\n",
      "src input: \n",
      " tensor([  2,  48,  53, 127,  36,  71,  18,  11,   8,   4, 268,   5,   3,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "发现 一句话 有2 作为 <SOS>; 3 作为 <EOS>; 1 作为 padding\n",
      "trg input shape: \n",
      " torch.Size([128, 28])\n",
      "trg input: \n",
      " tensor([  2,  43, 103,  80,  52,  47,  10,  12,   6, 320,   4,   3,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "       device='cuda:0')\n",
      "发现 一句话 有2 作为 <SOS>; 3 作为 <EOS>; 1 作为 padding\n",
      "model output shape: \n",
      " torch.Size([128, 27, 7853])\n",
      "model output_reshape shape: \n",
      " torch.Size([3456, 7853])\n",
      "model output_reshape: \n",
      " tensor([ 1.1452, -0.8259,  2.2421,  ..., -1.7233, -0.0876,  1.8815],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "trg_reshape shape: \n",
      " torch.Size([3456])\n",
      "trg_reshape: \n",
      " tensor(43, device='cuda:0')\n",
      "loss: \n",
      " tensor(10.0268, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1158792/425991949.py:35: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "# 先计算loss\n",
    "# 看一下原始的输入和输出\n",
    "print('src input shape: \\n', src.shape)\n",
    "# 取出其中的一句话 观察如何被tokenize\n",
    "print('src input: \\n', src[0])\n",
    "print('发现 一句话 有2 作为 <SOS>; 3 作为 <EOS>; 1 作为 padding')\n",
    "\n",
    "print('trg input shape: \\n', trg.shape)\n",
    "# 取出其中的一句话 观察如何被tokenize\n",
    "print('trg input: \\n', trg[0])\n",
    "print('发现 一句话 有2 作为 <SOS>; 3 作为 <EOS>; 1 作为 padding')\n",
    "\n",
    "# 在做decoder模型输入的时候，要将输入的序列shift right \n",
    "# 因为当decoder最后一个词输入时，直接输出eos，不需要decoder的eos\n",
    "\n",
    "trg_right = trg[:, :-1]\n",
    "\n",
    "# 实例化一个模型\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "# 使用kaiming_uniform对model初始化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)\n",
    "\n",
    "output = model(src, trg_right)\n",
    "print('model output shape: \\n', output.shape)\n",
    "\n",
    "output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "print('model output_reshape shape: \\n', output_reshape.shape)\n",
    "print('model output_reshape: \\n', output_reshape[0])\n",
    "# 这里处理groud truth的时候 注意是取消了sos这个token \n",
    "# 原因是要让decoder输入的 sos 对应上下一个词是 groud truth 句子的开头\n",
    "trg_reshape = trg[:, 1:].contiguous().view(-1)\n",
    "print('trg_reshape shape: \\n', trg_reshape.shape)\n",
    "print('trg_reshape: \\n', trg_reshape[0])\n",
    "# 需要注意的是 我们的 trg_reshape 是 indices 可以输入到pytorch的交叉熵损失函数中自动的one_hot\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "# 实例化一个损失函数,在pad的地方不需要计算损失\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "loss = criterion(output_reshape, trg_reshape)\n",
    "print('loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832daf2",
   "metadata": {},
   "source": [
    "#### 4.2 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "106b4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd23/miniconda3/envs/rbp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eopch: 0, step: 0.0%, step_loss: 10.056924819946289, epoch_loss: 10.056924819946289\n",
      "eopch: 1, step: 0.0%, step_loss: 9.769620895385742, epoch_loss: 9.769620895385742\n",
      "eopch: 2, step: 0.0%, step_loss: 9.516295433044434, epoch_loss: 9.516295433044434\n",
      "eopch: 3, step: 0.0%, step_loss: 9.316094398498535, epoch_loss: 9.316094398498535\n",
      "eopch: 4, step: 0.0%, step_loss: 9.120285987854004, epoch_loss: 9.120285987854004\n",
      "eopch: 5, step: 0.0%, step_loss: 8.998662948608398, epoch_loss: 8.998662948608398\n",
      "eopch: 6, step: 0.0%, step_loss: 8.820260047912598, epoch_loss: 8.820260047912598\n",
      "eopch: 7, step: 0.0%, step_loss: 8.691749572753906, epoch_loss: 8.691749572753906\n",
      "eopch: 8, step: 0.0%, step_loss: 8.572705268859863, epoch_loss: 8.572705268859863\n",
      "eopch: 9, step: 0.0%, step_loss: 8.440896034240723, epoch_loss: 8.440896034240723\n",
      "eopch: 10, step: 0.0%, step_loss: 8.35954761505127, epoch_loss: 8.35954761505127\n",
      "eopch: 11, step: 0.0%, step_loss: 8.260976791381836, epoch_loss: 8.260976791381836\n",
      "eopch: 12, step: 0.0%, step_loss: 8.172147750854492, epoch_loss: 8.172147750854492\n",
      "eopch: 13, step: 0.0%, step_loss: 8.076606750488281, epoch_loss: 8.076606750488281\n",
      "eopch: 14, step: 0.0%, step_loss: 7.9875006675720215, epoch_loss: 7.9875006675720215\n",
      "eopch: 15, step: 0.0%, step_loss: 7.899047374725342, epoch_loss: 7.899047374725342\n",
      "eopch: 16, step: 0.0%, step_loss: 7.843092441558838, epoch_loss: 7.843092441558838\n",
      "eopch: 17, step: 0.0%, step_loss: 7.7602763175964355, epoch_loss: 7.7602763175964355\n",
      "eopch: 18, step: 0.0%, step_loss: 7.706340312957764, epoch_loss: 7.706340312957764\n",
      "eopch: 19, step: 0.0%, step_loss: 7.657888889312744, epoch_loss: 7.657888889312744\n",
      "eopch: 20, step: 0.0%, step_loss: 7.614659786224365, epoch_loss: 7.614659786224365\n",
      "eopch: 21, step: 0.0%, step_loss: 7.529460906982422, epoch_loss: 7.529460906982422\n",
      "eopch: 22, step: 0.0%, step_loss: 7.489035129547119, epoch_loss: 7.489035129547119\n",
      "eopch: 23, step: 0.0%, step_loss: 7.437074661254883, epoch_loss: 7.437074661254883\n",
      "eopch: 24, step: 0.0%, step_loss: 7.406313419342041, epoch_loss: 7.406313419342041\n",
      "eopch: 25, step: 0.0%, step_loss: 7.351255893707275, epoch_loss: 7.351255893707275\n",
      "eopch: 26, step: 0.0%, step_loss: 7.321753025054932, epoch_loss: 7.321753025054932\n",
      "eopch: 27, step: 0.0%, step_loss: 7.290119647979736, epoch_loss: 7.290119647979736\n",
      "eopch: 28, step: 0.0%, step_loss: 7.263693332672119, epoch_loss: 7.263693332672119\n",
      "eopch: 29, step: 0.0%, step_loss: 7.229290008544922, epoch_loss: 7.229290008544922\n",
      "eopch: 30, step: 0.0%, step_loss: 7.201642036437988, epoch_loss: 7.201642036437988\n",
      "eopch: 31, step: 0.0%, step_loss: 7.152595520019531, epoch_loss: 7.152595520019531\n",
      "eopch: 32, step: 0.0%, step_loss: 7.155083656311035, epoch_loss: 7.155083656311035\n",
      "eopch: 33, step: 0.0%, step_loss: 7.126664161682129, epoch_loss: 7.126664161682129\n",
      "eopch: 34, step: 0.0%, step_loss: 7.088400363922119, epoch_loss: 7.088400363922119\n",
      "eopch: 35, step: 0.0%, step_loss: 7.0821332931518555, epoch_loss: 7.0821332931518555\n",
      "eopch: 36, step: 0.0%, step_loss: 7.04552698135376, epoch_loss: 7.04552698135376\n",
      "eopch: 37, step: 0.0%, step_loss: 7.0297651290893555, epoch_loss: 7.0297651290893555\n",
      "eopch: 38, step: 0.0%, step_loss: 7.010075569152832, epoch_loss: 7.010075569152832\n",
      "eopch: 39, step: 0.0%, step_loss: 6.99710750579834, epoch_loss: 6.99710750579834\n",
      "eopch: 40, step: 0.0%, step_loss: 6.934561252593994, epoch_loss: 6.934561252593994\n",
      "eopch: 41, step: 0.0%, step_loss: 6.9114203453063965, epoch_loss: 6.9114203453063965\n",
      "eopch: 42, step: 0.0%, step_loss: 6.905069828033447, epoch_loss: 6.905069828033447\n",
      "eopch: 43, step: 0.0%, step_loss: 6.898491859436035, epoch_loss: 6.898491859436035\n",
      "eopch: 44, step: 0.0%, step_loss: 6.882463455200195, epoch_loss: 6.882463455200195\n",
      "eopch: 45, step: 0.0%, step_loss: 6.8558220863342285, epoch_loss: 6.8558220863342285\n",
      "eopch: 46, step: 0.0%, step_loss: 6.852156639099121, epoch_loss: 6.852156639099121\n",
      "eopch: 47, step: 0.0%, step_loss: 6.837455749511719, epoch_loss: 6.837455749511719\n",
      "eopch: 48, step: 0.0%, step_loss: 6.8277716636657715, epoch_loss: 6.8277716636657715\n",
      "eopch: 49, step: 0.0%, step_loss: 6.817444801330566, epoch_loss: 6.817444801330566\n",
      "eopch: 50, step: 0.0%, step_loss: 6.792789936065674, epoch_loss: 6.792789936065674\n",
      "eopch: 51, step: 0.0%, step_loss: 6.737534999847412, epoch_loss: 6.737534999847412\n",
      "eopch: 52, step: 0.0%, step_loss: 6.744186878204346, epoch_loss: 6.744186878204346\n",
      "eopch: 53, step: 0.0%, step_loss: 6.750847339630127, epoch_loss: 6.750847339630127\n",
      "eopch: 54, step: 0.0%, step_loss: 6.71820068359375, epoch_loss: 6.71820068359375\n",
      "eopch: 55, step: 0.0%, step_loss: 6.689799785614014, epoch_loss: 6.689799785614014\n",
      "eopch: 56, step: 0.0%, step_loss: 6.699553966522217, epoch_loss: 6.699553966522217\n",
      "eopch: 57, step: 0.0%, step_loss: 6.658895969390869, epoch_loss: 6.658895969390869\n",
      "eopch: 58, step: 0.0%, step_loss: 6.671361446380615, epoch_loss: 6.671361446380615\n",
      "eopch: 59, step: 0.0%, step_loss: 6.64689302444458, epoch_loss: 6.64689302444458\n",
      "eopch: 60, step: 0.0%, step_loss: 6.6299943923950195, epoch_loss: 6.6299943923950195\n",
      "eopch: 61, step: 0.0%, step_loss: 6.6454758644104, epoch_loss: 6.6454758644104\n",
      "eopch: 62, step: 0.0%, step_loss: 6.631036281585693, epoch_loss: 6.631036281585693\n",
      "eopch: 63, step: 0.0%, step_loss: 6.609006881713867, epoch_loss: 6.609006881713867\n",
      "eopch: 64, step: 0.0%, step_loss: 6.576799392700195, epoch_loss: 6.576799392700195\n",
      "eopch: 65, step: 0.0%, step_loss: 6.56848669052124, epoch_loss: 6.56848669052124\n",
      "eopch: 66, step: 0.0%, step_loss: 6.5772600173950195, epoch_loss: 6.5772600173950195\n",
      "eopch: 67, step: 0.0%, step_loss: 6.558990478515625, epoch_loss: 6.558990478515625\n",
      "eopch: 68, step: 0.0%, step_loss: 6.549620151519775, epoch_loss: 6.549620151519775\n",
      "eopch: 69, step: 0.0%, step_loss: 6.533194065093994, epoch_loss: 6.533194065093994\n",
      "eopch: 70, step: 0.0%, step_loss: 6.541171073913574, epoch_loss: 6.541171073913574\n",
      "eopch: 71, step: 0.0%, step_loss: 6.539423942565918, epoch_loss: 6.539423942565918\n",
      "eopch: 72, step: 0.0%, step_loss: 6.513647556304932, epoch_loss: 6.513647556304932\n",
      "eopch: 73, step: 0.0%, step_loss: 6.4932732582092285, epoch_loss: 6.4932732582092285\n",
      "eopch: 74, step: 0.0%, step_loss: 6.485866069793701, epoch_loss: 6.485866069793701\n",
      "eopch: 75, step: 0.0%, step_loss: 6.463687419891357, epoch_loss: 6.463687419891357\n",
      "eopch: 76, step: 0.0%, step_loss: 6.482407093048096, epoch_loss: 6.482407093048096\n",
      "eopch: 77, step: 0.0%, step_loss: 6.456964015960693, epoch_loss: 6.456964015960693\n",
      "eopch: 78, step: 0.0%, step_loss: 6.4683146476745605, epoch_loss: 6.4683146476745605\n",
      "eopch: 79, step: 0.0%, step_loss: 6.457245349884033, epoch_loss: 6.457245349884033\n",
      "eopch: 80, step: 0.0%, step_loss: 6.464008808135986, epoch_loss: 6.464008808135986\n",
      "eopch: 81, step: 0.0%, step_loss: 6.420821666717529, epoch_loss: 6.420821666717529\n",
      "eopch: 82, step: 0.0%, step_loss: 6.42288064956665, epoch_loss: 6.42288064956665\n",
      "eopch: 83, step: 0.0%, step_loss: 6.414280414581299, epoch_loss: 6.414280414581299\n",
      "eopch: 84, step: 0.0%, step_loss: 6.38810396194458, epoch_loss: 6.38810396194458\n",
      "eopch: 85, step: 0.0%, step_loss: 6.3729472160339355, epoch_loss: 6.3729472160339355\n",
      "eopch: 86, step: 0.0%, step_loss: 6.407520771026611, epoch_loss: 6.407520771026611\n",
      "eopch: 87, step: 0.0%, step_loss: 6.377339839935303, epoch_loss: 6.377339839935303\n",
      "eopch: 88, step: 0.0%, step_loss: 6.366939544677734, epoch_loss: 6.366939544677734\n",
      "eopch: 89, step: 0.0%, step_loss: 6.338624954223633, epoch_loss: 6.338624954223633\n",
      "eopch: 90, step: 0.0%, step_loss: 6.345094203948975, epoch_loss: 6.345094203948975\n",
      "eopch: 91, step: 0.0%, step_loss: 6.345406532287598, epoch_loss: 6.345406532287598\n",
      "eopch: 92, step: 0.0%, step_loss: 6.344268798828125, epoch_loss: 6.344268798828125\n",
      "eopch: 93, step: 0.0%, step_loss: 6.338995456695557, epoch_loss: 6.338995456695557\n",
      "eopch: 94, step: 0.0%, step_loss: 6.321384429931641, epoch_loss: 6.321384429931641\n",
      "eopch: 95, step: 0.0%, step_loss: 6.280725002288818, epoch_loss: 6.280725002288818\n",
      "eopch: 96, step: 0.0%, step_loss: 6.34371280670166, epoch_loss: 6.34371280670166\n",
      "eopch: 97, step: 0.0%, step_loss: 6.307822227478027, epoch_loss: 6.307822227478027\n",
      "eopch: 98, step: 0.0%, step_loss: 6.31854248046875, epoch_loss: 6.31854248046875\n",
      "eopch: 99, step: 0.0%, step_loss: 6.295070648193359, epoch_loss: 6.295070648193359\n"
     ]
    }
   ],
   "source": [
    "# 配置参数\n",
    "optimizer = Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=init_lr,\n",
    "    weight_decay=weight_decay,\n",
    "    eps=adam_eps\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    verbose=True,\n",
    "    factor=factor,\n",
    "    patience=patience\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "# 定义训练函数\n",
    "\n",
    "def train(model, epoch, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(iterator)):\n",
    "        src, trg = iterator[i]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg_reshape = trg[:, 1:].contiguous().reshape(-1)\n",
    "        loss = criterion(output_reshape, trg_reshape)\n",
    "        loss.backward()\n",
    "        # 这里的clip是为了防止梯度太大时 引起梯度爆炸使用的\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'eopch: {epoch}, step: {round((i/len(iterator)) * 100, 2)}%, step_loss: {loss}, epoch_loss: {epoch_loss / len(iterator)}')\n",
    "\n",
    "epoch_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    train_loss = train(model, epoch, [(src, trg)], optimizer, criterion, clip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
