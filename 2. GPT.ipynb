{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "This notebook mainly implements GPT from nanoGPT.\n",
    "\n",
    "\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. pacakge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GPT model implementation\n",
    "\n",
    "It need to be mentioned that the modules implemented in previous charpter will not be detailed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataclass staticmethod classmethod: https://blog.csdn.net/sjxgghg/article/details/139861829\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # the length of a sentence\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embed: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CausalSelfAttention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # if we meet n_embed can not divided by n_head, raise an error\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "        # projection for qkv, it will be splitted below.\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias = config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n",
    "        # settings\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention will be deployed if avaliable, but support is only in pytorch > 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print('WARNING: using slow attention. Flash Attention requires Pytorch >= 2.0')\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence.\n",
    "            # about register buffer: https://blog.csdn.net/dagouxiaohui/article/details/125649813\n",
    "            # set parameters will be stored to state_dict, but not updated in training.\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))).view(1,1,config.block_size, config.block_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # batch_size, seq_len, embed_dim\n",
    "        B, T, C = x.size() \n",
    "\n",
    "        # split q, k, v\n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim = 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # apply torch attn\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention: q@k_t / sqrt(length)\n",
    "            att = (q @ k.transpose(-1, -2)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim = -1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "## MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias= config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias = config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GPT BLOCK\n",
    "Then we compile all the modules to a single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config=config)\n",
    "        self.ln_2 = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.mlp = MLP(config=config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we apply pre-norm before inputting into the attention\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # check vocab_size and block_size\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        # actually decoder\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config=config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embed, bias=config.bias)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size,  bias = False)\n",
    "        # weight sharing between embedding layer and lm_head layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        # init all weights (embedding and lm_head)\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std= 0.02 / math.sqrt(2 * config.n_layer))\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        device = x.device\n",
    "        b, t = x.size()\n",
    "        # 检查句长是否超过限制\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        # generate embed and dropout\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # we only need to calculate the next token\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # in-place\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "    \n",
    "    def get_num_params(self, non_embedding = True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.GPT Debug\n",
    "这一章节主要对于GPT的数据流程进行debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device and config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 50304\n",
    "block_size = 1024\n",
    "n_embed = 128\n",
    "n_head = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x shape: \n",
      " torch.Size([16, 256])\n",
      "input x: \n",
      " tensor([ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1], device='cuda:0')\n",
      "input y shape: \n",
      " torch.Size([16, 256])\n",
      "input y: \n",
      " tensor([13, 52, 42,  1, 26, 43, 56, 53,  1, 61], device='cuda:0')\n",
      "We found that actuall y is left-shfited for next token prediction\n"
     ]
    }
   ],
   "source": [
    "# 1. load data\n",
    "x = torch.load('X.tensor').to(device)\n",
    "y_true = torch.load('Y.tensor').to(device)\n",
    "print('input x shape: \\n', x.shape)\n",
    "print('input x: \\n', x[1, :10])\n",
    "print('input y shape: \\n', y_true.shape)\n",
    "print('input y: \\n', y_true[1, :10])\n",
    "print('We found that actuall y is left-shfited for next token prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 16, seq length is 256\n",
      "tok_embed x shape: \n",
      " torch.Size([16, 256, 128])\n",
      "tok_embed x: \n",
      " tensor([ 0.3910,  1.5044, -0.1323, -0.6133, -0.8597, -0.0640, -0.9161, -0.8664,\n",
      "         0.3146, -0.7122], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "pos: \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
      "pos_embed x shape: \n",
      " torch.Size([256, 128])\n",
      "pos_embed x: \n",
      " tensor([-1.1637,  0.9122, -1.1423, -0.2367,  0.8832,  0.9979,  1.9996, -0.4778,\n",
      "         0.7623,  0.3977], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "embed out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "embed out: \n",
      " tensor([ 0.2437,  0.7061, -1.4267,  0.9523, -1.5703, -0.1430, -1.4796,  0.1415,\n",
      "        -0.2720, -1.2497], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "YOU may notice that the out passes through the dropout layer and its value is changed!\n",
      "MORE INFOR: https://blog.csdn.net/weixin_43953686/article/details/105978308\n",
      "dropout out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "dropout out: \n",
      " tensor([ 0.3047,  0.8827, -1.7833,  0.0000, -0.0000, -0.1788, -1.8495,  0.1769,\n",
      "        -0.3399, -1.5622], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. massenge passing through gpt block\n",
    "# b: batch_size, t: seq_len\n",
    "b, t = x.size()\n",
    "print(f'batch size is {b}, seq length is {t}')\n",
    "\n",
    "# 2.1 token and position embedding\n",
    "tok_embed = nn.Embedding(vocab_size, n_embed).to(device)\n",
    "tok_x = tok_embed(x)\n",
    "print('tok_embed x shape: \\n', tok_x.shape)\n",
    "print('tok_embed x: \\n', tok_x[0, 1, :10])\n",
    "pos = torch.arange(0, t, device=device, dtype=torch.long).to(device)\n",
    "print('pos: \\n', pos[:10])\n",
    "pos_embed = nn.Embedding(block_size, n_embed).to(device)\n",
    "pos_x = pos_embed(pos)\n",
    "print('pos_embed x shape: \\n', pos_x.shape)\n",
    "print('pos_embed x: \\n', pos_x[0, :10])\n",
    "# add and drop\n",
    "out = tok_x + pos_x\n",
    "print('embed out shape: \\n', out.shape)\n",
    "print('embed out: \\n', out[0, 1, :10])\n",
    "out = nn.Dropout(dropout)(out)\n",
    "print('YOU may notice that the out passes through the dropout layer and its value is changed!')\n",
    "print('MORE INFOR: https://blog.csdn.net/weixin_43953686/article/details/105978308')\n",
    "print('dropout out shape: \\n', out.shape)\n",
    "print('dropout out: \\n', out[0, 1, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "LayerNorm out: \n",
      " tensor([ 0.3690,  0.7618, -1.0496,  0.1621,  0.1621,  0.0406, -1.0946,  0.2822,\n",
      "        -0.0689, -0.8994], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "qkv shape: \n",
      " torch.Size([16, 256, 384])\n",
      "qkv: \n",
      " tensor([-0.2769,  0.3850,  0.1450,  0.3017,  0.2100,  0.0191, -0.3082, -0.4025,\n",
      "        -0.4228,  0.4773], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Split qkv shape: \n",
      " torch.Size([16, 256, 128])\n",
      "Split qkv: \n",
      " tensor([-0.2769,  0.3850,  0.1450,  0.3017,  0.2100,  0.0191, -0.3082, -0.4025,\n",
      "        -0.4228,  0.4773], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Multi-head q shape: \n",
      " torch.Size([16, 4, 256, 32])\n",
      "Multi-head q : \n",
      " tensor([-0.2769,  0.3850,  0.1450,  0.3017,  0.2100,  0.0191, -0.3082, -0.4025,\n",
      "        -0.4228,  0.4773], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "attn score shape: \n",
      " torch.Size([16, 4, 256, 256])\n",
      "attn score : \n",
      " tensor([-0.0238,  0.0788, -0.0131, -0.0140,  0.1106,  0.1915, -0.2432, -0.0502,\n",
      "        -0.0857, -0.0119], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "mask shape: \n",
      " torch.Size([1, 1, 1024, 1024])\n",
      "mask : \n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "masked attn shape: \n",
      " torch.Size([16, 4, 256, 256])\n",
      "masked attn : \n",
      " tensor([[ 0.1451,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.0238,  0.0788,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.1656,  0.1338, -0.0318,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0315,  0.0536,  0.0331,  0.1128,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.1667, -0.0236, -0.1550, -0.0931,  0.0608,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.0279, -0.0423,  0.1012, -0.2386,  0.0817, -0.0979,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.1556, -0.0053, -0.0651,  0.2478,  0.0135, -0.4457,  0.2005,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.0134, -0.3186,  0.3046,  0.1198, -0.0933, -0.0263,  0.4168,  0.0554,\n",
      "            -inf,    -inf],\n",
      "        [-0.0165,  0.0540,  0.0102, -0.0886,  0.1708, -0.0638, -0.0813, -0.1046,\n",
      "         -0.0765,    -inf],\n",
      "        [ 0.2964, -0.0541,  0.1016, -0.1009, -0.3529, -0.0010,  0.2366,  0.0255,\n",
      "         -0.1479, -0.3933]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "attn output shape: \n",
      " torch.Size([16, 4, 256, 32])\n",
      "attn output : \n",
      " tensor([[ 0.0493,  0.1931,  0.3707,  0.6034,  0.4418, -1.2721, -0.9076,  1.1355,\n",
      "          0.9257,  0.1348],\n",
      "        [ 0.0234,  0.0916,  0.1758,  0.2862,  0.2096, -0.6035, -0.4306,  0.5386,\n",
      "          0.4391,  0.0639],\n",
      "        [ 0.2802,  0.2819, -0.3054, -0.0444,  0.1939, -0.4347, -0.2125,  0.1343,\n",
      "          0.3180,  0.3567],\n",
      "        [ 0.0990,  1.0696, -0.4220,  0.2568,  0.1777, -0.5435, -0.3720, -0.1348,\n",
      "          0.7456,  0.5038],\n",
      "        [ 0.0831,  0.8649, -0.2909, -0.0547, -0.0183, -0.6306, -0.3416, -0.2130,\n",
      "          0.8901,  0.4467],\n",
      "        [-0.0262,  0.6672, -0.1857, -0.1855, -0.1821, -0.2841, -0.0670, -0.3251,\n",
      "          0.5405,  0.2907],\n",
      "        [ 0.0809,  0.6901, -0.1000,  0.0453,  0.1696, -0.4337,  0.0333, -0.1871,\n",
      "          0.6504,  0.1283],\n",
      "        [ 0.2747,  0.5402, -0.1255, -0.0290,  0.1871, -0.2941,  0.2804, -0.0164,\n",
      "          0.4493, -0.0361],\n",
      "        [ 0.0962,  0.4531, -0.3472, -0.3560, -0.0535, -0.1240,  0.3113, -0.0526,\n",
      "          0.1543,  0.2123],\n",
      "        [-0.0108,  0.5513, -0.3904, -0.1248,  0.0140, -0.4098, -0.1056, -0.0094,\n",
      "          0.4077,  0.2966]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "y shape: \n",
      " torch.Size([16, 256, 128])\n",
      "y : \n",
      " tensor([ 0.0493,  0.1931,  0.3707,  0.6034,  0.4418, -1.2721, -0.9076,  1.1355,\n",
      "         0.9257,  0.1348], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "y proj out shape: \n",
      " torch.Size([16, 256, 128])\n",
      "y proj out : \n",
      " tensor([ 0.4999,  0.3998,  0.2482,  0.3862,  0.9892,  0.4968,  0.1932, -0.6824,\n",
      "        -0.6104, -0.0000], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "logits shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "logits: \n",
      " tensor([[-0.1032, -0.6194, -0.0784,  1.6256,  0.3642, -0.2889, -0.1829, -0.5818,\n",
      "         -0.1270,  1.0914]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "predicted logit for first word: \n",
      " tensor([1.6256], device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.2 attention\n",
    "\n",
    "## pre-layernorm\n",
    "out = nn.LayerNorm(n_embed).to(device)(out)\n",
    "print('LayerNorm out shape: \\n', out.shape)\n",
    "print('LayerNorm out: \\n', out[0, 1, :10])\n",
    "\n",
    "## qkv\n",
    "# generate qkv once\n",
    "qkv = nn.Linear(n_embed, 3 * n_embed).to(device)(out)\n",
    "print('qkv shape: \\n', qkv.shape)\n",
    "print('qkv: \\n', qkv[0, 1, :10])\n",
    "q, k, v = qkv.split(n_embed, dim = -1)\n",
    "print('Split qkv shape: \\n', q.shape)\n",
    "print('Split qkv: \\n', q[0, 1, :10])\n",
    "\n",
    "## multi-head\n",
    "q = q.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "k = k.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "v = v.view(b, t, n_head, n_embed // n_head).transpose(1, 2)\n",
    "print('Multi-head q shape: \\n', q.shape)\n",
    "print('Multi-head q : \\n', q[0, 0, 1, :10])\n",
    "\n",
    "# self attention\n",
    "attn = q@k.transpose(-1,-2) / math.sqrt(n_embed)\n",
    "print('attn score shape: \\n', attn.shape)\n",
    "print('attn score : \\n', attn[0, 0, 1, :10])\n",
    "\n",
    "# mask fill\n",
    "## generate mask -> mask is a tril matrix for next token prediction \n",
    "## mask is applied to q@k and shape is (b, t, seq_len, seq_len)\n",
    "bias = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size).to(device=device)\n",
    "print('mask shape: \\n', bias.shape)\n",
    "print('mask : \\n', bias[0, 0, :10, :10])\n",
    "## some times, t may less than max_len\n",
    "attn = attn.masked_fill(bias[:, :, :t, :t] == 0, float('-inf'))\n",
    "print('masked attn shape: \\n', attn.shape)\n",
    "print('masked attn : \\n', attn[0, 0, :10, :10])\n",
    "\n",
    "# softmax\n",
    "attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "# dropout\n",
    "attn = nn.Dropout(dropout)(attn)\n",
    "\n",
    "# output\n",
    "## (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "y = attn @ v\n",
    "print('attn output shape: \\n', y.shape)\n",
    "print('attn output : \\n', y[0, 0, :10, :10])\n",
    "## transpose and output\n",
    "y = y.transpose(1, 2).contiguous().view(b, t, n_embed)\n",
    "print('y shape: \\n', y.shape)\n",
    "print('y : \\n', y[0, 0, :10])\n",
    "# proj\n",
    "y = nn.Dropout(dropout)(nn.Linear(n_embed, n_embed).to(device)(y))\n",
    "print('y proj out shape: \\n', y.shape)\n",
    "print('y proj out : \\n', y[0, 0, :10])\n",
    "\n",
    "# layerNorm\n",
    "y = nn.LayerNorm(n_embed).to(device)(y)\n",
    "\n",
    "# output\n",
    "## we just get last token logits\n",
    "logits = nn.Linear(n_embed, vocab_size).to(device)(y)\n",
    "print('logits shape: \\n', logits.shape)\n",
    "print('logits: \\n', logits[0, [-1], :10])\n",
    "print('predicted logit for first word: \\n', logits[0, [-1], torch.argmax(logits[0, [-1], :10])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "probs: \n",
      " tensor([[-0.1032, -0.6194, -0.0784,  1.6256,  0.3642, -0.2889, -0.1829, -0.5818,\n",
      "         -0.1270,  1.0914]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "y lable shape: \n",
      " torch.Size([16, 256])\n",
      "y lable: \n",
      " tensor([40, 43,  0, 42, 39, 51, 52, 43, 42,  1], device='cuda:0')\n",
      "loss shape: \n",
      " torch.Size([16, 256, 50304])\n",
      "loss: \n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8260, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## compute loss\n",
    "\n",
    "probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "print('probs shape: \\n', logits.shape)\n",
    "print('probs: \\n', logits[0, [-1], :10])\n",
    "\n",
    "print('y lable shape: \\n',y_true.shape)\n",
    "print('y lable: \\n', y_true[0,:10])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(probs.transpose(1, 2), y_true)\n",
    "\n",
    "print('loss shape: \\n', logits.shape)\n",
    "print('loss: \\n', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50304\n",
      "number of parameters: 123.69M\n",
      "model: \n",
      " GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "### load model\n",
    "config = GPTConfig()\n",
    "print(config.vocab_size)\n",
    "model = GPT(config)\n",
    "print('model: \\n', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
